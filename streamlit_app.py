import streamlit as st
import requests
import json
import re
import os
import pandas as pd
import plotly.graph_objects as go
import io
import time
from typing import Tuple, Dict, Any, List
from openpyxl import load_workbook
from openpyxl.styles import PatternFill

# ===============================
# 1. é¡µé¢é…ç½®
# ===============================
st.set_page_config(
    page_title="æ±‰è¯­è¯ç±»éš¶å±åº¦æ£€æµ‹ (æ‰¹é‡å¢å¼ºç‰ˆ)",
    page_icon="ğŸ›¡ï¸",
    layout="wide",
    initial_sidebar_state="collapsed"
)

st.markdown("""
<style>
header {visibility: hidden;}
footer {visibility: hidden;}
.dataframe {font-size: 12px;}
.stApp > div:first-child { padding-top: 2rem; }
/* ä¼˜åŒ–ä»£ç å—æ˜¾ç¤ºï¼Œé˜²æ­¢è¿‡é«˜ */
.stCode { max-height: 300px; overflow-y: auto; }
</style>
""", unsafe_allow_html=True)

# ===============================
# 2. æ¨¡å‹é…ç½® (OpenAI å…¼å®¹åè®®)
# ===============================
MODEL_CONFIGS = {
    "deepseek": {
        "base_url": "https://api.deepseek.com/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": 4096, "temperature": 0.0, "stream": True
        },
    },
    "openai": {
        "base_url": "https://api.openai.com/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": 4096, "temperature": 0.0, "stream": True
        },
    },
    "moonshot": {
        "base_url": "https://api.moonshot.cn/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": 4096, "temperature": 0.0, "stream": True
        },
    },
    "qwen": {
        "base_url": "https://dashscope.aliyuncs.com/api/v1",
        "endpoint": "/services/aigc/text-generation/generation",
        "headers": lambda key: {
            "Authorization": f"Bearer {key}", "Content-Type": "application/json",
            "X-DashScope-SSE": "enable", "Accept": "text/event-stream"
        },
        "payload": lambda model, messages, **kw: {
            "model": model, "input": {"messages": messages}, 
            "parameters": {"max_tokens": 4096, "temperature": 0.0, "result_format": "message", "incremental_output": True},
        },
    },
}

MODEL_OPTIONS = {
    "DeepSeek Chat": {"provider": "deepseek", "model": "deepseek-chat", "api_key": os.getenv("DEEPSEEK_API_KEY"), "env_var": "DEEPSEEK_API_KEY"},
    "OpenAI GPT-4o-mini": {"provider": "openai", "model": "gpt-4o-mini", "api_key": os.getenv("OPENAI_API_KEY"), "env_var": "OPENAI_API_KEY"},
    "Moonshot (Kimi)": {"provider": "moonshot", "model": "moonshot-v1-32k", "api_key": os.getenv("MOONSHOT_API_KEY"), "env_var": "MOONSHOT_API_KEY"},
    "Qwen (é€šä¹‰åƒé—®)": {"provider": "qwen", "model": "qwen-max", "api_key": os.getenv("QWEN_API_KEY"), "env_var": "QWEN_API_KEY"},
}

AVAILABLE_MODEL_OPTIONS = {name: info for name, info in MODEL_OPTIONS.items() if info["api_key"]}
if not AVAILABLE_MODEL_OPTIONS: AVAILABLE_MODEL_OPTIONS = MODEL_OPTIONS

# ===============================
# 3. è§„åˆ™å®šä¹‰
# ===============================
RULE_SETS = {
    "åè¯": [
        {"name": "N1_å¯å—æ•°é‡è¯ä¿®é¥°", "desc": "å¯ä»¥å—æ•°é‡è¯ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "N2_ä¸èƒ½å—å‰¯è¯ä¿®é¥°", "desc": "ä¸èƒ½å—å‰¯è¯ä¿®é¥°", "match_score": 20, "mismatch_score": -20},
        {"name": "N3_å¯ä½œä¸»å®¾è¯­", "desc": "å¯ä»¥åšå…¸å‹çš„ä¸»è¯­æˆ–å®¾è¯­", "match_score": 20, "mismatch_score": 0},
        {"name": "N4_å¯ä½œä¸­å¿ƒè¯­æˆ–ä½œå®šè¯­", "desc": "å¯ä»¥åšä¸­å¿ƒè¯­å—å…¶ä»–åè¯ä¿®é¥°ï¼Œæˆ–è€…ä½œå®šè¯­ç›´æ¥ä¿®é¥°å…¶ä»–åè¯", "match_score": 10, "mismatch_score": 0},
        {"name": "N5_å¯åé™„çš„å­—ç»“æ„", "desc": "å¯ä»¥åé™„åŠ©è¯â€œçš„â€æ„æˆâ€œçš„â€å­—ç»“æ„", "match_score": 10, "mismatch_score": 0},
        {"name": "N6_å¯åé™„æ–¹ä½è¯æ„å¤„æ‰€", "desc": "å¯ä»¥åé™„æ–¹ä½è¯æ„æˆå¤„æ‰€ç»“æ„", "match_score": 10, "mismatch_score": 0},
        {"name": "N7_ä¸èƒ½ä½œè°“è¯­æ ¸å¿ƒ", "desc": "ä¸èƒ½åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒ", "match_score": 10, "mismatch_score": -10},
        {"name": "N8_ä¸èƒ½ä½œè¡¥è¯­/ä¸€èˆ¬ä¸ä½œçŠ¶è¯­", "desc": "ä¸èƒ½ä½œè¡¥è¯­ï¼Œå¹¶ä¸”ä¸€èˆ¬ä¸èƒ½åšçŠ¶è¯­", "match_score": 10, "mismatch_score": 0},
    ],
    "åŠ¨è¯": [
        {"name": "V1_å¯å—å¦å®š'ä¸/æ²¡æœ‰'ä¿®é¥°", "desc": "å¯ä»¥å—å¦å®šå‰¯è¯'ä¸'æˆ–'æ²¡æœ‰'ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "V2_å¯åé™„/æ’å…¥æ—¶ä½“åŠ©è¯", "desc": "å¯ä»¥åé™„æˆ–ä¸­é—´æ’å…¥æ—¶ä½“åŠ©è¯'ç€/äº†/è¿‡'", "match_score": 10, "mismatch_score": 0},
        {"name": "V3_å¯å¸¦çœŸå®¾è¯­", "desc": "å¯ä»¥å¸¦çœŸå®¾è¯­ï¼Œæˆ–é€šè¿‡ä»‹è¯å¼•å¯¼è®ºå…ƒ", "match_score": 20, "mismatch_score": 0},
        {"name": "V4_ç¨‹åº¦å‰¯è¯ä¸å¸¦å®¾è¯­çš„å…³ç³»", "desc": "ä¸èƒ½å—ç¨‹åº¦å‰¯è¯'å¾ˆ'ä¿®é¥°ï¼Œæˆ–èƒ½åŒæ—¶å—'å¾ˆ'ä¿®é¥°å¹¶å¸¦å®¾è¯­", "match_score": 10, "mismatch_score": -10},
        {"name": "V5_å¯æœ‰é‡å /æ­£åé‡å å½¢å¼", "desc": "å¯ä»¥æœ‰'VV, Vä¸€V'ç­‰å½¢å¼", "match_score": 10, "mismatch_score": 0},
        {"name": "V6_å¯åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒ", "desc": "å¯ä»¥åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒ", "match_score": 10, "mismatch_score": -10},
        {"name": "V7_ä¸èƒ½ä½œçŠ¶è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "desc": "ä¸èƒ½ä½œçŠ¶è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "match_score": 10, "mismatch_score": 0},
        {"name": "V8_å¯ä½œ'æ€ä¹ˆ/æ€æ ·'æé—®", "desc": "å¯ä»¥è·Ÿåœ¨'æ€ä¹ˆ/æ€æ ·'ä¹‹åæé—®", "match_score": 10, "mismatch_score": 0},
        {"name": "V9_ä¸èƒ½è·Ÿåœ¨'å¤š/å¤šä¹ˆ'ä¹‹å", "desc": "ä¸èƒ½è·Ÿåœ¨'å¤š'ä¹‹åå¯¹æ€§è´¨æé—®", "match_score": 10, "mismatch_score": -10},
    ],
    "ååŠ¨è¯": [
        {"name": "NV1_å¯è¢«å¦å®šä¸”è‚¯å®šå½¢å¼-1", "desc": "å¯ä»¥ç”¨'ä¸/æ²¡æœ‰'å¦å®š", "match_score": 10, "mismatch_score": -10},
        {"name": "NV2_å¯é™„æ—¶ä½“åŠ©è¯", "desc": "å¯ä»¥åé™„æ—¶ä½“åŠ©è¯'ç€/äº†/è¿‡'", "match_score": 10, "mismatch_score": -10},
        {"name": "NV3_å¯å¸¦çœŸå®¾è¯­ä¸”ä¸å—å¾ˆä¿®é¥°", "desc": "å¯ä»¥å¸¦çœŸå®¾è¯­ï¼Œå¹¶ä¸”ä¸èƒ½å—'å¾ˆ'ä¿®é¥°", "match_score": 10, "mismatch_score": -10},
        {"name": "NV4_æœ‰é‡å å’Œæ­£åé‡å å½¢å¼", "desc": "æœ‰é‡å å½¢å¼", "match_score": 10, "mismatch_score": 0},
        {"name": "NV5_å¯ä½œå¤šç§å¥æ³•æˆåˆ†", "desc": "æ—¢å¯ä»¥ä½œè°“è¯­ï¼Œåˆå¯ä»¥ä½œä¸»è¯­æˆ–å®¾è¯­", "match_score": 10, "mismatch_score": -10},
        {"name": "NV6_ä¸èƒ½ç›´æ¥ä½œçŠ¶è¯­", "desc": "ä¸èƒ½ç›´æ¥ä½œçŠ¶è¯­", "match_score": 10, "mismatch_score": -10},
        {"name": "NV7_å¯ä¿®é¥°åè¯æˆ–å—åè¯ä¿®é¥°", "desc": "å¯ä»¥ä¿®é¥°åè¯æˆ–è€…å—åè¯ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "NV8_å¯è·Ÿåœ¨æ€ä¹ˆ/æ€æ ·ä¹‹å", "desc": "å¯ä»¥è·Ÿåœ¨'æ€ä¹ˆ/æ€æ ·'ä¹‹åæé—®", "match_score": 10, "mismatch_score": 0},
        {"name": "NV9_ä¸èƒ½è·Ÿåœ¨å¤š/å¤šä¹ˆä¹‹å", "desc": "ä¸èƒ½è·Ÿåœ¨'å¤š/å¤šä¹ˆ'ä¹‹å", "match_score": 10, "mismatch_score": -10},
        {"name": "NV10_å¯åé™„æ–¹ä½è¯", "desc": "å¯ä»¥åé™„æ–¹ä½è¯æ„æˆå¤„æ‰€ç»“æ„", "match_score": 10, "mismatch_score": 0},
    ]
}

# ===============================
# 4. æ ¸å¿ƒå·¥å…·å‡½æ•°
# ===============================
def extract_text_from_response(resp_json: Dict[str, Any]) -> str:
    """æå–APIå“åº”æ–‡æœ¬ï¼Œå…¼å®¹å¤šç§æ ¼å¼"""
    if not isinstance(resp_json, dict): return ""
    try:
        # Qwen
        if "output" in resp_json and "text" in resp_json["output"]: return resp_json["output"]["text"]
        # OpenAI/Compatible
        if "choices" in resp_json and len(resp_json["choices"]) > 0:
            choice = resp_json["choices"][0]
            if "message" in choice and "content" in choice["message"]: return choice["message"]["content"]
        return json.dumps(resp_json, ensure_ascii=False)
    except Exception: return json.dumps(resp_json, ensure_ascii=False)

def extract_json_from_text(text: str) -> Tuple[Dict[str, Any], str]:
    """å¼ºåŠ›JSONæå–ï¼Œä¼˜å…ˆä»£ç å—ï¼Œå…¶æ¬¡å¤§æ‹¬å·ï¼Œæå–å¤±è´¥è¿”å›None"""
    if not text: return None, ""
    json_str = ""
    # ç­–ç•¥1: Markdownä»£ç å—
    code_match = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
    if code_match: json_str = code_match.group(1).strip()
    # ç­–ç•¥2: æœ€å¤–å±‚å¤§æ‹¬å·
    if not json_str:
        match = re.search(r"(\{.*\})", text.strip(), re.DOTALL)
        if match: json_str = match.group(1).strip()
    
    if not json_str: return None, text
    try:
        return json.loads(json_str), json_str
    except:
        return None, text

def map_to_allowed_score(rule: dict, raw_val) -> int:
    match, mismatch = rule["match_score"], rule["mismatch_score"]
    if isinstance(raw_val, bool): return match if raw_val else mismatch
    if isinstance(raw_val, str):
        s = raw_val.strip().lower()
        if s in ("yes", "y", "true", "æ˜¯", "âˆš", "ç¬¦åˆ"): return match
        return mismatch
    return mismatch

def calculate_membership(scores_all: Dict[str, Dict[str, int]]) -> Dict[str, float]:
    membership = {}
    for pos, scores in scores_all.items():
        total = sum(scores.values())
        membership[pos] = max(-1.0, min(1.0, total / 100))
    return membership

def get_top_10_positions(membership: Dict[str, float]) -> List[Tuple[str, float]]:
    return sorted(membership.items(), key=lambda x: x[1], reverse=True)[:10]

# ===============================
# 5. API è°ƒç”¨ (æµå¼ + è¶…æ—¶ä¿æŠ¤)
# ===============================
def call_llm_api_cached(_provider, _model, _api_key, messages, max_tokens=4096, temperature=0.0):
    if not _api_key: return False, {"error": "API Keyç¼ºå¤±"}, "Keyæœªè®¾ç½®"
    cfg = MODEL_CONFIGS[_provider]
    url = f"{cfg['base_url'].rstrip('/')}{cfg['endpoint']}"
    headers = cfg["headers"](_api_key)
    payload = cfg["payload"](_model, messages, max_tokens=max_tokens, temperature=temperature)
    
    full_content = ""
    try:
        # è®¾ç½® stream=True å’Œ 60s è¿æ¥è¶…æ—¶
        with requests.post(url, headers=headers, json=payload, stream=True, timeout=60) as response:
            response.raise_for_status()
            for line in response.iter_lines():
                if not line: continue
                line_text = line.decode('utf-8').strip()
                if line_text.startswith("data:"): json_str = line_text[5:].strip()
                else: json_str = line_text
                if json_str == "[DONE]": break
                try:
                    chunk = json.loads(json_str)
                    delta_text = ""
                    if "choices" in chunk and len(chunk["choices"]) > 0:
                        delta_text = chunk["choices"][0].get("delta", {}).get("content", "")
                    elif "output" in chunk: # Qwen
                        if "choices" in chunk["output"]:
                            delta_text = chunk["output"]["choices"][0].get("message", {}).get("content", "")
                        elif "text" in chunk["output"]:
                            delta_text = chunk["output"]["text"]
                    if delta_text: full_content += delta_text
                except: continue
        
        if not full_content: return False, {"error": "ç©ºå“åº”"}, "ç©ºå“åº”"
        return True, {"choices": [{"message": {"content": full_content}}], "output": {"text": full_content}}, ""
    except Exception as e:
        return False, {"error": str(e)}, str(e)

# ===============================
# 6. å•ä¸ªè¯åˆ†æé€»è¾‘
# ===============================
def ask_model_for_pos_and_scores(word: str, provider: str, model: str, api_key: str):
    full_rules = {p: "\n".join([f"- {r['name']}: {r['desc']}" for r in rs]) for p, rs in RULE_SETS.items()}
    
    system = f"""ä½ æ˜¯ä¸€åæ±‰è¯­è¯æ³•ä¸“å®¶ã€‚åˆ†æè¯è¯­ã€Œ{word}ã€ã€‚
ä»»åŠ¡ï¼š
1. è¯¦ç»†åˆ†æè¯¥è¯æ˜¯å¦ç¬¦åˆã€åè¯ã€‘ã€åŠ¨è¯ã€‘ã€ååŠ¨è¯ã€‘çš„å„é¡¹è§„åˆ™ã€‚
2. ç»™å‡ºæ˜ç¡®çš„"ç¬¦åˆ/ä¸ç¬¦åˆ"åˆ¤æ–­ã€‚
3. æœ€åè¾“å‡ºç¬¦åˆè§„èŒƒçš„ JSONã€‚

è§„åˆ™å‚è€ƒï¼š
{json.dumps(full_rules, ensure_ascii=False, indent=2)}

JSONæ ¼å¼è¦æ±‚ï¼š
{{
  "explanation": "è¿™é‡Œå†™è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹...",
  "predicted_pos": "åè¯/åŠ¨è¯/ååŠ¨è¯",
  "scores": {{
    "åè¯": {{ "N1_...": true, ... }},
    "åŠ¨è¯": {{ "V1_...": false, ... }},
    "ååŠ¨è¯": {{ "NV1_...": true, ... }}
  }}
}}
"""
    ok, resp, err = call_llm_api_cached(provider, model, api_key, [
        {"role": "system", "content": system},
        {"role": "user", "content": f"åˆ†æã€Œ{word}ã€"}
    ])
    
    if not ok: return {}, "", "å¤±è´¥", err
    
    raw = extract_text_from_response(resp)
    data, _ = extract_json_from_text(raw)
    
    # å…œåº•ï¼šå¦‚æœJSONè§£æå¤±è´¥ï¼Œexplanationå°±æ˜¯rawæ–‡æœ¬
    if data:
        expl = data.get("explanation", raw)
        pred = data.get("predicted_pos", "æœªçŸ¥")
        raw_scores = data.get("scores", {})
    else:
        expl = "è§£æJSONå¤±è´¥ï¼ŒåŸå§‹è¾“å‡ºå¦‚ä¸‹ï¼š\n" + raw
        pred = "æœªçŸ¥"
        raw_scores = {}
        
    # åˆ†æ•°æ ‡å‡†åŒ–
    scores_out = {p: {} for p in RULE_SETS}
    for pos, rules in RULE_SETS.items():
        s_map = raw_scores.get(pos, {})
        for r in rules:
            val = False
            # æ¨¡ç³ŠåŒ¹é…é”®å
            for k, v in s_map.items():
                if k.replace(" ", "").upper() == r["name"].replace(" ", "").upper():
                    val = v
                    break
            scores_out[pos][r["name"]] = map_to_allowed_score(r, val)
            
    return scores_out, raw, pred, expl

# ===============================
# 7. æ‰¹é‡å¤„ç†é€»è¾‘ (å®æ—¶ä¿å­˜ + å¢é‡æ›´æ–°)
# ===============================
def process_batch(df, model_info, col_name):
    """
    æ ¸å¿ƒä¿®æ”¹ï¼š
    1. ä½¿ç”¨ 'history_database.csv' ä½œä¸ºæŒä¹…åŒ–å­˜å‚¨ã€‚
    2. ä¼˜å…ˆè¯»å– CSV è·³è¿‡å·²å¤„ç†è¯æ±‡ã€‚
    3. æ¯å¤„ç†ä¸€ä¸ªè¯ï¼Œè¿½åŠ å†™å…¥ CSVã€‚
    """
    db_file = "history_database.csv"
    output = io.BytesIO()
    
    # A. è¯»å–å†å²è®°å½•å»ºç«‹ç¼“å­˜
    history_cache = {}
    if os.path.exists(db_file):
        try:
            # å¼ºåˆ¶æŒ‰å­—ç¬¦ä¸²è¯»å–ï¼Œé¿å…æ•°å­—/æ–‡æœ¬æ··æ·†
            hist_df = pd.read_csv(db_file, dtype=str)
            for _, row in hist_df.iterrows():
                if "è¯è¯­" in row and pd.notna(row["è¯è¯­"]):
                    history_cache[str(row["è¯è¯­"]).strip()] = row.to_dict()
            st.info(f"ğŸ“š å·²åŠ è½½æœ¬åœ°å†å²è®°å½• {len(history_cache)} æ¡ï¼Œå°†è‡ªåŠ¨è·³è¿‡è¿™äº›è¯ã€‚")
        except Exception as e:
            st.warning(f"å†å²æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œå°†é‡æ–°åˆ†æ: {e}")

    # B. å‡†å¤‡è¿›åº¦æ¡
    total = len(df)
    bar = st.progress(0)
    status = st.empty()
    
    final_rows = []
    
    # C. å¼€å§‹å¾ªç¯
    for i, row_data in df.iterrows():
        word = str(row_data[col_name]).strip()
        
        # 1. æ£€æŸ¥ç¼“å­˜
        if word in history_cache:
            status.text(f"â™»ï¸ [è·³è¿‡] {word} (å·²åœ¨å†å²è®°å½•)")
            final_rows.append(history_cache[word])
            # å°å»¶æ—¶è®©ç•Œé¢åˆ·æ–°
            time.sleep(0.01) 
            bar.progress((i + 1) / total)
            continue
            
        # 2. ä¸åœ¨ç¼“å­˜ï¼Œè°ƒç”¨ API (å¸¦é‡è¯•)
        status.text(f"ğŸš€ [åˆ†æä¸­] {word} ({i + 1}/{total})")
        
        retries = 3
        success = False
        scores, raw, pred, expl = {}, "", "è¯·æ±‚å¤±è´¥", "å¤šæ¬¡é‡è¯•å¤±è´¥"
        
        for attempt in range(retries):
            try:
                scores, raw, pred, expl = ask_model_for_pos_and_scores(
                    word, model_info["provider"], model_info["model"], model_info["api_key"]
                )
                # åªè¦ raw ä¸ä¸ºç©ºå°±ç®—æœ‰å“åº”
                if raw:
                    success = True
                    break
                time.sleep(2)
            except:
                time.sleep(2)
        
        # 3. è®¡ç®—ç»“æœ
        if success and scores:
            mem = calculate_membership(scores)
            v = mem.get("åŠ¨è¯", 0.0)
            n = mem.get("åè¯", 0.0)
            nv = mem.get("ååŠ¨è¯", 0.0)
        else:
            v, n, nv = 0.0, 0.0, 0.0
            
        # 4. æ„é€ æ–°è¡Œ
        new_row = {
            "è¯è¯­": word,
            "åŠ¨è¯": v, "åè¯": n, "ååŠ¨è¯": nv,
            "å·®å€¼/è·ç¦»": round(abs(v - n), 4),
            "åŸå§‹å“åº”": expl if len(expl) > 5 else raw, # ç¡®ä¿æ¨ç†ä¸ä¸¢å¤±
            "_predicted_pos": pred
        }
        final_rows.append(new_row)
        
        # 5. ã€æ ¸å¿ƒã€‘å®æ—¶è¿½åŠ å†™å…¥ CSV
        try:
            temp_df = pd.DataFrame([new_row])
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™å†™è¡¨å¤´ï¼Œå­˜åœ¨åˆ™ä¸å†™è¡¨å¤´ç›´æ¥è¿½åŠ 
            write_header = not os.path.exists(db_file)
            temp_df.to_csv(db_file, mode='a', header=write_header, index=False, encoding='utf-8-sig')
        except Exception as e:
            print(f"å†™å…¥å¤±è´¥: {e}")
            
        # 6. é˜²å°å·å»¶æ—¶
        time.sleep(1)
        bar.progress((i + 1) / total)

    # D. å¾ªç¯ç»“æŸï¼Œç”Ÿæˆæ¼‚äº®çš„ Excel
    status.success("âœ… å…¨éƒ¨å®Œæˆï¼")
    
    if not final_rows: return None
    
    res_df = pd.DataFrame(final_rows)
    # å¯¼å‡º
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        cols = ["è¯è¯­", "åŠ¨è¯", "åè¯", "ååŠ¨è¯", "å·®å€¼/è·ç¦»", "åŸå§‹å“åº”"]
        # ç¡®ä¿åˆ—å­˜åœ¨
        valid_cols = [c for c in cols if c in res_df.columns]
        res_df[valid_cols].to_excel(writer, index=False, sheet_name='ç»“æœ')
        # æ ‡é»„
        try:
            ws = writer.sheets['ç»“æœ']
            fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
            for idx, r in enumerate(final_rows):
                # å…¼å®¹ä»CSVè¯»å–çš„æ•°æ®ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼‰å’Œåˆšç”Ÿæˆçš„æ•°æ®
                p = str(r.get("_predicted_pos", ""))
                target = None
                if "åŠ¨è¯" in p: target = 2
                elif "åè¯" in p: target = 3
                elif "ååŠ¨è¯" in p: target = 4
                if target: ws.cell(row=idx+2, column=target).fill = fill
        except: pass
        
    return output.getvalue()

# ===============================
# 8. ä¸»ç¨‹åº UI
# ===============================
def main():
    st.title("ğŸ“° æ±‰è¯­è¯ç±»éš¶å±åº¦æ£€æµ‹ (æ‰¹é‡æ——èˆ°ç‰ˆ)")
    
    # é…ç½®åŒº
    with st.container():
        c1, c2 = st.columns([3, 1])
        with c1:
            if not AVAILABLE_MODEL_OPTIONS:
                st.error("âŒ æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·é…ç½®ç¯å¢ƒå˜é‡ã€‚")
                info = {"api_key": None}
            else:
                name = st.selectbox("é€‰æ‹©æ¨¡å‹", list(AVAILABLE_MODEL_OPTIONS.keys()))
                info = AVAILABLE_MODEL_OPTIONS[name]
        with c2:
            st.write("")
            if st.button("è¿æ¥æµ‹è¯•"):
                ok, _, msg = call_llm_api_cached(info["provider"], info["model"], info["api_key"], [{"role":"user","content":"hi"}], 5)
                if ok: st.success("é€šç•…")
                else: st.error(msg)

    st.markdown("---")
    
    t1, t2 = st.tabs(["ğŸ” å•ä¸ªè¯åˆ†æ", "ğŸ“‚ æ‰¹é‡å…¨è‡ªåŠ¨å¤„ç†"])
    
    # --- Tab 1: å•ä¸ª ---
    with t1:
        w = st.text_input("è¾“å…¥è¯è¯­", key="single_w")
        if st.button("åˆ†æ", disabled=not (w and info["api_key"])):
            with st.spinner("æ€è€ƒä¸­..."):
                scores, raw, pred, expl = ask_model_for_pos_and_scores(w, info["provider"], info["model"], info["api_key"])
                if scores:
                    mem = calculate_membership(scores)
                    st.success(f"ç»“æœ: {pred}")
                    c_a, c_b = st.columns(2)
                    with c_a:
                        st.table(pd.DataFrame(get_top_10_positions(mem), columns=["è¯ç±»","éš¶å±åº¦"]))
                        fig = go.Figure(go.Scatterpolar(r=list(mem.values())+[list(mem.values())[0]], theta=list(mem.keys())+[list(mem.keys())[0]], fill='toself'))
                        st.plotly_chart(fig, use_container_width=True)
                    with c_b:
                        st.info(expl)
                        with st.expander("åŸå§‹ JSON"): st.code(raw)

    # --- Tab 2: æ‰¹é‡ ---
    with t2:
        st.info("ä¸Šä¼  Excel (éœ€å«'è¯è¯­'åˆ—)ã€‚ç¨‹åºä¼šè‡ªåŠ¨ä¿å­˜è¿›åº¦åˆ° `history_database.csv`ï¼Œä¸­æ–­åé‡è·‘å³å¯è‡ªåŠ¨ç»­ä¼ ã€‚")
        
        up = st.file_uploader("ä¸Šä¼  Excel", type=["xlsx"])
        
        if up and info["api_key"]:
            try:
                df = pd.read_excel(up)
                target = next((c for c in df.columns if "è¯" in str(c) or "word" in str(c).lower()), None)
                
                if target:
                    if st.button("ğŸš€ å¼€å§‹æ‰¹é‡ (æ”¯æŒæ–­ç‚¹ç»­ä¼ )"):
                        res = process_batch(df, info, target)
                        if res:
                            st.download_button("ğŸ“¥ ä¸‹è½½æœ€ç»ˆç»“æœ (Excel)", res, "final_result.xlsx")
                else:
                    st.error("æœªæ‰¾åˆ°åŒ…å«'è¯'çš„åˆ—")
            except Exception as e:
                st.error(f"æ–‡ä»¶é”™è¯¯: {e}")

        # --- å†å²æ•°æ®ç®¡ç†åŒº ---
        st.markdown("---")
        st.subheader("ğŸ’¾ æ•°æ®ä¿é™©ç®±")
        db = "history_database.csv"
        if os.path.exists(db):
            try:
                hist = pd.read_csv(db)
                st.write(f"å½“å‰å·²å®‰å…¨ä¿å­˜ **{len(hist)}** æ¡æ•°æ®ã€‚")
                c_d1, c_d2 = st.columns([1, 4])
                with c_d1:
                    st.download_button("ğŸ“¥ ä¸‹è½½å†å²è®°å½• (CSV)", hist.to_csv(index=False).encode('utf-8-sig'), "history_database.csv", "text/csv")
                with c_d2:
                    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå†å² (é‡æ–°å¼€å§‹)"):
                        os.remove(db)
                        st.rerun()
                with st.expander("é¢„è§ˆæ•°æ®"):
                    st.dataframe(hist)
            except:
                st.error("å†å²æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œå¯èƒ½æ­£åœ¨å†™å…¥ä¸­ï¼Œè¯·ç¨ååˆ·æ–°ã€‚")

if __name__ == "__main__":
    main()
