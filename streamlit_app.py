import streamlit as st
import requests
import json
import re
import os
import pandas as pd
import plotly.graph_objects as go
from typing import Tuple, Dict, Any, List
from concurrent.futures import ThreadPoolExecutor # å¼•å…¥å¤šçº¿ç¨‹å¹¶å‘åº“

# ===============================
# é¡µé¢é…ç½®
# ===============================
st.set_page_config(
    page_title="æ±‰è¯­è¯ç±»éš¶å±åº¦æ£€æµ‹åˆ’ç±»",
    page_icon="ğŸ“°",
    layout="wide",  # ä½¿ç”¨å®½å¸ƒå±€
    initial_sidebar_state="collapsed",  # é»˜è®¤æŠ˜å ä¾§è¾¹æ 
    menu_items=None
)

# è‡ªå®šä¹‰CSSæ ·å¼
hide_streamlit_style = """
<style>
/* éšè—é¡¶éƒ¨èœå•æ å’Œé¡µè„š */
header {visibility: hidden;}
footer {visibility: hidden;}

/* è°ƒæ•´è¡¨æ ¼æ ·å¼ */
.dataframe {font-size: 12px;}

/* éšè—é»˜è®¤çš„ä¾§è¾¹æ  */
[data-testid="stSidebar"] {
    display: none !important;
}

/* ä¸ºé¡¶éƒ¨æ§åˆ¶åŒºæ·»åŠ è¾¹æ¡†å’ŒèƒŒæ™¯è‰²ï¼Œä½¿å…¶çœ‹èµ·æ¥åƒä¸€ä¸ªå›ºå®šçš„é¢æ¿ */
.stApp > div:first-child {
    padding-top: 2rem;
}
</style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# ===============================
# æ¨¡å‹é…ç½® (ä»…ä»ç¯å¢ƒå˜é‡è·å–API Keyï¼Œç§»é™¤ç¡¬ç¼–ç å€¼ä»¥æé«˜å®‰å…¨æ€§)
# ===============================
MODEL_CONFIGS = {
    "deepseek": {
        "base_url": "https://api.deepseek.com/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": kw.get("max_tokens", 4096), "temperature": kw.get("temperature", 0.0), "stream": False,
        },
    },
    "openai": {
        "base_url": "https://api.openai.com/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": kw.get("max_tokens", 4096), "temperature": kw.get("temperature", 0.0), "stream": False,
        },
    },
    "moonshot": {
        "base_url": "https://api.moonshot.cn/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": kw.get("max_tokens", 4096), "temperature": kw.get("temperature", 0.0), "stream": False,
        },
    },
    "qwen": {
        "base_url": "https://dashscope.aliyuncs.com/api/v1",
        "endpoint": "/services/aigc/text-generation/generation",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "input": {"messages": messages}, "parameters": {"max_tokens": kw.get("max_tokens", 4096), "temperature": kw.get("temperature", 0.0)},
        },
    },
}

# æ¨¡å‹é€‰é¡¹ï¼ˆä»…ä»ç¯å¢ƒå˜é‡è·å–API Keyï¼‰
MODEL_OPTIONS = {
    "DeepSeek Chat": {
        "provider": "deepseek", 
        "model": "deepseek-chat", 
        "api_key": os.getenv("DEEPSEEK_API_KEY"),
        "env_var": "DEEPSEEK_API_KEY"
    },
    "OpenAI GPT-4oï¼ˆæ¨èï¼‰": {
        "provider": "openai", 
        "model": "gpt-4o-mini", 
        "api_key": os.getenv("OPENAI_API_KEY"),
        "env_var": "OPENAI_API_KEY"
    },
    "Moonshotï¼ˆKimiï¼‰": {
        "provider": "moonshot", 
        "model": "moonshot-v1-32k", 
        "api_key": os.getenv("MOONSHOT_API_KEY"),
        "env_var": "MOONSHOT_API_KEY"
    },
    "Qwenï¼ˆé€šä¹‰åƒé—®ï¼‰": {
        "provider": "qwen", 
        "model": "qwen-max", 
        "api_key": os.getenv("QWEN_API_KEY"),
        "env_var": "QWEN_API_KEY"
    },
}

# è¿‡æ»¤æ‰æ²¡æœ‰é…ç½® API Key çš„æ¨¡å‹ï¼Œåªä¿ç•™å¯ç”¨çš„
AVAILABLE_MODEL_OPTIONS = {
    name: info for name, info in MODEL_OPTIONS.items() if info["api_key"]
}

# å¦‚æœæ²¡æœ‰å¯ç”¨æ¨¡å‹ï¼Œåˆ™æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹ä½†ç»™å‡ºè­¦å‘Š
if not AVAILABLE_MODEL_OPTIONS:
    AVAILABLE_MODEL_OPTIONS = MODEL_OPTIONS

# ===============================
# è¯ç±»è§„åˆ™ä¸æœ€å¤§å¾—åˆ†
# ===============================
RULE_SETS = {
    # 1.1 åè¯
    "åè¯": [
        {"name": "N1_å¯å—æ•°é‡è¯ä¿®é¥°", "desc": "å¯ä»¥å—æ•°é‡è¯ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "N2_ä¸èƒ½å—å‰¯è¯ä¿®é¥°", "desc": "ä¸èƒ½å—å‰¯è¯ä¿®é¥°", "match_score": 20, "mismatch_score": -20},
        {"name": "N3_å¯ä½œä¸»å®¾è¯­", "desc": "å¯ä»¥åšå…¸å‹çš„ä¸»è¯­æˆ–å®¾è¯­", "match_score": 20, "mismatch_score": 0},
        {"name": "N4_å¯ä½œä¸­å¿ƒè¯­æˆ–ä½œå®šè¯­", "desc": "å¯ä»¥åšä¸­å¿ƒè¯­å—å…¶ä»–åè¯ä¿®é¥°ï¼Œæˆ–è€…ä½œå®šè¯­ç›´æ¥ä¿®é¥°å…¶ä»–åè¯", "match_score": 10, "mismatch_score": 0},
        {"name": "N5_å¯åé™„çš„å­—ç»“æ„", "desc": "å¯ä»¥åé™„åŠ©è¯â€œçš„â€æ„æˆâ€œçš„â€å­—ç»“æ„", "match_score": 10, "mismatch_score": 0},
        {"name": "N6_å¯åé™„æ–¹ä½è¯æ„å¤„æ‰€", "desc": "å¯ä»¥åé™„æ–¹ä½è¯æ„æˆå¤„æ‰€ç»“æ„", "match_score": 10, "mismatch_score": 0},
        {"name": "N7_ä¸èƒ½ä½œè°“è¯­æ ¸å¿ƒ", "desc": "ä¸èƒ½åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒï¼ˆä¸èƒ½å¸¦å®¾è¯­ï¼Œä¸èƒ½å—çŠ¶è¯­å’Œè¡¥è¯­ï¼Œä¸èƒ½åé™„æ—¶ä½“åŠ©è¯ï¼‰", "match_score": 10, "mismatch_score": -10},
        {"name": "N8_ä¸èƒ½ä½œè¡¥è¯­/ä¸€èˆ¬ä¸ä½œçŠ¶è¯­", "desc": "ä¸èƒ½ä½œè¡¥è¯­ï¼Œå¹¶ä¸”ä¸€èˆ¬ä¸èƒ½åšçŠ¶è¯­ç›´æ¥ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "match_score": 10, "mismatch_score": 0},
    ],
    # 1.5 åŠ¨è¯
    "åŠ¨è¯": [
        {"name": "V1_å¯å—å¦å®š'ä¸/æ²¡æœ‰'ä¿®é¥°", "desc": "å¯ä»¥å—å¦å®šå‰¯è¯'ä¸'æˆ–'æ²¡æœ‰'ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "V2_å¯åé™„/æ’å…¥æ—¶ä½“åŠ©è¯'ç€/äº†/è¿‡'", "desc": "å¯ä»¥åé™„æˆ–ä¸­é—´æ’å…¥æ—¶ä½“åŠ©è¯'ç€/äº†/è¿‡'ï¼Œæˆ–è¿›å…¥'...äº†æ²¡æœ‰'æ ¼å¼", "match_score": 10, "mismatch_score": 0},
        {"name": "V3_å¯å¸¦çœŸå®¾è¯­æˆ–é€šè¿‡ä»‹è¯å¼•å¯¼è®ºå…ƒ", "desc": "å¯ä»¥å¸¦çœŸå®¾è¯­ï¼Œæˆ–é€šè¿‡'å’Œ/ä¸º/å¯¹/å‘/æ‹¿/äº'ç­‰ä»‹è¯å¼•å¯¼è®ºå…ƒ", "match_score": 20, "mismatch_score": 0},
        {"name": "V4_ç¨‹åº¦å‰¯è¯ä¸å¸¦å®¾è¯­çš„å…³ç³»", "desc": "ä¸èƒ½å—ç¨‹åº¦å‰¯è¯'å¾ˆ'ä¿®é¥°ï¼Œæˆ–èƒ½åŒæ—¶å—'å¾ˆ'ä¿®é¥°å¹¶å¸¦å®¾è¯­ï¼ˆæŒ‰æ¡ç›®ç»™äºˆå¾—åˆ†ï¼‰", "match_score": 10, "mismatch_score": -10},
        {"name": "V5_å¯æœ‰é‡å /æ­£åé‡å å½¢å¼", "desc": "å¯ä»¥æœ‰'VV, Vä¸€V, Väº†V, Vä¸V, Väº†æ²¡æœ‰'ç­‰å½¢å¼", "match_score": 10, "mismatch_score": 0},
        {"name": "V6_å¯åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒ", "desc": "å¯ä»¥åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒï¼ˆä¸€èˆ¬å¯å—çŠ¶è¯­æˆ–è¡¥è¯­ä¿®é¥°ï¼‰", "match_score": 10, "mismatch_score": -10},
        {"name": "V7_ä¸èƒ½ä½œçŠ¶è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "desc": "ä¸èƒ½ä½œçŠ¶è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "match_score": 10, "mismatch_score": 0},
        {"name": "V8_å¯ä½œ'æ€ä¹ˆ/æ€æ ·'æé—®æˆ–'è¿™ä¹ˆ/è¿™æ ·/é‚£ä¹ˆ'å›ç­”", "desc": "å¯ä»¥è·Ÿåœ¨'æ€ä¹ˆ/æ€æ ·'ä¹‹åæé—®æˆ–è·Ÿåœ¨'è¿™ä¹ˆ/è¿™æ ·/é‚£ä¹ˆ'ä¹‹åå›ç­”", "match_score": 10, "mismatch_score": 0},
        {"name": "V9_ä¸èƒ½è·Ÿåœ¨'å¤š/å¤šä¹ˆ'ä¹‹åæé—®æˆ–è¡¨ç¤ºæ„Ÿå¹", "desc": "ä¸èƒ½è·Ÿåœ¨'å¤š'ä¹‹åå¯¹æ€§è´¨æé—®ï¼Œä¸èƒ½è·Ÿåœ¨'å¤šä¹ˆ'ä¹‹åè¡¨ç¤ºæ„Ÿå¹", "match_score": 10, "mismatch_score": -10},
    ],
    # 4.6 ååŠ¨è¯
    "ååŠ¨è¯": [
        {"name": "NV1_å¯è¢«\"ä¸/æ²¡æœ‰\"å¦å®šä¸”è‚¯å®šå½¢å¼-1", "desc": "å¯ä»¥ç”¨\"ä¸\"å’Œ\"æ²¡æœ‰\"æ¥å¦å®šï¼Œå¹¶ä¸”\"æ²¡æœ‰â€¦â€¦\"çš„è‚¯å®šå½¢å¼å¯ä»¥æ˜¯\"â€¦â€¦äº†\"å’Œ\"æœ‰â€¦â€¦\"(å‰ä¸€ç§æƒ…å†µä¸­çš„\"æ²¡æœ‰\"æ˜¯å‰¯è¯ï¼Œåä¸€ç§æƒ…å†µä¸­çš„\"æ²¡æœ‰\"æ˜¯åŠ¨è¯)", "match_score": 10, "mismatch_score": -10},
        {"name": "NV2_å¯é™„æ—¶ä½“åŠ©è¯æˆ–è¿›å…¥\"â€¦â€¦äº†æ²¡æœ‰\"æ ¼å¼", "desc": "å¯ä»¥åé™„æ—¶ä½“åŠ©è¯\"ç€ã€äº†ã€è¿‡\"ï¼Œæˆ–è€…å¯ä»¥è¿›å…¥\"â€¦â€¦â€¦äº†æ²¡æœ‰\"æ ¼å¼", "match_score": 10, "mismatch_score": -10},
        {"name": "NV3_å¯å¸¦çœŸå®¾è¯­ä¸”ä¸å—\"å¾ˆ\"ä¿®é¥°", "desc": "å¯ä»¥å¸¦çœŸå®¾è¯­ï¼Œå¹¶ä¸”ä¸èƒ½å—ç¨‹åº¦å‰¯è¯\"å¾ˆ\"ç­‰ä¿®é¥°", "match_score": 10, "mismatch_score": -10},
        {"name": "NV4_æœ‰é‡å å’Œæ­£åé‡å å½¢å¼", "desc": "å¯ä»¥æœ‰\"VVã€Vä¸€Vã€Väº†Vã€Vä¸V\"ç­‰é‡å å’Œæ­£åé‡å å½¢å¼", "match_score": 10, "mismatch_score": 0},
        {"name": "NV5_å¯ä½œå¤šç§å¥æ³•æˆåˆ†ä¸”å¯ä½œå½¢å¼åŠ¨è¯å®¾è¯­", "desc": "æ—¢å¯ä»¥ä½œè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒï¼Œåˆå¯ä»¥ä½œä¸»è¯­æˆ–å®¾è¯­ï¼Œå¹¶ä¸”ï¼Œå¯ä»¥ä½œå½¢å¼åŠ¨è¯\"ä½œã€è¿›è¡Œã€åŠ ä»¥ã€ç»™äºˆã€å—åˆ°\"ç­‰çš„å®¾è¯­", "match_score": 10, "mismatch_score": -10},
        {"name": "NV6_ä¸èƒ½ç›´æ¥ä½œçŠ¶è¯­", "desc": "ä¸èƒ½ç›´æ¥ä½œçŠ¶è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "match_score": 10, "mismatch_score": -10},
        {"name": "NV7_å¯ä¿®é¥°åè¯æˆ–å—åè¯/æ•°é‡è¯ä¿®é¥°", "desc": "å¯ä»¥ä¿®é¥°åè¯æˆ–è€…å—åè¯ä¿®é¥°ï¼Œæˆ–è€…å¯ä»¥å—æ•°é‡è¯ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "NV8_å¯è·Ÿåœ¨\"æ€ä¹ˆ/æ€æ ·/è¿™ä¹ˆ/è¿™æ ·/é‚£ä¹ˆ/é‚£æ ·\"ä¹‹å", "desc": "å¯ä»¥è·Ÿåœ¨\"æ€ä¹ˆã€æ€æ ·\"ä¹‹åï¼Œå¯¹åŠ¨ä½œçš„æ–¹å¼è¿›è¡Œæé—®ï¼Œå¹¶ä¸”å¯ä»¥è·Ÿåœ¨\"è¿™ä¹ˆã€è¿™æ ·ã€é‚£ä¹ˆã€é‚£æ ·\"ä¹‹åï¼Œç”¨ä»¥ä½œå‡ºç›¸åº”çš„å›ç­”", "match_score": 10, "mismatch_score": 0},
        {"name": "NV9_ä¸èƒ½è·Ÿåœ¨\"å¤š/å¤šä¹ˆ\"ä¹‹å", "desc": "ä¸èƒ½è·Ÿåœ¨\"å¤š\"ä¹‹åï¼Œå¯¹æ€§è´¨çš„ç¨‹åº¦è¿›è¡Œæé—®ï¼Œä¹Ÿä¸èƒ½è·Ÿåœ¨\"å¤šä¹ˆ\"ä¹‹åï¼Œè¡¨ç¤ºæ„Ÿå¹", "match_score": 10, "mismatch_score": -10},
        {"name": "NV10_å¯åé™„æ–¹ä½è¯æ„æˆå¤„æ‰€ç»“æ„", "desc": "å¯ä»¥åé™„æ–¹ä½è¯æ„æˆå¤„æ‰€ç»“æ„(ç„¶åä½œâ€œåœ¨ã€åˆ°ã€ä»â€ç­‰ä»‹è¯çš„å®¾è¯­ï¼Œè¿™ç§ä»‹è¯ç»“æ„åˆå¯ä»¥ä½œçŠ¶è¯­æˆ–è¡¥è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†)", "match_score": 10, "mismatch_score": 0},
    ]
}

# ===============================
# å·¥å…·å‡½æ•°
# ===============================
def extract_text_from_response(resp_json: Dict[str, Any]) -> str:
    """ä»ä¸åŒæ ¼å¼çš„LLMå“åº”ä¸­å®‰å…¨æå–æ–‡æœ¬å†…å®¹ã€‚"""
    if not isinstance(resp_json, dict):
        return ""
    try:
        if "output" in resp_json and "text" in resp_json["output"]:
            return resp_json["output"]["text"]
        
        if "choices" in resp_json and len(resp_json["choices"]) > 0:
            choice = resp_json["choices"][0]
            if "message" in choice and "content" in choice["message"]:
                return choice["message"]["content"]
            
        return json.dumps(resp_json, ensure_ascii=False)
    except Exception as e:
        st.error(f"æå–æ–‡æœ¬å¤±è´¥: {e}")
        return json.dumps(resp_json, ensure_ascii=False)

def extract_json_from_text(text: str) -> Tuple[Dict[str, Any], str]:
    """ä»åŒ…å«æ¨ç†è¿‡ç¨‹å’ŒJSONçš„æ··åˆæ–‡æœ¬ä¸­æå–å¹¶è§£æJSONå¯¹è±¡ã€‚"""
    # ä½¿ç”¨ re.DOTALL ç¡®ä¿ '.' åŒ¹é…æ¢è¡Œç¬¦
    match = re.search(r"(\{.*\})", text.strip(), re.DOTALL)
    
    if not match:
        return None, text

    json_text = match.group(1).strip()
    
    try:
        parsed_json = json.loads(json_text)
        return parsed_json, json_text
    except json.JSONDecodeError as e:
        # st.error(f"JSONè§£æå¤±è´¥: {e}. å°è¯•è§£æçš„æ–‡æœ¬ç‰‡æ®µ:\n{json_text}") # è°ƒè¯•ä¿¡æ¯ä¸åœ¨æœ€ç»ˆåº”ç”¨ä¸­æ˜¾ç¤º
        return None, json_text

def normalize_key(k: str, pos_rules: list) -> str:
    """æ ‡å‡†åŒ–æ¨¡å‹è¿”å›çš„è§„åˆ™åç§°ï¼Œç¡®ä¿åŒ¹é…åˆ° RULE_SETS ä¸­çš„é”®ã€‚"""
    if not isinstance(k, str): return None
    k_norm = re.sub(r'[\s_]+', '', k).upper()
    for r in pos_rules:
        r_norm = re.sub(r'[\s_]+', '', r["name"]).upper()
        if r_norm == k_norm:
            return r["name"]
    return None

def map_to_allowed_score(rule: dict, raw_val) -> int:
    """å°†æ¨¡å‹è¿”å›çš„å¸ƒå°”å€¼/å­—ç¬¦ä¸²æ˜ å°„ä¸ºè§„åˆ™å®šä¹‰çš„ match_score æˆ– mismatch_scoreã€‚"""
    match_score, mismatch_score = rule["match_score"], rule["mismatch_score"]
    
    if isinstance(raw_val, bool):
        return match_score if raw_val is True else mismatch_score
    
    if isinstance(raw_val, str):
        s = raw_val.strip().lower()
        if s in ("yes", "y", "true", "æ˜¯", "âˆš", "ç¬¦åˆ"):
            return match_score
        if s in ("no", "n", "false", "å¦", "Ã—", "ä¸ç¬¦åˆ"):
            return mismatch_score
            
    if isinstance(raw_val, (int, float)):
        raw_val_int = int(raw_val)
        if raw_val_int == match_score: return match_score
        if raw_val_int == mismatch_score: return mismatch_score
    
    # é»˜è®¤è¿”å›ä¸åŒ¹é…å¾—åˆ†
    return mismatch_score

def calculate_membership(scores_all: Dict[str, Dict[str, int]]) -> Dict[str, float]:
    """è®¡ç®—éš¶å±åº¦ï¼šæ€»åˆ†é™¤ä»¥ 100ï¼Œå¹¶é™åˆ¶åœ¨ [-1, 1] åŒºé—´ã€‚"""
    membership = {}
    for pos, scores in scores_all.items():
        total_score = sum(scores.values())
        normalized = total_score / 100
        membership[pos] = max(-1.0, min(1.0, normalized))
    return membership

def get_top_10_positions(membership: Dict[str, float]) -> List[Tuple[str, float]]:
    """è·å–éš¶å±åº¦æœ€é«˜çš„å‰ 10 ä¸ªè¯ç±»ã€‚"""
    return sorted(membership.items(), key=lambda x: x[1], reverse=True)[:10]

# ===============================
# å®‰å…¨çš„ LLM è°ƒç”¨å‡½æ•° (å¢åŠ è¶…æ—¶)
# ===============================
def call_llm_api_cached(_provider, _model, _api_key, messages, max_tokens=4096, temperature=0.0):
    """å°è£…è¯·æ±‚é€»è¾‘ï¼Œå¢åŠ è¶…æ—¶å¤„ç†å’Œé”™è¯¯ä¿¡æ¯æå–ã€‚"""
    if not _api_key: return False, {"error": "API Key ä¸ºç©º"}, "API Key æœªæä¾›"
    if _provider not in MODEL_CONFIGS: return False, {"error": f"æœªçŸ¥æä¾›å•† {_provider}"}, f"æœªçŸ¥æä¾›å•† {_provider}"

    cfg = MODEL_CONFIGS[_provider]
    url = f"{cfg['base_url'].rstrip('/')}{cfg['endpoint']}"
    headers = cfg["headers"](_api_key)
    payload = cfg["payload"](_model, messages, max_tokens=max_tokens, temperature=temperature)

    try:
        # å¢åŠ è¶…æ—¶è®¾ç½®åˆ°120ç§’
        response = requests.post(url, headers=headers, json=payload, timeout=120)
        response.raise_for_status()
        return True, response.json(), ""
    except requests.exceptions.Timeout:
        error_msg = "è¯·æ±‚è¶…æ—¶ã€‚æ¨¡å‹å¯èƒ½æ­£å¿™æˆ–ç½‘ç»œè¿æ¥è¾ƒæ…¢ã€‚"
        return False, {"error": error_msg}, error_msg
    except requests.exceptions.RequestException as e:
        error_msg = f"APIè¯·æ±‚å¤±è´¥: {str(e)}"
        if hasattr(e, 'response') and e.response is not None:
            try:
                error_details = e.response.json()
                if 'error' in error_details:
                    detail_msg = error_details['error'].get('message') or json.dumps(error_details['error'])
                    error_msg += f" è¯¦æƒ…: {detail_msg}"
                else:
                     error_msg += f" å“åº”å†…å®¹: {e.response.text[:200]}..." 
            except:
                error_msg += f" å“åº”å†…å®¹: {e.response.text[:200]}..." 
        return False, {"error": error_msg}, error_msg
    except Exception as e:
        error_msg = f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}"
        return False, {"error": error_msg}, error_msg

# ===============================
# è¯ç±»åˆ¤å®šä¸»å‡½æ•° (é’ˆå¯¹å•ä¸ªè¯è¯­)
# ===============================
def ask_model_for_pos_and_scores(word: str, provider: str, model: str, api_key: str) -> Dict[str, Any]:
    """
    å¯¹å•ä¸ªè¯è¯­è¿›è¡Œåˆ†æï¼Œå¹¶è¿”å›åŒ…å«æ‰€æœ‰ç»“æœçš„å­—å…¸ã€‚
    """
    if not word:
        return {"word": "", "error": "è¯è¯­ä¸ºç©º", "scores_all": {}, "predicted_pos": "æœªçŸ¥", "explanation": ""}

    # è§„åˆ™æ–‡å­—è¯´æ˜ï¼ˆç»™æ¨¡å‹çœ‹ï¼‰
    full_rules_by_pos = {
        pos: "\n".join([
            f"- {r['name']}: {r['desc']}ï¼ˆç¬¦åˆ: {r['match_score']} åˆ†ï¼Œä¸ç¬¦åˆ: {r['mismatch_score']} åˆ†ï¼‰"
            for r in rules
        ])
        for pos, rules in RULE_SETS.items()
    }

    # ===== ç³»ç»Ÿæç¤ºï¼šåªå…è®¸è¾“å‡ºâ€œç¬¦åˆ/ä¸ç¬¦åˆâ€ï¼Œç¦æ­¢è‡ªå·±æ‰“æ•°å­—åˆ† =====
    system_msg = f"""ä½ æ˜¯ä¸€åä¸­æ–‡è¯æ³•ä¸è¯­æ³•æ–¹é¢çš„ä¸“å®¶ã€‚ç°åœ¨è¦åˆ†æè¯è¯­ã€Œ{word}ã€åœ¨ä¸‹åˆ—è¯ç±»ä¸­çš„è¡¨ç°ï¼š

- éœ€è¦åˆ¤æ–­çš„è¯ç±»ï¼šåè¯ã€åŠ¨è¯ã€ååŠ¨è¯
- è¯„åˆ†è§„åˆ™å·²ç»ç”±ç³»ç»Ÿå®šä¹‰ï¼Œä½ **ä¸è¦**è‡ªå·±è®¾è®¡åˆ†å€¼ï¼Œä¹Ÿ**ä¸è¦**åœ¨ JSON ä¸­ç»™å‡ºå…·ä½“æ•°å­—åˆ†æ•°ã€‚
- ä½ åªéœ€è¦åˆ¤æ–­æ¯ä¸€æ¡è§„åˆ™æ˜¯â€œç¬¦åˆâ€è¿˜æ˜¯â€œä¸ç¬¦åˆâ€ã€‚

ã€å„è¯ç±»çš„è§„åˆ™è¯´æ˜ï¼ˆä»…ä¾›ä½ åˆ¤æ–­ä½¿ç”¨ï¼‰ã€‘

ã€åè¯ã€‘
{full_rules_by_pos["åè¯"]}

ã€åŠ¨è¯ã€‘
{full_rules_by_pos["åŠ¨è¯"]}

ã€ååŠ¨è¯ã€‘
{full_rules_by_pos["ååŠ¨è¯"]}

ã€è¾“å‡ºè¦æ±‚ã€‘
1. åœ¨ explanation å­—æ®µä¸­ï¼Œå¿…é¡»**é€æ¡è§„åˆ™**è¯´æ˜åˆ¤æ–­ä¾æ®ï¼Œå¹¶ä¸¾ä¾‹ï¼ˆå¯ä»¥è‡ªå·±é€ å¥ï¼‰ã€‚
2. åœ¨ JSON ä¸­çš„ scores å­—æ®µé‡Œï¼Œæ¯ä¸€ç±»ä¸‹çš„æ¯ä¸€æ¡è§„åˆ™ï¼Œåªèƒ½ç»™å‡º **å¸ƒå°”å€¼ true / false**ã€‚
3. predicted_posï¼šè¯·é€‰æ‹©ã€Œåè¯ã€ã€ŒåŠ¨è¯ã€ã€ŒååŠ¨è¯ã€ä¹‹ä¸€ï¼Œä½œä¸ºè¯¥è¯è¯­æœ€å…¸å‹çš„è¯ç±»ã€‚
4. **æœ€åè¾“å‡ºæ—¶ï¼Œå…ˆå†™è¯¦ç»†çš„æ–‡å­—æ¨ç†ï¼Œæœ€åå•ç‹¬ä¸”å®Œæ•´åœ°ç»™å‡ºä¸€æ®µåˆæ³•çš„ JSONï¼ˆä¸è¦å†åŠ æ³¨é‡Šï¼‰ã€‚**
"""
    user_prompt = f"""è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è¦æ±‚åˆ†æè¯è¯­ã€Œ{word}ã€ã€‚è¯·å…ˆç»™å‡ºè¯¦ç»†æ¨ç†è¿‡ç¨‹ï¼Œç„¶ååœ¨æœ€åå•ç‹¬è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ã€‚"""

    # è°ƒç”¨æ¨¡å‹
    ok, resp_json, err_msg = call_llm_api_cached(
        _provider=provider,
        _model=model,
        _api_key=api_key,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]
    )

    result = {"word": word, "error": "", "scores_all": {}, "predicted_pos": "æœªçŸ¥", "explanation": "", "raw_text": ""}

    if not ok:
        result["error"] = f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {err_msg}"
        return result

    raw_text = extract_text_from_response(resp_json)
    result["raw_text"] = raw_text
    parsed_json, _ = extract_json_from_text(raw_text)

    # è§£æ JSON å¹¶è½¬æ¢åˆ†æ•°
    if parsed_json and isinstance(parsed_json, dict):
        result["explanation"] = parsed_json.get("explanation", "æ¨¡å‹æœªæä¾›è¯¦ç»†æ¨ç†è¿‡ç¨‹ã€‚")
        result["predicted_pos"] = parsed_json.get("predicted_pos", "æœªçŸ¥")
        raw_scores = parsed_json.get("scores", {})
        
        scores_out = {pos: {} for pos in RULE_SETS.keys()}
        for pos, rules in RULE_SETS.items():
            raw_pos_scores = raw_scores.get(pos, {})
            if isinstance(raw_pos_scores, dict):
                for k, v in raw_pos_scores.items():
                    normalized_key = normalize_key(k, rules)
                    if normalized_key:
                        rule_def = next(r for r in rules if r["name"] == normalized_key)
                        scores_out[pos][normalized_key] = map_to_allowed_score(rule_def, v)
            
            # ä¿è¯æ¯æ¡è§„åˆ™éƒ½æœ‰å¾—åˆ†ï¼Œç¼ºå¤±é»˜è®¤ 0 åˆ†
            for rule in rules:
                if rule["name"] not in scores_out[pos]:
                    scores_out[pos][rule["name"]] = 0
        
        result["scores_all"] = scores_out
        
    else:
        result["error"] = "æœªèƒ½ä»æ¨¡å‹å“åº”ä¸­è§£æå‡ºæœ‰æ•ˆçš„JSONã€‚"
        result["explanation"] = "æ— æ³•è§£ææ¨¡å‹è¾“å‡ºã€‚åŸå§‹å“åº”ï¼š\n" + raw_text

    return result

# ===============================
# æ‰¹é‡å¤„ç†å‡½æ•°ï¼ˆåˆ©ç”¨å¹¶å‘ï¼‰
# ===============================
def process_batch(words: List[str], model_info: Dict[str, Any], max_workers: int = 5) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨ ThreadPoolExecutor å¹¶å‘å¤„ç†å¤šä¸ªè¯è¯­ã€‚
    """
    if not words:
        return []

    results = []
    
    # ä½¿ç”¨ ThreadPoolExecutor æ¥ç®¡ç†å¹¶å‘çº¿ç¨‹
    # max_workers = 5 æ˜¯ä¸€ä¸ªåˆç†çš„é»˜è®¤å€¼ï¼Œé˜²æ­¢APIé™é€Ÿ
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ¯ä¸ªè¯è¯­çš„åˆ†æä»»åŠ¡
        futures = {
            executor.submit(
                ask_model_for_pos_and_scores, 
                word.strip(), 
                model_info["provider"], 
                model_info["model"], 
                model_info["api_key"]
            ): word 
            for word in words if word.strip()
        }
        
        # è·å–ç»“æœï¼Œä¿æŒæäº¤çš„é¡ºåº
        for future in futures:
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                # æ•è·çº¿ç¨‹æ‰§è¡Œä¸­çš„æ„å¤–é”™è¯¯
                results.append({"word": "æœªçŸ¥", "error": f"å¹¶å‘æ‰§è¡Œå‘ç”Ÿå¼‚å¸¸: {e}", "scores_all": {}, "predicted_pos": "æœªçŸ¥", "explanation": ""})
                
    return results

# ===============================
# é›·è¾¾å›¾
# ===============================
def plot_radar_chart_streamlit(scores_norm: Dict[str, float], title: str):
    if not scores_norm:
        st.warning("æ— æ³•ç»˜åˆ¶é›·è¾¾å›¾ï¼šæ²¡æœ‰æœ‰æ•ˆæ•°æ®ã€‚")
        return
        
    # åªå–å‰ä¸‰ä¸ªè¯ç±»ï¼ˆåè¯ã€åŠ¨è¯ã€ååŠ¨è¯ï¼‰ç»˜åˆ¶é›·è¾¾å›¾
    relevant_pos = {k: scores_norm[k] for k in ["åè¯", "åŠ¨è¯", "ååŠ¨è¯"] if k in scores_norm}
    
    categories = list(relevant_pos.keys())
    values = list(relevant_pos.values())
    
    if not categories:
        st.warning("æ— æ³•ç»˜åˆ¶é›·è¾¾å›¾ï¼šæ²¡æœ‰åˆ†ææ‰€éœ€çš„è¯ç±»æ•°æ®ã€‚")
        return
        
    # é—­åˆé›·è¾¾å›¾
    categories += [categories[0]]
    values += [values[0]]
    
    min_val = min(values)
    max_val = max(values)
    axis_min = min(min_val, -0.1) 
    axis_max = max(max_val, 1.0)
    
    fig = go.Figure(data=[
        go.Scatterpolar(
            r=values, 
            theta=categories, 
            fill="toself", 
            name="éš¶å±åº¦",
            hovertemplate = '<b>%{theta}</b><br>éš¶å±åº¦: %{r:.4f}<extra></extra>'
        )
    ])
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True, 
                range=[axis_min, axis_max],
                tickvals=[-1.0, -0.5, 0, 0.5, 1.0] 
            )
        ),
        showlegend=False,
        title=dict(text=title, x=0.5, font=dict(size=16))
    )
    st.plotly_chart(fig, use_container_width=True)

# ===============================
# ä¸»é¡µé¢é€»è¾‘
# ===============================
def main():
    st.title("ğŸ“° æ±‰è¯­è¯ç±»éš¶å±åº¦æ£€æµ‹åˆ’ç±»")
    
    # --- é¡¶éƒ¨å›ºå®šæ§åˆ¶åŒº ---
    control_container = st.container()
    with control_container:
        col1, col2, col3 = st.columns([2, 1, 3])
        
        with col1:
            st.subheader("âš™ï¸ æ¨¡å‹è®¾ç½®")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨æ¨¡å‹
            if not AVAILABLE_MODEL_OPTIONS:
                st.error("âŒ æ‰¾ä¸åˆ°å¯ç”¨çš„ API Keyï¼è¯·è®¾ç½®ç¯å¢ƒå˜é‡æ¥å¯ç”¨æ¨¡å‹ã€‚")
                for name, info in MODEL_OPTIONS.items():
                     st.code(f"export {info['env_var']}='ä½ çš„API Key'", language="bash")
                
                selected_model_display_name = list(MODEL_OPTIONS.keys())[0] 
                selected_model_info = MODEL_OPTIONS[selected_model_display_name]
                st.selectbox("é€‰æ‹©å¤§æ¨¡å‹ (ä¸å¯ç”¨)", list(MODEL_OPTIONS.keys()), disabled=True)
            else:
                selected_model_display_name = st.selectbox(
                    "é€‰æ‹©å¤§æ¨¡å‹", 
                    list(AVAILABLE_MODEL_OPTIONS.keys()), 
                    key="model_select"
                )
                selected_model_info = AVAILABLE_MODEL_OPTIONS[selected_model_display_name]
                
        
        with col2:
            st.subheader("ğŸ”— è¿æ¥æµ‹è¯•")
            if not selected_model_info.get("api_key"):
                st.button("æµ‹è¯•æ¨¡å‹é“¾æ¥ (ä¸å¯ç”¨)", type="secondary", disabled=True)
            else:
                if st.button("æµ‹è¯•æ¨¡å‹é“¾æ¥", type="secondary"):
                    with st.spinner("æ­£åœ¨æµ‹è¯•è¿æ¥..."):
                        ok, _, err_msg = call_llm_api_cached(
                            _provider=selected_model_info["provider"],
                            _model=selected_model_info["model"],
                            _api_key=selected_model_info["api_key"],
                            messages=[{"role": "user", "content": "è¯·å›å¤'pong'"}],
                            max_tokens=10
                        )
                    if ok:
                        st.success("âœ… æ¨¡å‹é“¾æ¥æµ‹è¯•æˆåŠŸï¼")
                    else:
                        st.error(f"âŒ æ¨¡å‹é“¾æ¥æµ‹è¯•å¤±è´¥: {err_msg}")

        with col3:
            st.subheader("ğŸ”¤ è¯è¯­è¾“å…¥ (æ”¯æŒæ‰¹é‡)")
            
            # æ›´æ”¹ä¸º text_area æ”¯æŒæ‰¹é‡è¾“å…¥
            words_input = st.text_area(
                "è¯·è¾“å…¥è¦åˆ†æçš„æ±‰è¯­è¯è¯­ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰", 
                placeholder="ä¾‹å¦‚ï¼š\nè‹¹æœ\nè·‘\nç¾ä¸½", 
                key="words_input", 
                height=150
            )
            
            words_list = [w.strip() for w in words_input.split('\n') if w.strip()]
            
            # å¼€å§‹åˆ†ææŒ‰é’®ï¼ˆAPI Keyä¸ºç©ºæˆ–è¯è¯­ä¸ºç©ºæ—¶ç¦ç”¨ï¼‰
            analyze_button = st.button(
                f"ğŸš€ å¼€å§‹åˆ†æ ({len(words_list)}ä¸ªè¯)", 
                type="primary",
                disabled=not (selected_model_info.get("api_key") and words_list)
            )

    st.markdown("---")
    
     # --- ç»“æœæ˜¾ç¤ºåŒº ---
    if analyze_button and words_list and selected_model_info.get("api_key"):
        status_placeholder = st.empty()
        status_placeholder.info(f"æ­£åœ¨ä½¿ç”¨å¹¶å‘å¤„ç† **{len(words_list)}** ä¸ªè¯è¯­ï¼Œè¯·ç¨å€™...")
        
        # è¿è¡Œå¹¶å‘æ‰¹é‡å¤„ç†
        results = process_batch(words_list, selected_model_info, max_workers=5)
        
        status_placeholder.empty()
        st.success(f'**æ‰¹é‡åˆ†æå®Œæˆ**ï¼šå…±å¤„ç† **{len(words_list)}** ä¸ªè¯è¯­ã€‚')
        
        # è¿­ä»£å¹¶æ˜¾ç¤ºæ¯ä¸ªè¯è¯­çš„ç»“æœ
        for result in results:
            word = result["word"]
            error = result["error"]
            scores_all = result["scores_all"]
            predicted_pos = result["predicted_pos"]
            explanation = result["explanation"]
            raw_text = result["raw_text"]
            
            st.markdown(f"## ğŸ” è¯è¯­åˆ†æç»“æœï¼š ã€Œ{word}ã€")

            if error:
                st.error(f"åˆ†æå¤±è´¥: {error}")
                with st.expander("åŸå§‹å“åº”", expanded=False):
                    st.code(raw_text, language="text")
                st.markdown("---")
                continue

            membership = calculate_membership(scores_all)
            final_membership = membership.get(predicted_pos, 0)
            
            st.info(f'**é¢„æµ‹è¯ç±»**ï¼š **ã€{predicted_pos}ã€‘**ï¼Œéš¶å±åº¦ä¸º **{final_membership:.4f}**')
            
            col_results_1, col_results_2 = st.columns(2)
            
            with col_results_1:
                st.subheader("ğŸ’¡ æ¨¡å‹è¯¦ç»†æ¨ç†è¿‡ç¨‹")
                st.markdown(explanation)
                st.markdown("---")
                
                st.subheader("ğŸ“Š è¯ç±»éš¶å±åº¦é›·è¾¾å›¾")
                # åªæ˜¾ç¤ºåè¯ã€åŠ¨è¯ã€ååŠ¨è¯çš„éš¶å±åº¦
                plot_radar_chart_streamlit(membership, f"ã€Œ{word}ã€çš„è¯ç±»éš¶å±åº¦åˆ†å¸ƒ")
                
            with col_results_2:
                st.subheader("ğŸ“‹ å„è¯ç±»è¯¦ç»†å¾—åˆ†")
                
                pos_total_scores = {pos: sum(scores_all[pos].values()) for pos in RULE_SETS.keys()}
                sorted_pos_names = sorted(pos_total_scores.keys(), key=lambda pos: pos_total_scores[pos], reverse=True)
                
                for pos in sorted_pos_names:
                    total_score = pos_total_scores[pos]
                    max_rule = max(scores_all[pos].items(), key=lambda x: x[1], default=("æ— ", 0))
                    
                    with st.expander(f"**{pos}** (æ€»åˆ†: {total_score}, æœ€é«˜åˆ†è§„åˆ™: {max_rule[0]} - {max_rule[1]}åˆ†)"):
                        rule_data = []
                        for rule in RULE_SETS[pos]:
                            rule_score = scores_all[pos][rule["name"]]
                            rule_data.append({
                                "è§„åˆ™ä»£ç ": rule["name"],
                                "è§„åˆ™æè¿°": rule["desc"],
                                "å¾—åˆ†": rule_score
                            })
                        
                        rule_data_sorted = sorted(rule_data, key=lambda x: x["å¾—åˆ†"], reverse=True)
                        rule_df = pd.DataFrame(rule_data_sorted)
                        
                        styled_df = rule_df.style.applymap(
                            lambda x: "color: #ff4b4b; font-weight: bold" if isinstance(x, int) and x < 0 else "",
                            subset=["å¾—åˆ†"]
                        )
                        
                        st.dataframe(
                            styled_df,
                            use_container_width=True,
                            height=min(len(rule_df) * 30 + 50, 400) 
                        )
                
                st.subheader("ğŸ“¥ æ¨¡å‹åŸå§‹å“åº”")
                with st.expander("ç‚¹å‡»å±•å¼€æŸ¥çœ‹åŸå§‹å“åº”", expanded=False):
                    st.code(raw_text, language="json")

            st.markdown("---") # ç»“æœåˆ†éš”çº¿
            
# ===============================
# è¿è¡Œä¸»å‡½æ•°
# ===============================
if __name__ == "__main__":
    main()
# ===============================
# é¡µé¢åº•éƒ¨è¯´æ˜
# ===============================
st.markdown("---")
st.markdown(
    "<div style='text-align:center; color:#666;'>"
    "Â© 2025 æ±‰è¯­è¯ç±»éš¶å±åº¦æ£€æµ‹åˆ’ç±»  "
    "</div>",
    unsafe_allow_html=True
)
