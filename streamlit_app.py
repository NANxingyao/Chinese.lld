import streamlit as st
import requests
import json
import re
import os
import pandas as pd
import plotly.graph_objects as go
import io
from typing import Tuple, Dict, Any, List
from openpyxl import load_workbook
from openpyxl.styles import PatternFill

# ===============================
# é¡µé¢é…ç½®
# ===============================
st.set_page_config(
    page_title="æ±‰è¯­è¯ç±»éš¶å±åº¦æ£€æµ‹åˆ’ç±» (ä¿®å¤ç‰ˆ)",
    page_icon="ğŸ“°",
    layout="wide",
    initial_sidebar_state="collapsed",
    menu_items=None
)

# è‡ªå®šä¹‰CSSæ ·å¼
hide_streamlit_style = """
<style>
header {visibility: hidden;}
footer {visibility: hidden;}
.dataframe {font-size: 12px;}
[data-testid="stSidebar"] { display: none !important; }
.stApp > div:first-child { padding-top: 2rem; }
/* å¢åŠ åŸå§‹å“åº”æ–‡æœ¬åŒºåŸŸçš„é«˜åº¦ */
.stCode { max-height: 400px; overflow-y: auto; }
</style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# ===============================
# æ¨¡å‹é…ç½®
# ===============================
MODEL_CONFIGS = {
    "deepseek": {
        "base_url": "[https://api.deepseek.com/v1](https://api.deepseek.com/v1)",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": kw.get("max_tokens", 4096), 
            "temperature": kw.get("temperature", 0.0), 
            "stream": True, 
        },
    },
    "openai": {
        "base_url": "[https://api.openai.com/v1](https://api.openai.com/v1)",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": kw.get("max_tokens", 4096), 
            "temperature": kw.get("temperature", 0.0), 
            "stream": True,
        },
    },
    "moonshot": {
        "base_url": "[https://api.moonshot.cn/v1](https://api.moonshot.cn/v1)",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": kw.get("max_tokens", 4096), 
            "temperature": kw.get("temperature", 0.0), 
            "stream": True,
        },
    },
    "qwen": {
        "base_url": "[https://dashscope.aliyuncs.com/api/v1](https://dashscope.aliyuncs.com/api/v1)",
        "endpoint": "/services/aigc/text-generation/generation",
        "headers": lambda key: {
            "Authorization": f"Bearer {key}", 
            "Content-Type": "application/json",
            "X-DashScope-SSE": "enable",
            "Accept": "text/event-stream"
        },
        "payload": lambda model, messages, **kw: {
            "model": model, 
            "input": {"messages": messages}, 
            "parameters": {
                "max_tokens": kw.get("max_tokens", 4096), 
                "temperature": kw.get("temperature", 0.0),
                "result_format": "message",
                "incremental_output": True 
            },
        },
    },
}

MODEL_OPTIONS = {
    "DeepSeek Chat": {"provider": "deepseek", "model": "deepseek-chat", "api_key": os.getenv("DEEPSEEK_API_KEY"), "env_var": "DEEPSEEK_API_KEY"},
    "OpenAI GPT-4o-mini": {"provider": "openai", "model": "gpt-4o-mini", "api_key": os.getenv("OPENAI_API_KEY"), "env_var": "OPENAI_API_KEY"},
    "Moonshot (Kimi)": {"provider": "moonshot", "model": "moonshot-v1-32k", "api_key": os.getenv("MOONSHOT_API_KEY"), "env_var": "MOONSHOT_API_KEY"},
    "Qwen Turbo": {"provider": "qwen", "model": "qwen-turbo", "api_key": os.getenv("QWEN_API_KEY"), "env_var": "QWEN_API_KEY"},
    "Qwen Max": {"provider": "qwen", "model": "qwen-max", "api_key": os.getenv("QWEN_API_KEY"), "env_var": "QWEN_API_KEY"},
}

AVAILABLE_MODEL_OPTIONS = {name: info for name, info in MODEL_OPTIONS.items() if info["api_key"]}
if not AVAILABLE_MODEL_OPTIONS: AVAILABLE_MODEL_OPTIONS = MODEL_OPTIONS

# ===============================
# è§„åˆ™å®šä¹‰
# ===============================
RULE_SETS = {
    "åè¯": [
        {"name": "N1_å¯å—æ•°é‡è¯ä¿®é¥°", "desc": "å¯ä»¥å—æ•°é‡è¯ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "N2_ä¸èƒ½å—å‰¯è¯ä¿®é¥°", "desc": "ä¸èƒ½å—å‰¯è¯ä¿®é¥°", "match_score": 20, "mismatch_score": -20},
        {"name": "N3_å¯ä½œä¸»å®¾è¯­", "desc": "å¯ä»¥åšå…¸å‹çš„ä¸»è¯­æˆ–å®¾è¯­", "match_score": 20, "mismatch_score": 0},
        {"name": "N4_å¯ä½œä¸­å¿ƒè¯­æˆ–ä½œå®šè¯­", "desc": "å¯ä»¥åšä¸­å¿ƒè¯­å—å…¶ä»–åè¯ä¿®é¥°ï¼Œæˆ–è€…ä½œå®šè¯­ç›´æ¥ä¿®é¥°å…¶ä»–åè¯", "match_score": 10, "mismatch_score": 0},
        {"name": "N5_å¯åé™„çš„å­—ç»“æ„", "desc": "å¯ä»¥åé™„åŠ©è¯â€œçš„â€æ„æˆâ€œçš„â€å­—ç»“æ„", "match_score": 10, "mismatch_score": 0},
        {"name": "N6_å¯åé™„æ–¹ä½è¯æ„å¤„æ‰€", "desc": "å¯ä»¥åé™„æ–¹ä½è¯æ„æˆå¤„æ‰€ç»“æ„", "match_score": 10, "mismatch_score": 0},
        {"name": "N7_ä¸èƒ½ä½œè°“è¯­æ ¸å¿ƒ", "desc": "ä¸èƒ½åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒï¼ˆä¸èƒ½å¸¦å®¾è¯­ï¼Œä¸èƒ½å—çŠ¶è¯­å’Œè¡¥è¯­ï¼Œä¸èƒ½åé™„æ—¶ä½“åŠ©è¯ï¼‰", "match_score": 10, "mismatch_score": -10},
        {"name": "N8_ä¸èƒ½ä½œè¡¥è¯­/ä¸€èˆ¬ä¸ä½œçŠ¶è¯­", "desc": "ä¸èƒ½ä½œè¡¥è¯­ï¼Œå¹¶ä¸”ä¸€èˆ¬ä¸èƒ½åšçŠ¶è¯­ç›´æ¥ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "match_score": 10, "mismatch_score": 0},
    ],
    "åŠ¨è¯": [
        {"name": "V1_å¯å—å¦å®š'ä¸/æ²¡æœ‰'ä¿®é¥°", "desc": "å¯ä»¥å—å¦å®šå‰¯è¯'ä¸'æˆ–'æ²¡æœ‰'ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "V2_å¯åé™„/æ’å…¥æ—¶ä½“åŠ©è¯'ç€/äº†/è¿‡'", "desc": "å¯ä»¥åé™„æˆ–ä¸­é—´æ’å…¥æ—¶ä½“åŠ©è¯'ç€/äº†/è¿‡'ï¼Œæˆ–è¿›å…¥'...äº†æ²¡æœ‰'æ ¼å¼", "match_score": 10, "mismatch_score": 0},
        {"name": "V3_å¯å¸¦çœŸå®¾è¯­æˆ–é€šè¿‡ä»‹è¯å¼•å¯¼è®ºå…ƒ", "desc": "å¯ä»¥å¸¦çœŸå®¾è¯­ï¼Œæˆ–é€šè¿‡'å’Œ/ä¸º/å¯¹/å‘/æ‹¿/äº'ç­‰ä»‹è¯å¼•å¯¼è®ºå…ƒ", "match_score": 20, "mismatch_score": 0},
        {"name": "V4_ç¨‹åº¦å‰¯è¯ä¸å¸¦å®¾è¯­çš„å…³ç³»", "desc": "ä¸èƒ½å—ç¨‹åº¦å‰¯è¯'å¾ˆ'ä¿®é¥°ï¼Œæˆ–èƒ½åŒæ—¶å—'å¾ˆ'ä¿®é¥°å¹¶å¸¦å®¾è¯­", "match_score": 10, "mismatch_score": -10},
        {"name": "V5_å¯æœ‰é‡å /æ­£åé‡å å½¢å¼", "desc": "å¯ä»¥æœ‰'VV, Vä¸€V, Väº†V, Vä¸V, Väº†æ²¡æœ‰'ç­‰å½¢å¼", "match_score": 10, "mismatch_score": 0},
        {"name": "V6_å¯åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒ", "desc": "å¯ä»¥åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒ", "match_score": 10, "mismatch_score": -10},
        {"name": "V7_ä¸èƒ½ä½œçŠ¶è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "desc": "ä¸èƒ½ä½œçŠ¶è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "match_score": 10, "mismatch_score": 0},
        {"name": "V8_å¯ä½œ'æ€ä¹ˆ/æ€æ ·'æé—®æˆ–'è¿™ä¹ˆ/è¿™æ ·/é‚£ä¹ˆ'å›ç­”", "desc": "å¯ä»¥è·Ÿåœ¨'æ€ä¹ˆ/æ€æ ·'ä¹‹åæé—®æˆ–è·Ÿåœ¨'è¿™ä¹ˆ/è¿™æ ·/é‚£ä¹ˆ'ä¹‹åå›ç­”", "match_score": 10, "mismatch_score": 0},
        {"name": "V9_ä¸èƒ½è·Ÿåœ¨'å¤š/å¤šä¹ˆ'ä¹‹åæé—®æˆ–è¡¨ç¤ºæ„Ÿå¹", "desc": "ä¸èƒ½è·Ÿåœ¨'å¤š'ä¹‹åå¯¹æ€§è´¨æé—®ï¼Œä¸èƒ½è·Ÿåœ¨'å¤šä¹ˆ'ä¹‹åè¡¨ç¤ºæ„Ÿå¹", "match_score": 10, "mismatch_score": -10},
    ],
    "ååŠ¨è¯": [
        {"name": "NV1_å¯è¢«\"ä¸/æ²¡æœ‰\"å¦å®šä¸”è‚¯å®šå½¢å¼-1", "desc": "å¯ä»¥ç”¨\"ä¸\"å’Œ\"æ²¡æœ‰\"æ¥å¦å®šï¼Œè‚¯å®šå½¢å¼å¯ä»¥æ˜¯\"â€¦â€¦äº†\"å’Œ\"æœ‰â€¦â€¦\"", "match_score": 10, "mismatch_score": -10},
        {"name": "NV2_å¯é™„æ—¶ä½“åŠ©è¯æˆ–è¿›å…¥\"â€¦â€¦äº†æ²¡æœ‰\"æ ¼å¼", "desc": "å¯ä»¥åé™„æ—¶ä½“åŠ©è¯\"ç€ã€äº†ã€è¿‡\"ï¼Œæˆ–è€…å¯ä»¥è¿›å…¥\"â€¦â€¦â€¦äº†æ²¡æœ‰\"æ ¼å¼", "match_score": 10, "mismatch_score": -10},
        {"name": "NV3_å¯å¸¦çœŸå®¾è¯­ä¸”ä¸å—\"å¾ˆ\"ä¿®é¥°", "desc": "å¯ä»¥å¸¦çœŸå®¾è¯­ï¼Œå¹¶ä¸”ä¸èƒ½å—ç¨‹åº¦å‰¯è¯\"å¾ˆ\"ç­‰ä¿®é¥°", "match_score": 10, "mismatch_score": -10},
        {"name": "NV4_æœ‰é‡å å’Œæ­£åé‡å å½¢å¼", "desc": "æœ‰é‡å å’Œæ­£åé‡å å½¢å¼", "match_score": 10, "mismatch_score": 0},
        {"name": "NV5_å¯ä½œå¤šç§å¥æ³•æˆåˆ†ä¸”å¯ä½œå½¢å¼åŠ¨è¯å®¾è¯­", "desc": "æ—¢å¯ä»¥ä½œè°“è¯­ï¼Œåˆå¯ä»¥ä½œä¸»è¯­æˆ–å®¾è¯­ï¼Œä¸”å¯ä½œå½¢å¼åŠ¨è¯å®¾è¯­", "match_score": 10, "mismatch_score": -10},
        {"name": "NV6_ä¸èƒ½ç›´æ¥ä½œçŠ¶è¯­", "desc": "ä¸èƒ½ç›´æ¥ä½œçŠ¶è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "match_score": 10, "mismatch_score": -10},
        {"name": "NV7_å¯ä¿®é¥°åè¯æˆ–å—åè¯/æ•°é‡è¯ä¿®é¥°", "desc": "å¯ä»¥ä¿®é¥°åè¯æˆ–è€…å—åè¯ä¿®é¥°ï¼Œæˆ–è€…å¯ä»¥å—æ•°é‡è¯ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "NV8_å¯è·Ÿåœ¨\"æ€ä¹ˆ/æ€æ ·/è¿™ä¹ˆ/è¿™æ ·/é‚£ä¹ˆ/é‚£æ ·\"ä¹‹å", "desc": "å¯ä»¥è·Ÿåœ¨\"æ€ä¹ˆã€æ€æ ·\"ä¹‹åæé—®ï¼Œè·Ÿåœ¨\"è¿™ä¹ˆ\"ä¹‹åå›ç­”", "match_score": 10, "mismatch_score": 0},
        {"name": "NV9_ä¸èƒ½è·Ÿåœ¨\"å¤š/å¤šä¹ˆ\"ä¹‹å", "desc": "ä¸èƒ½è·Ÿåœ¨\"å¤š/å¤šä¹ˆ\"ä¹‹å", "match_score": 10, "mismatch_score": -10},
        {"name": "NV10_å¯åé™„æ–¹ä½è¯æ„æˆå¤„æ‰€ç»“æ„", "desc": "å¯ä»¥åé™„æ–¹ä½è¯æ„æˆå¤„æ‰€ç»“æ„", "match_score": 10, "mismatch_score": 0},
    ]
}

# ===============================
# å·¥å…·å‡½æ•° (é‡ç‚¹ä¿®å¤äº† JSON è§£æ)
# ===============================
def extract_text_from_response(resp_json: Dict[str, Any]) -> str:
    """ä»APIå“åº”ä¸­æå–æ–‡æœ¬"""
    if not isinstance(resp_json, dict): return ""
    try:
        # å…¼å®¹ OpenAI / DeepSeek / Moonshot
        if "choices" in resp_json and len(resp_json["choices"]) > 0:
            choice = resp_json["choices"][0]
            if "message" in choice and "content" in choice["message"]:
                return choice["message"]["content"]
        # å…¼å®¹ Qwen
        if "output" in resp_json and "text" in resp_json["output"]:
            return resp_json["output"]["text"]
        return json.dumps(resp_json, ensure_ascii=False)
    except Exception:
        return json.dumps(resp_json, ensure_ascii=False)

def extract_json_from_text(text: str) -> Tuple[Dict[str, Any], str]:
    """
    ã€ä¿®å¤ç‰ˆã€‘æ›´å¼ºå£®çš„ JSON æå–é€»è¾‘
    1. ä¼˜å…ˆå°è¯•æå– Markdown ä»£ç å— ```json ... ```
    2. å¦‚æœå¤±è´¥ï¼Œå°è¯•æå–æœ€å¤–å±‚çš„ { ... }
    """
    if not text:
        return None, ""
        
    json_str = ""
    
    # ç­–ç•¥ 1: å¯»æ‰¾ Markdown ä»£ç å—
    code_block_match = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
    if code_block_match:
        json_str = code_block_match.group(1).strip()
    
    # ç­–ç•¥ 2: å¦‚æœæ²¡ä»£ç å—ï¼Œå¯»æ‰¾æœ€å¤–å±‚å¤§æ‹¬å·
    if not json_str:
        # è´ªå©ªåŒ¹é…ç¬¬ä¸€ä¸ª { åˆ° æœ€åä¸€ä¸ª }
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å¤„ç†å¯èƒ½çš„åµŒå¥—ï¼Œä½†ç®€å•çš„ regex åªèƒ½æ‰¾é¦–å°¾
        match = re.search(r"(\{.*\})", text.strip(), re.DOTALL)
        if match:
            json_str = match.group(1).strip()
            
    if not json_str:
        return None, text

    try:
        parsed_json = json.loads(json_str)
        return parsed_json, json_str
    except json.JSONDecodeError as e:
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å› Noneï¼Œä½†åœ¨ç•Œé¢ä¸Šæˆ‘ä»¬ä¼šæ˜¾ç¤ºåŸå§‹æ–‡æœ¬ä¾›è°ƒè¯•
        return None, text

def normalize_key(k: str, pos_rules: list) -> str:
    if not isinstance(k, str): return None
    k_norm = re.sub(r'[\s_]+', '', k).upper()
    for r in pos_rules:
        r_norm = re.sub(r'[\s_]+', '', r["name"]).upper()
        if r_norm == k_norm: return r["name"]
    return None

def map_to_allowed_score(rule: dict, raw_val) -> int:
    match_score, mismatch_score = rule["match_score"], rule["mismatch_score"]
    if isinstance(raw_val, bool): return match_score if raw_val is True else mismatch_score
    if isinstance(raw_val, str):
        s = raw_val.strip().lower()
        if s in ("yes", "y", "true", "æ˜¯", "âˆš", "ç¬¦åˆ"): return match_score
        if s in ("no", "n", "false", "å¦", "Ã—", "ä¸ç¬¦åˆ"): return mismatch_score
    return mismatch_score

def calculate_membership(scores_all: Dict[str, Dict[str, int]]) -> Dict[str, float]:
    membership = {}
    for pos, scores in scores_all.items():
        total_score = sum(scores.values())
        normalized = total_score / 100
        membership[pos] = max(-1.0, min(1.0, normalized))
    return membership

def get_top_10_positions(membership: Dict[str, float]) -> List[Tuple[str, float]]:
    return sorted(membership.items(), key=lambda x: x[1], reverse=True)[:10]

# ===============================
# API è°ƒç”¨ (æµå¼)
# ===============================
def call_llm_api_cached(_provider, _model, _api_key, messages, max_tokens=4096, temperature=0.0):
    if not _api_key: return False, {"error": "API Key ä¸ºç©º"}, "API Key æœªæä¾›"
    if _provider not in MODEL_CONFIGS: return False, {"error": f"æœªçŸ¥æä¾›å•† {_provider}"}, f"æœªçŸ¥æä¾›å•† {_provider}"

    cfg = MODEL_CONFIGS[_provider]
    url = f"{cfg['base_url'].rstrip('/')}{cfg['endpoint']}"
    headers = cfg["headers"](_api_key)
    payload = cfg["payload"](_model, messages, max_tokens=max_tokens, temperature=temperature)
    
    full_content = ""
    try:
        # è®¾ç½® stream=True é¿å…è¶…æ—¶
        with requests.post(url, headers=headers, json=payload, stream=True, timeout=120) as response:
            response.raise_for_status()
            for line in response.iter_lines():
                if not line: continue
                line_text = line.decode('utf-8').strip()
                
                # å¤„ç† SSE æ•°æ®
                if line_text.startswith("data:"): 
                    json_str = line_text[5:].strip()
                else: 
                    json_str = line_text
                
                if json_str == "[DONE]": break
                
                try:
                    chunk = json.loads(json_str)
                    delta_text = ""
                    
                    # æå–å†…å®¹
                    if "choices" in chunk and len(chunk["choices"]) > 0:
                        delta = chunk["choices"][0].get("delta", {})
                        delta_text = delta.get("content", "")
                    elif "output" in chunk: # Qwen
                        output = chunk["output"]
                        if "choices" in output and len(output["choices"]) > 0:
                             msg = output["choices"][0].get("message", {})
                             delta_text = msg.get("content", "")
                        elif "text" in output:
                             delta_text = output["text"]
                    
                    if delta_text:
                        full_content += delta_text
                except json.JSONDecodeError: continue
        
        # æ„é€ ä¼ªå®Œæ•´å“åº”
        mock_response = {"choices": [{"message": {"content": full_content}}], "output": {"text": full_content}}
        
        if not full_content: return False, {"error": "æ¨¡å‹æœªè¿”å›ä»»ä½•å†…å®¹"}, "æ¨¡å‹æ— å“åº”"
        return True, mock_response, ""

    except Exception as e:
        # å¦‚æœå‡ºé”™ä½†å·²ç»æ¥æ”¶äº†éƒ¨åˆ†å†…å®¹ï¼Œä¹Ÿå°è¯•è¿”å›
        if full_content:
            mock_response = {"choices": [{"message": {"content": full_content}}], "output": {"text": full_content}}
            return True, mock_response, f"æµå¼ä¸­æ–­: {str(e)}"
        return False, {"error": str(e)}, str(e)

# ===============================
# åˆ†æä¸»é€»è¾‘ (ä¿®å¤ Prompt å’Œè§£æå…œåº•)
# ===============================
def ask_model_for_pos_and_scores(word: str, provider: str, model: str, api_key: str) -> Tuple[Dict[str, Dict[str, int]], str, str, str]:
    if not word: return {}, "", "æœªçŸ¥", ""

    # ç®€åŒ–çš„è§„åˆ™æè¿°
    full_rules_by_pos = {
        pos: "\n".join([f"- {r['name']}: {r['desc']}" for r in rules])
        for pos, rules in RULE_SETS.items()
    }

    # ã€é‡è¦ä¿®æ”¹ã€‘Prompt å¼ºåˆ¶è¦æ±‚ detailed reasoning å¹¶ä¸”æ”¾åœ¨ JSON é‡Œï¼Œæˆ–è€…å…è®¸ JSON å¤–æ–‡æœ¬
    system_msg = f"""ä½ æ˜¯ä¸€åæ±‰è¯­è¯­è¨€å­¦ä¸“å®¶ã€‚
ä»»åŠ¡ï¼šåˆ†æè¯è¯­ã€Œ{word}ã€åœ¨ã€åè¯ã€‘ã€ã€åŠ¨è¯ã€‘ã€ã€ååŠ¨è¯ã€‘ä¸‰ä¸ªè¯ç±»ä¸‹çš„è¡¨ç°ã€‚

è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è¾“å‡ºæ ¼å¼ï¼š

1. é¦–å…ˆï¼Œä½ å¯ä»¥è¿›è¡Œç®€çŸ­çš„æ€ç»´é“¾åˆ†æã€‚
2. ç„¶åï¼Œå¿…é¡»è¾“å‡ºä¸€ä¸ª Markdown ä»£ç å— ```json ... ```ï¼Œå…¶ä¸­åŒ…å«åˆ†æç»“æœã€‚
3. JSON ç»“æ„å¿…é¡»åŒ…å«ï¼š
   - "explanation": (å­—ç¬¦ä¸²) è¿™é‡Œå¿…é¡»åŒ…å«è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆç¬¦åˆæˆ–ä¸ç¬¦åˆæŸäº›å…³é”®è§„åˆ™ã€‚ä¸è¦å¤ªç®€ç•¥ã€‚
   - "predicted_pos": (å­—ç¬¦ä¸²) "åè¯" / "åŠ¨è¯" / "ååŠ¨è¯"
   - "scores": (å¯¹è±¡) åŒ…å«ä¸‰ä¸ªè¯ç±»ä¸‹æ‰€æœ‰è§„åˆ™çš„å¸ƒå°”å€¼ (true/false)

è§„åˆ™åˆ—è¡¨ä¾›å‚è€ƒï¼š
ã€åè¯è§„åˆ™ã€‘
{full_rules_by_pos["åè¯"]}
ã€åŠ¨è¯è§„åˆ™ã€‘
{full_rules_by_pos["åŠ¨è¯"]}
ã€ååŠ¨è¯è§„åˆ™ã€‘
{full_rules_by_pos["ååŠ¨è¯"]}
"""
    user_prompt = f"è¯·åˆ†æã€Œ{word}ã€ã€‚è¯·ç¡®ä¿è¾“å‡ºåˆæ³•çš„ JSONã€‚"

    # è°ƒç”¨ API
    ok, resp_json, err_msg = call_llm_api_cached(provider, model, api_key, [{"role": "system", "content": system_msg}, {"role": "user", "content": user_prompt}])

    if not ok: return {}, f"è°ƒç”¨å¤±è´¥: {err_msg}", "æœªçŸ¥", f"è°ƒç”¨å¤±è´¥: {err_msg}"

    raw_text = extract_text_from_response(resp_json)
    
    # å°è¯•è§£æ JSON
    parsed_json, json_str = extract_json_from_text(raw_text)

    # ã€å…œåº•é€»è¾‘ã€‘
    if parsed_json and isinstance(parsed_json, dict):
        # ä¼˜å…ˆä½¿ç”¨ JSON é‡Œçš„ explanation
        explanation = parsed_json.get("explanation", "")
        # å¦‚æœ JSON é‡Œçš„ explanation å¤ªçŸ­ï¼Œä¸”åŸå§‹æ–‡æœ¬é‡Œæœ‰é JSON çš„éƒ¨åˆ†ï¼Œåˆ™å°è¯•æ‹¼æ¥
        if len(explanation) < 10 and len(raw_text) > len(json_str) + 20:
             explanation = raw_text.replace(json_str, "").replace("```json", "").replace("```", "").strip()
        
        # å¦‚æœè¿˜æ˜¯æ²¡å†…å®¹ï¼Œå°±ç”¨ raw_text
        if not explanation: explanation = raw_text

        predicted_pos = parsed_json.get("predicted_pos", "æœªçŸ¥")
        raw_scores = parsed_json.get("scores", {})
    else:
        # è§£æå®Œå…¨å¤±è´¥çš„æƒ…å†µ
        return {}, raw_text, "è§£æå¤±è´¥", raw_text  # æŠŠåŸå§‹æ–‡æœ¬å…¨éƒ¨å½“åš explanation è¿”å›

    # åˆ†æ•°è½¬æ¢é€»è¾‘
    scores_out = {pos: {} for pos in RULE_SETS.keys()}
    for pos, rules in RULE_SETS.items():
        raw_pos_scores = raw_scores.get(pos, {})
        if isinstance(raw_pos_scores, dict):
            for k, v in raw_pos_scores.items():
                normalized_key = normalize_key(k, rules)
                if normalized_key:
                    rule_def = next(r for r in rules if r["name"] == normalized_key)
                    scores_out[pos][normalized_key] = map_to_allowed_score(rule_def, v)
        # è¡¥å…¨ç¼ºå¤±
        for rule in rules:
            if rule["name"] not in scores_out[pos]: scores_out[pos][rule["name"]] = 0

    return scores_out, raw_text, predicted_pos, explanation

# ===============================
# Excel æ‰¹é‡å¤„ç†é€»è¾‘ (ä¿å­˜æ¨ç†è¿‡ç¨‹)
# ===============================
def process_and_style_excel(df, selected_model_info, target_col_name):
    output = io.BytesIO()
    processed_rows = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    total = len(df)

    for index, row in df.iterrows():
        word = str(row[target_col_name]).strip()
        status_text.text(f"æ­£åœ¨å¤„ç† ({index + 1}/{total}): {word}")
        
        # è°ƒç”¨æ¨¡å‹
        scores_all, raw_text, predicted_pos, explanation = ask_model_for_pos_and_scores(
            word=word,
            provider=selected_model_info["provider"],
            model=selected_model_info["model"],
            api_key=selected_model_info["api_key"]
        )
        
        # è®¡ç®—åˆ†æ•°
        membership = calculate_membership(scores_all) if scores_all else {}
        score_v = membership.get("åŠ¨è¯", 0.0)
        score_n = membership.get("åè¯", 0.0)
        score_nv = membership.get("ååŠ¨è¯", 0.0)
        
        # è®¡ç®—å·®å€¼ |åŠ¨-å|
        diff_val = round(abs(score_v - score_n), 4)
        
        # æ„é€ è¡Œæ•°æ® (åŒ…å«åŸå§‹å“åº”å’Œæ¨ç†è¯´æ˜)
        new_row = {
            "è¯è¯­": word,
            "åŠ¨è¯": score_v,
            "åè¯": score_n,
            "ååŠ¨è¯": score_nv,
            "å·®å€¼/è·ç¦»": diff_val,
            "åŸå§‹å“åº”": explanation if explanation and len(explanation) > 10 else raw_text, # ä¼˜å…ˆå±•ç¤ºæ¨ç†
            "_predicted_pos": predicted_pos
        }
        processed_rows.append(new_row)
        progress_bar.progress((index + 1) / total)

    # å¯¼å‡º
    result_df = pd.DataFrame(processed_rows)
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        cols = ["è¯è¯­", "åŠ¨è¯", "åè¯", "ååŠ¨è¯", "å·®å€¼/è·ç¦»", "åŸå§‹å“åº”"]
        result_df[cols].to_excel(writer, index=False, sheet_name='åˆ†æç»“æœ')
        
        worksheet = writer.sheets['åˆ†æç»“æœ']
        yellow_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
        
        for i, data_row in enumerate(processed_rows):
            row_num = i + 2 
            pred = data_row["_predicted_pos"]
            target_idx = None
            if pred == "åŠ¨è¯": target_idx = 2
            elif pred == "åè¯": target_idx = 3
            elif pred == "ååŠ¨è¯": target_idx = 4
            if target_idx:
                worksheet.cell(row=row_num, column=target_idx).fill = yellow_fill

    status_text.success("âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼")
    return output.getvalue()

# ===============================
# é›·è¾¾å›¾å·¥å…·
# ===============================
def plot_radar_chart_streamlit(scores_norm: Dict[str, float], title: str):
    if not scores_norm: return
    categories = list(scores_norm.keys())
    values = list(scores_norm.values())
    categories += [categories[0]]
    values += [values[0]]
    
    fig = go.Figure(data=[
        go.Scatterpolar(r=values, theta=categories, fill="toself", name="éš¶å±åº¦")
    ])
    fig.update_layout(
        polar=dict(radialaxis=dict(visible=True, range=[-1, 1])),
        showlegend=False,
        title=dict(text=title, x=0.5)
    )
    st.plotly_chart(fig, use_container_width=True)

# ===============================
# ä¸»ç•Œé¢
# ===============================
def main():
    st.title("ğŸ“° æ±‰è¯­è¯ç±»éš¶å±åº¦æ£€æµ‹ (ä¿®å¤ç‰ˆ)")
    
    # è®¾ç½®æ 
    with st.container():
        col1, col2 = st.columns([3, 1])
        with col1:
            if not AVAILABLE_MODEL_OPTIONS:
                st.error("âŒ æœªæ£€æµ‹åˆ° API Keyã€‚")
                selected_model_info = {"api_key": None}
            else:
                s_name = st.selectbox("é€‰æ‹©æ¨¡å‹", list(AVAILABLE_MODEL_OPTIONS.keys()))
                selected_model_info = AVAILABLE_MODEL_OPTIONS[s_name]
        with col2:
            st.write("")
            if st.button("æµ‹è¯•è¿æ¥"):
                ok, _, msg = call_llm_api_cached(selected_model_info["provider"], selected_model_info["model"], selected_model_info["api_key"], [{"role":"user","content":"hi"}], max_tokens=5)
                if ok: st.success("è¿æ¥æˆåŠŸ")
                else: st.error(f"å¤±è´¥: {msg}")

    st.markdown("---")
    
    tab1, tab2 = st.tabs(["ğŸ” å•ä¸ªè¯è¯­è¯¦ç»†åˆ†æ", "ğŸ“‚ Excel æ‰¹é‡å¤„ç†"])
    
    # Tab 1: å•è¯åˆ†æ
    with tab1:
        word = st.text_input("è¾“å…¥è¯è¯­", placeholder="ä¾‹å¦‚ï¼šå‘å±•", key="single_input")
        if st.button("å¼€å§‹åˆ†æ", type="primary", disabled=not (word and selected_model_info["api_key"])):
            with st.spinner("æ€è€ƒä¸åˆ†æä¸­..."):
                scores, raw, pred, expl = ask_model_for_pos_and_scores(word, selected_model_info["provider"], selected_model_info["model"], selected_model_info["api_key"])
                
                if scores:
                    mem = calculate_membership(scores)
                    st.success(f"é¢„æµ‹ç»“æœï¼š**{pred}** (éš¶å±åº¦: {mem.get(pred,0):.2f})")
                    
                    c1, c2 = st.columns(2)
                    with c1:
                        top10_df = pd.DataFrame(get_top_10_positions(mem), columns=["è¯ç±»", "éš¶å±åº¦"])
                        st.table(top10_df)
                        plot_radar_chart_streamlit(mem, f"{word} é›·è¾¾å›¾")
                    with c2:
                        st.subheader("ğŸ“ æ¨ç†è¿‡ç¨‹")
                        # å¼ºåˆ¶æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹ï¼Œå¦‚æœ JSON é‡Œçš„ä¸ºç©ºï¼Œæ˜¾ç¤ºåŸå§‹æ–‡æœ¬
                        display_text = expl if expl and len(expl) > 5 else raw
                        st.info(display_text)
                        
                        with st.expander("æŸ¥çœ‹åŸå§‹ JSON å“åº”"):
                            st.code(raw, language="json")
                else:
                    st.error("è§£æå¤±è´¥ï¼Œè¯·æŸ¥çœ‹ä¸‹æ–¹åŸå§‹å“åº”æ‰‹åŠ¨åˆ¤æ–­ã€‚")
                    st.text_area("åŸå§‹å“åº”", raw, height=300)

    # Tab 2: æ‰¹é‡å¤„ç†
    with tab2:
        st.info("ä¸Šä¼  Excel (éœ€åŒ…å«'è¯è¯­'åˆ—)ï¼Œè‡ªåŠ¨ç”Ÿæˆç»“æœå¹¶æ ‡é»„ã€‚")
        uploaded_file = st.file_uploader("ä¸Šä¼  Excel", type=["xlsx", "xls"])
        
        if uploaded_file and selected_model_info["api_key"]:
            try:
                df = pd.read_excel(uploaded_file)
                target_col = next((c for c in df.columns if "è¯" in str(c) or "word" in str(c).lower()), None)
                
                if target_col:
                    st.write(f"âœ… ç›®æ ‡åˆ—ï¼š`{target_col}`ï¼Œå…± {len(df)} ä¸ªè¯ã€‚")
                    if st.button("ğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ"):
                        excel_data = process_and_style_excel(df, selected_model_info, target_col)
                        st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ", excel_data, file_name="åˆ†æç»“æœ.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
                else:
                    st.error("âŒ æœªæ‰¾åˆ°åŒ…å« 'è¯' çš„åˆ—åã€‚")
            except Exception as e:
                st.error(f"æ–‡ä»¶é”™è¯¯: {e}")

if __name__ == "__main__":
    main()
