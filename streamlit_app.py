import streamlit as st
import requests
import json
import re
import os
import pandas as pd
import plotly.graph_objects as go
import io
import time
from typing import Tuple, Dict, Any, List
from openpyxl import load_workbook
from openpyxl.styles import PatternFill

# ===============================
# 1. é¡µé¢é…ç½®
# ===============================
st.set_page_config(
    page_title="æ±‰è¯­è¯ç±»éš¶å±åº¦æ£€æµ‹ ",
    page_icon="ğŸ›¡ï¸",
    layout="wide",
    initial_sidebar_state="collapsed"
)

st.markdown("""
<style>
header {visibility: hidden;}
footer {visibility: hidden;}
.dataframe {font-size: 12px;}
.stApp > div:first-child { padding-top: 2rem; }
/* ä¼˜åŒ–ä»£ç å—æ˜¾ç¤ºï¼Œé˜²æ­¢è¿‡é«˜ */
.stCode { max-height: 300px; overflow-y: auto; }
</style>
""", unsafe_allow_html=True)

# ===============================
# 2. æ¨¡å‹é…ç½® (OpenAI å…¼å®¹åè®®)
# ===============================
MODEL_CONFIGS = {
    "deepseek": {
        "base_url": "https://api.deepseek.com/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": 4096, "temperature": 0.0, "stream": True
        },
    },
    "openai": {
        "base_url": "https://api.openai.com/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": 4096, "temperature": 0.0, "stream": True
        },
    },
    "moonshot": {
        "base_url": "https://api.moonshot.cn/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": 4096, "temperature": 0.0, "stream": True
        },
    },
    "qwen": {
        "base_url": "https://dashscope.aliyuncs.com/api/v1",
        "endpoint": "/services/aigc/text-generation/generation",
        "headers": lambda key: {
            "Authorization": f"Bearer {key}", "Content-Type": "application/json",
            "X-DashScope-SSE": "enable", "Accept": "text/event-stream"
        },
        "payload": lambda model, messages, **kw: {
            "model": model, "input": {"messages": messages}, 
            "parameters": {"max_tokens": 4096, "temperature": 0.0, "result_format": "message", "incremental_output": True},
        },
    },
}

MODEL_OPTIONS = {
    "DeepSeek Chat": {"provider": "deepseek", "model": "deepseek-chat", "api_key": os.getenv("DEEPSEEK_API_KEY"), "env_var": "DEEPSEEK_API_KEY"},
    "OpenAI GPT-4o-mini": {"provider": "openai", "model": "gpt-4o-mini", "api_key": os.getenv("OPENAI_API_KEY"), "env_var": "OPENAI_API_KEY"},
    "Moonshot (Kimi)": {"provider": "moonshot", "model": "moonshot-v1-32k", "api_key": os.getenv("MOONSHOT_API_KEY"), "env_var": "MOONSHOT_API_KEY"},
    "Qwen (é€šä¹‰åƒé—®)": {"provider": "qwen", "model": "qwen-max", "api_key": os.getenv("QWEN_API_KEY"), "env_var": "QWEN_API_KEY"},
}

AVAILABLE_MODEL_OPTIONS = {name: info for name, info in MODEL_OPTIONS.items() if info["api_key"]}
if not AVAILABLE_MODEL_OPTIONS: AVAILABLE_MODEL_OPTIONS = MODEL_OPTIONS

# ===============================
# 3. è§„åˆ™å®šä¹‰
# ===============================
RULE_SETS = {
    "åè¯": [
        {"name": "N1_å¯å—æ•°é‡è¯ä¿®é¥°", "desc": "å¯ä»¥å—æ•°é‡è¯ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "N2_ä¸èƒ½å—å‰¯è¯ä¿®é¥°", "desc": "ä¸èƒ½å—å‰¯è¯ä¿®é¥°", "match_score": 20, "mismatch_score": -20},
        {"name": "N3_å¯ä½œä¸»å®¾è¯­", "desc": "å¯ä»¥åšå…¸å‹çš„ä¸»è¯­æˆ–å®¾è¯­", "match_score": 20, "mismatch_score": 0},
        {"name": "N4_å¯ä½œä¸­å¿ƒè¯­æˆ–ä½œå®šè¯­", "desc": "å¯ä»¥åšä¸­å¿ƒè¯­å—å…¶ä»–åè¯ä¿®é¥°ï¼Œæˆ–è€…ä½œå®šè¯­ç›´æ¥ä¿®é¥°å…¶ä»–åè¯", "match_score": 10, "mismatch_score": 0},
        {"name": "N5_å¯åé™„çš„å­—ç»“æ„", "desc": "å¯ä»¥åé™„åŠ©è¯â€œçš„â€æ„æˆâ€œçš„â€å­—ç»“æ„", "match_score": 10, "mismatch_score": 0},
        {"name": "N6_å¯åé™„æ–¹ä½è¯æ„å¤„æ‰€", "desc": "å¯ä»¥åé™„æ–¹ä½è¯æ„æˆå¤„æ‰€ç»“æ„", "match_score": 10, "mismatch_score": 0},
        {"name": "N7_ä¸èƒ½ä½œè°“è¯­æ ¸å¿ƒ", "desc": "ä¸èƒ½åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒ", "match_score": 10, "mismatch_score": -10},
        {"name": "N8_ä¸èƒ½ä½œè¡¥è¯­/ä¸€èˆ¬ä¸ä½œçŠ¶è¯­", "desc": "ä¸èƒ½ä½œè¡¥è¯­ï¼Œå¹¶ä¸”ä¸€èˆ¬ä¸èƒ½åšçŠ¶è¯­", "match_score": 10, "mismatch_score": 0},
    ],
    "åŠ¨è¯": [
        {"name": "V1_å¯å—å¦å®š'ä¸/æ²¡æœ‰'ä¿®é¥°", "desc": "å¯ä»¥å—å¦å®šå‰¯è¯'ä¸'æˆ–'æ²¡æœ‰'ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "V2_å¯åé™„/æ’å…¥æ—¶ä½“åŠ©è¯", "desc": "å¯ä»¥åé™„æˆ–ä¸­é—´æ’å…¥æ—¶ä½“åŠ©è¯'ç€/äº†/è¿‡'", "match_score": 10, "mismatch_score": 0},
        {"name": "V3_å¯å¸¦çœŸå®¾è¯­", "desc": "å¯ä»¥å¸¦çœŸå®¾è¯­ï¼Œæˆ–é€šè¿‡ä»‹è¯å¼•å¯¼è®ºå…ƒ", "match_score": 20, "mismatch_score": 0},
        {"name": "V4_ç¨‹åº¦å‰¯è¯ä¸å¸¦å®¾è¯­çš„å…³ç³»", "desc": "ä¸èƒ½å—ç¨‹åº¦å‰¯è¯'å¾ˆ'ä¿®é¥°ï¼Œæˆ–èƒ½åŒæ—¶å—'å¾ˆ'ä¿®é¥°å¹¶å¸¦å®¾è¯­", "match_score": 10, "mismatch_score": -10},
        {"name": "V5_å¯æœ‰é‡å /æ­£åé‡å å½¢å¼", "desc": "å¯ä»¥æœ‰'VV, Vä¸€V'ç­‰å½¢å¼", "match_score": 10, "mismatch_score": 0},
        {"name": "V6_å¯åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒ", "desc": "å¯ä»¥åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒ", "match_score": 10, "mismatch_score": -10},
        {"name": "V7_ä¸èƒ½ä½œçŠ¶è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "desc": "ä¸èƒ½ä½œçŠ¶è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "match_score": 10, "mismatch_score": 0},
        {"name": "V8_å¯ä½œ'æ€ä¹ˆ/æ€æ ·'æé—®", "desc": "å¯ä»¥è·Ÿåœ¨'æ€ä¹ˆ/æ€æ ·'ä¹‹åæé—®", "match_score": 10, "mismatch_score": 0},
        {"name": "V9_ä¸èƒ½è·Ÿåœ¨'å¤š/å¤šä¹ˆ'ä¹‹å", "desc": "ä¸èƒ½è·Ÿåœ¨'å¤š'ä¹‹åå¯¹æ€§è´¨æé—®", "match_score": 10, "mismatch_score": -10},
    ],
    "ååŠ¨è¯": [
        {"name": "NV1_å¯è¢«å¦å®šä¸”è‚¯å®šå½¢å¼-1", "desc": "å¯ä»¥ç”¨'ä¸/æ²¡æœ‰'å¦å®š", "match_score": 10, "mismatch_score": -10},
        {"name": "NV2_å¯é™„æ—¶ä½“åŠ©è¯", "desc": "å¯ä»¥åé™„æ—¶ä½“åŠ©è¯'ç€/äº†/è¿‡'", "match_score": 10, "mismatch_score": -10},
        {"name": "NV3_å¯å¸¦çœŸå®¾è¯­ä¸”ä¸å—å¾ˆä¿®é¥°", "desc": "å¯ä»¥å¸¦çœŸå®¾è¯­ï¼Œå¹¶ä¸”ä¸èƒ½å—'å¾ˆ'ä¿®é¥°", "match_score": 10, "mismatch_score": -10},
        {"name": "NV4_æœ‰é‡å å’Œæ­£åé‡å å½¢å¼", "desc": "æœ‰é‡å å½¢å¼", "match_score": 10, "mismatch_score": 0},
        {"name": "NV5_å¯ä½œå¤šç§å¥æ³•æˆåˆ†", "desc": "æ—¢å¯ä»¥ä½œè°“è¯­ï¼Œåˆå¯ä»¥ä½œä¸»è¯­æˆ–å®¾è¯­", "match_score": 10, "mismatch_score": -10},
        {"name": "NV6_ä¸èƒ½ç›´æ¥ä½œçŠ¶è¯­", "desc": "ä¸èƒ½ç›´æ¥ä½œçŠ¶è¯­", "match_score": 10, "mismatch_score": -10},
        {"name": "NV7_å¯ä¿®é¥°åè¯æˆ–å—åè¯ä¿®é¥°", "desc": "å¯ä»¥ä¿®é¥°åè¯æˆ–è€…å—åè¯ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "NV8_å¯è·Ÿåœ¨æ€ä¹ˆ/æ€æ ·ä¹‹å", "desc": "å¯ä»¥è·Ÿåœ¨'æ€ä¹ˆ/æ€æ ·'ä¹‹åæé—®", "match_score": 10, "mismatch_score": 0},
        {"name": "NV9_ä¸èƒ½è·Ÿåœ¨å¤š/å¤šä¹ˆä¹‹å", "desc": "ä¸èƒ½è·Ÿåœ¨'å¤š/å¤šä¹ˆ'ä¹‹å", "match_score": 10, "mismatch_score": -10},
        {"name": "NV10_å¯åé™„æ–¹ä½è¯", "desc": "å¯ä»¥åé™„æ–¹ä½è¯æ„æˆå¤„æ‰€ç»“æ„", "match_score": 10, "mismatch_score": 0},
    ]
}

# ===============================
# 4. æ ¸å¿ƒå·¥å…·å‡½æ•°
# ===============================
def extract_text_from_response(resp_json: Dict[str, Any]) -> str:
    """æå–APIå“åº”æ–‡æœ¬ï¼Œå…¼å®¹å¤šç§æ ¼å¼"""
    if not isinstance(resp_json, dict): return ""
    try:
        # Qwen
        if "output" in resp_json and "text" in resp_json["output"]: return resp_json["output"]["text"]
        # OpenAI/Compatible
        if "choices" in resp_json and len(resp_json["choices"]) > 0:
            choice = resp_json["choices"][0]
            if "message" in choice and "content" in choice["message"]: return choice["message"]["content"]
        return json.dumps(resp_json, ensure_ascii=False)
    except Exception: return json.dumps(resp_json, ensure_ascii=False)

def extract_json_from_text(text: str) -> Tuple[Dict[str, Any], str]:
    """å¼ºåŠ›JSONæå–ï¼Œä¼˜å…ˆä»£ç å—ï¼Œå…¶æ¬¡å¤§æ‹¬å·ï¼Œæå–å¤±è´¥è¿”å›None"""
    if not text: return None, ""
    json_str = ""
    # ç­–ç•¥1: Markdownä»£ç å—
    code_match = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
    if code_match: json_str = code_match.group(1).strip()
    # ç­–ç•¥2: æœ€å¤–å±‚å¤§æ‹¬å·
    if not json_str:
        match = re.search(r"(\{.*\})", text.strip(), re.DOTALL)
        if match: json_str = match.group(1).strip()
    
    if not json_str: return None, text
    try:
        return json.loads(json_str), json_str
    except:
        return None, text

def map_to_allowed_score(rule: dict, raw_val) -> int:
    match, mismatch = rule["match_score"], rule["mismatch_score"]
    if isinstance(raw_val, bool): return match if raw_val else mismatch
    if isinstance(raw_val, str):
        s = raw_val.strip().lower()
        if s in ("yes", "y", "true", "æ˜¯", "âˆš", "ç¬¦åˆ"): return match
        return mismatch
    return mismatch

def calculate_membership(scores_all: Dict[str, Dict[str, int]]) -> Dict[str, float]:
    membership = {}
    for pos, scores in scores_all.items():
        total = sum(scores.values())
        membership[pos] = max(-1.0, min(1.0, total / 100))
    return membership

def get_top_10_positions(membership: Dict[str, float]) -> List[Tuple[str, float]]:
    return sorted(membership.items(), key=lambda x: x[1], reverse=True)[:10]

# ===============================
# 5. API è°ƒç”¨ (æµå¼ + è¶…æ—¶ä¿æŠ¤)
# ===============================
def call_llm_api_cached(_provider, _model, _api_key, messages, max_tokens=4096, temperature=0.0):
    if not _api_key: return False, {"error": "API Keyç¼ºå¤±"}, "Keyæœªè®¾ç½®"
    cfg = MODEL_CONFIGS[_provider]
    url = f"{cfg['base_url'].rstrip('/')}{cfg['endpoint']}"
    headers = cfg["headers"](_api_key)
    payload = cfg["payload"](_model, messages, max_tokens=max_tokens, temperature=temperature)
    
    full_content = ""
    try:
        # è®¾ç½® stream=True å’Œ 60s è¿æ¥è¶…æ—¶
        with requests.post(url, headers=headers, json=payload, stream=True, timeout=60) as response:
            response.raise_for_status()
            for line in response.iter_lines():
                if not line: continue
                line_text = line.decode('utf-8').strip()
                if line_text.startswith("data:"): json_str = line_text[5:].strip()
                else: json_str = line_text
                if json_str == "[DONE]": break
                try:
                    chunk = json.loads(json_str)
                    delta_text = ""
                    if "choices" in chunk and len(chunk["choices"]) > 0:
                        delta_text = chunk["choices"][0].get("delta", {}).get("content", "")
                    elif "output" in chunk: # Qwen
                        if "choices" in chunk["output"]:
                            delta_text = chunk["output"]["choices"][0].get("message", {}).get("content", "")
                        elif "text" in chunk["output"]:
                            delta_text = chunk["output"]["text"]
                    if delta_text: full_content += delta_text
                except: continue
        
        if not full_content: return False, {"error": "ç©ºå“åº”"}, "ç©ºå“åº”"
        return True, {"choices": [{"message": {"content": full_content}}], "output": {"text": full_content}}, ""
    except Exception as e:
        return False, {"error": str(e)}, str(e)

# ===============================
# 6. å•ä¸ªè¯åˆ†æé€»è¾‘ (Prompt æ·±åº¦ä¼˜åŒ–ç‰ˆ)
# ===============================
def ask_model_for_pos_and_scores(word: str, provider: str, model: str, api_key: str) -> Tuple[Dict[str, Dict[str, int]], str, str, str]:
    if not word:
        return {}, "", "æœªçŸ¥", ""

    # è§„åˆ™æ–‡å­—è¯´æ˜ï¼ˆç»™æ¨¡å‹çœ‹ï¼Œè®©å®ƒè€è€å®å®æŒ‰è§„åˆ™æ¥åˆ¤æ–­ï¼‰
    full_rules_by_pos = {
        pos: "\n".join([
            f"- {r['name']}: {r['desc']}ï¼ˆç¬¦åˆ: {r['match_score']} åˆ†ï¼Œä¸ç¬¦åˆ: {r['mismatch_score']} åˆ†ï¼‰"
            for r in rules
        ])
        for pos, rules in RULE_SETS.items()
    }

    # ===== ç³»ç»Ÿæç¤ºï¼šåªå…è®¸è¾“å‡ºâ€œç¬¦åˆ/ä¸ç¬¦åˆâ€ï¼Œç¦æ­¢è‡ªå·±æ‰“æ•°å­—åˆ† =====
    system_msg = f"""ä½ æ˜¯ä¸€åä¸­æ–‡è¯æ³•ä¸è¯­æ³•æ–¹é¢çš„ä¸“å®¶ã€‚ç°åœ¨è¦åˆ†æè¯è¯­ã€Œ{word}ã€åœ¨ä¸‹åˆ—è¯ç±»ä¸­çš„è¡¨ç°ï¼š

- éœ€è¦åˆ¤æ–­çš„è¯ç±»ï¼šåè¯ã€åŠ¨è¯ã€ååŠ¨è¯
- è¯„åˆ†è§„åˆ™å·²ç»ç”±ç³»ç»Ÿå®šä¹‰ï¼Œä½ **ä¸è¦**è‡ªå·±è®¾è®¡åˆ†å€¼ï¼Œä¹Ÿ**ä¸è¦**åœ¨ JSON ä¸­ç»™å‡ºå…·ä½“æ•°å­—åˆ†æ•°ã€‚ç¨‹åºå°†æ ¹æ®ä½ çš„åˆ¤æ–­ï¼ˆtrue/falseï¼‰è‡ªåŠ¨èµ‹å€¼ã€‚
- ä½ åªéœ€è¦åˆ¤æ–­æ¯ä¸€æ¡è§„åˆ™æ˜¯â€œç¬¦åˆâ€è¿˜æ˜¯â€œä¸ç¬¦åˆâ€ã€‚

ã€å„è¯ç±»çš„è§„åˆ™è¯´æ˜ï¼ˆä»…ä¾›ä½ åˆ¤æ–­ä½¿ç”¨ï¼‰ã€‘

ã€åè¯ã€‘
{full_rules_by_pos["åè¯"]}

ã€åŠ¨è¯ã€‘
{full_rules_by_pos["åŠ¨è¯"]}

ã€ååŠ¨è¯ã€‘
{full_rules_by_pos["ååŠ¨è¯"]}

ã€è¾“å‡ºè¦æ±‚ã€‘

1. åœ¨ explanation å­—æ®µä¸­ï¼Œå¿…é¡»**é€æ¡è§„åˆ™**è¯´æ˜åˆ¤æ–­ä¾æ®ï¼Œå¹¶ä¸¾ä¾‹ï¼ˆå¯ä»¥è‡ªå·±é€ å¥ï¼‰ï¼š
   - æ ¼å¼ç¤ºä¾‹ï¼š
     - ã€Œåè¯-N1_å¯å—æ•°é‡è¯ä¿®é¥°ï¼šç¬¦åˆã€‚ç†ç”±ï¼šâ€¦â€¦ã€‚ä¾‹å¥ï¼šâ€¦â€¦ã€‚ã€
     - ã€ŒåŠ¨è¯-V2_å¯åé™„/æ’å…¥æ—¶ä½“åŠ©è¯'ç€/äº†/è¿‡'ï¼šä¸ç¬¦åˆã€‚ç†ç”±ï¼šâ€¦â€¦ã€‚ä¾‹å¥ï¼šâ€¦â€¦ã€‚ã€
   - explanation é‡Œè¦è¦†ç›– **ä¸‰ä¸ªè¯ç±»çš„æ‰€æœ‰è§„åˆ™**ï¼Œä¸èƒ½åªå†™å‡ æ¡ã€‚

2. åœ¨ JSON ä¸­çš„ scores å­—æ®µé‡Œï¼š
   - æ¯ä¸€ç±»ä¸‹çš„æ¯ä¸€æ¡è§„åˆ™ï¼Œåªèƒ½ç»™å‡º **å¸ƒå°”å€¼ true / false**ï¼Œè¡¨ç¤ºæ˜¯å¦ç¬¦åˆè¯¥è§„åˆ™
   - ä¸¥ç¦åœ¨ scores é‡Œä½¿ç”¨æ•°å€¼åˆ†æ•°ï¼ˆä¾‹å¦‚ 0, 5, 10 ç­‰ï¼‰
   - å¦‚æœä½ ä¸ç¡®å®šï¼Œä¹Ÿå¿…é¡»åšå‡ºåˆ¤æ–­ï¼ˆtrue æˆ– falseï¼‰ï¼Œä¸è¦ç”¨ nullã€0 æˆ–å…¶å®ƒå€¼
   - JSON ç»“æ„å¿…é¡»æ˜¯ï¼š{{"explanation": "...", "predicted_pos": "...", "scores": {{"åè¯": {{...}}, "åŠ¨è¯": {{...}}, "ååŠ¨è¯": {{...}}}}}}

3. predicted_posï¼š
   - è¯·é€‰æ‹©ã€Œåè¯ã€ã€ŒåŠ¨è¯ã€ã€ŒååŠ¨è¯ã€ä¹‹ä¸€ï¼Œä½œä¸ºè¯¥è¯è¯­æœ€å…¸å‹çš„è¯ç±»ã€‚

4. **æœ€åè¾“å‡ºæ—¶ï¼Œå…ˆå†™è¯¦ç»†çš„æ–‡å­—æ¨ç†ï¼Œæœ€åå•ç‹¬ä¸”å®Œæ•´åœ°ç»™å‡ºä¸€æ®µåˆæ³•çš„ JSONï¼ˆä¸è¦å†åŠ æ³¨é‡Šï¼‰ã€‚**
"""

    # ç”¨æˆ·æç¤ºï¼šå†å¼ºè°ƒä¸€æ¬¡
    user_prompt = f"""
è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è¦æ±‚åˆ†æè¯è¯­ã€Œ{word}ã€ã€‚

ç‰¹åˆ«æ³¨æ„ï¼š
- åœ¨ JSON çš„ scores éƒ¨åˆ†ï¼Œåªèƒ½ç”¨ true/false è¡¨ç¤ºâ€œæ˜¯å¦ç¬¦åˆè§„åˆ™â€ï¼Œä¸èƒ½ä½¿ç”¨ä»»ä½•æ•°å­—ã€‚
- explanation ä¸­å¿…é¡»å¯¹æ¯ä¸€æ¡è§„åˆ™å†™æ˜â€œç¬¦åˆ/ä¸ç¬¦åˆ + ç†ç”± + ä¾‹å¥â€ã€‚

è¯·å…ˆç»™å‡ºè¯¦ç»†æ¨ç†è¿‡ç¨‹ï¼Œç„¶ååœ¨æœ€åå•ç‹¬è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ã€‚
"""

    with st.spinner(f"æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ ({model}) è¿›è¡Œåˆ†æï¼Œè¯·ç¨å€™..."):
        ok, resp_json, err_msg = call_llm_api_cached(
            _provider=provider,
            _model=model,
            _api_key=api_key,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_prompt}
            ]
        )

    if not ok:
        st.error(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {err_msg}")
        # è¿”å›ä¸€ä¸ªç©ºç»“æœï¼Œä½†ä¿ç•™é”™è¯¯ä¿¡æ¯
        return {}, f"è°ƒç”¨å¤±è´¥: {err_msg}", "æœªçŸ¥", f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {err_msg}"

    raw_text = extract_text_from_response(resp_json)
    
    # ã€ä¿®å¤æ ¸å¿ƒé—®é¢˜ã€‘è°ƒç”¨æ–°å¢çš„ JSON æå–å‡½æ•°
    parsed_json, cleaned_json_text = extract_json_from_text(raw_text)

    # è§£æ JSON
    if parsed_json and isinstance(parsed_json, dict):
        explanation = parsed_json.get("explanation", "æ¨¡å‹æœªæä¾›è¯¦ç»†æ¨ç†è¿‡ç¨‹ã€‚")
        predicted_pos = parsed_json.get("predicted_pos", "æœªçŸ¥")
        raw_scores = parsed_json.get("scores", {})
        if predicted_pos not in RULE_SETS:
             st.warning(f"æ¨¡å‹é¢„æµ‹çš„è¯ç±» '{predicted_pos}' ä¸åœ¨åˆ†æèŒƒå›´å†… ('åè¯', 'åŠ¨è¯', 'ååŠ¨è¯')ã€‚")
    else:
        st.error("âŒ æœªèƒ½ä»æ¨¡å‹å“åº”ä¸­è§£æå‡ºæœ‰æ•ˆçš„JSONã€‚è¯·æ£€æŸ¥æ¨¡å‹è¾“å‡ºæ˜¯å¦ç¬¦åˆè¦æ±‚ã€‚")
        explanation = "æ— æ³•è§£ææ¨¡å‹è¾“å‡ºã€‚åŸå§‹å“åº”ï¼š\n" + raw_text
        predicted_pos = "æœªçŸ¥"
        raw_scores = {}
        cleaned_json_text = raw_text  # å±•ç¤ºåŸå§‹æ–‡æœ¬

    # --- å…³é”®ï¼šåˆå§‹åŒ–æ‰€æœ‰è¯ç±»çš„å¾—åˆ†å­—å…¸ï¼Œå¹¶è¿›è¡Œåˆ†æ•°è½¬æ¢ ---
    scores_out = {pos: {} for pos in RULE_SETS.keys()}

    # åªæŠŠâ€œç¬¦åˆ/ä¸ç¬¦åˆâ€è½¬æˆå…·ä½“åˆ†å€¼ï¼ˆæ­£åˆ† / è´Ÿåˆ†ï¼‰
    for pos, rules in RULE_SETS.items():
        raw_pos_scores = raw_scores.get(pos, {})
        if isinstance(raw_pos_scores, dict):
            for k, v in raw_pos_scores.items():
                normalized_key = normalize_key(k, rules)
                if normalized_key:
                    # æ‰¾åˆ°å½“å‰è§„åˆ™çš„å®šä¹‰
                    rule_def = next(r for r in rules if r["name"] == normalized_key)
                    # ä½¿ç”¨ map_to_allowed_scoreï¼štrue/false/â€œæ˜¯/å¦â€ç­‰ â†’ match_score / mismatch_score
                    scores_out[pos][normalized_key] = map_to_allowed_score(rule_def, v)

    # ä¿è¯æ¯æ¡è§„åˆ™éƒ½æœ‰å¾—åˆ†ï¼Œæ²¡æœ‰å°±é»˜è®¤ Mismatch Scoreï¼ˆæ›´ä¸¥æ ¼ï¼‰æˆ– 0 åˆ†ï¼ˆæ›´ä¿å®ˆï¼‰
    # é‡‡ç”¨æ›´ä¿å®ˆçš„ 0 åˆ†ï¼Œå› ä¸ºæ¨¡å‹æ²¡æåŠï¼Œå¯èƒ½æ˜¯ä¸é€‚ç”¨ï¼Œè€Œä¸æ˜¯æ˜ç¡®ä¸ç¬¦åˆ
    for pos, rules in RULE_SETS.items():
        for rule in rules:
            rule_name = rule["name"]
            if rule_name not in scores_out[pos]:
                scores_out[pos][rule_name] = 0

    return scores_out, raw_text, predicted_pos, explanation

# ===============================
# 7. æ‰¹é‡å¤„ç†é€»è¾‘ (å®æ—¶ä¿å­˜ + å¢é‡æ›´æ–°)
# ===============================
def process_batch(df, model_info, col_name):
    """
    æ ¸å¿ƒä¿®æ”¹ï¼š
    1. ä½¿ç”¨ 'history_database.csv' ä½œä¸ºæŒä¹…åŒ–å­˜å‚¨ã€‚
    2. ä¼˜å…ˆè¯»å– CSV è·³è¿‡å·²å¤„ç†è¯æ±‡ã€‚
    3. æ¯å¤„ç†ä¸€ä¸ªè¯ï¼Œè¿½åŠ å†™å…¥ CSVã€‚
    """
    db_file = "history_database.csv"
    output = io.BytesIO()
    
    # A. è¯»å–å†å²è®°å½•å»ºç«‹ç¼“å­˜
    history_cache = {}
    if os.path.exists(db_file):
        try:
            # å¼ºåˆ¶æŒ‰å­—ç¬¦ä¸²è¯»å–ï¼Œé¿å…æ•°å­—/æ–‡æœ¬æ··æ·†
            hist_df = pd.read_csv(db_file, dtype=str)
            for _, row in hist_df.iterrows():
                if "è¯è¯­" in row and pd.notna(row["è¯è¯­"]):
                    history_cache[str(row["è¯è¯­"]).strip()] = row.to_dict()
            st.info(f"ğŸ“š å·²åŠ è½½æœ¬åœ°å†å²è®°å½• {len(history_cache)} æ¡ï¼Œå°†è‡ªåŠ¨è·³è¿‡è¿™äº›è¯ã€‚")
        except Exception as e:
            st.warning(f"å†å²æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œå°†é‡æ–°åˆ†æ: {e}")

    # B. å‡†å¤‡è¿›åº¦æ¡
    total = len(df)
    bar = st.progress(0)
    status = st.empty()
    
    final_rows = []
    
    # C. å¼€å§‹å¾ªç¯
    for i, row_data in df.iterrows():
        word = str(row_data[col_name]).strip()
        
        # 1. æ£€æŸ¥ç¼“å­˜
        if word in history_cache:
            status.text(f"â™»ï¸ [è·³è¿‡] {word} (å·²åœ¨å†å²è®°å½•)")
            final_rows.append(history_cache[word])
            # å°å»¶æ—¶è®©ç•Œé¢åˆ·æ–°
            time.sleep(0.01) 
            bar.progress((i + 1) / total)
            continue
            
        # 2. ä¸åœ¨ç¼“å­˜ï¼Œè°ƒç”¨ API (å¸¦é‡è¯•)
        status.text(f"ğŸš€ [åˆ†æä¸­] {word} ({i + 1}/{total})")
        
        retries = 3
        success = False
        scores, raw, pred, expl = {}, "", "è¯·æ±‚å¤±è´¥", "å¤šæ¬¡é‡è¯•å¤±è´¥"
        
        for attempt in range(retries):
            try:
                scores, raw, pred, expl = ask_model_for_pos_and_scores(
                    word, model_info["provider"], model_info["model"], model_info["api_key"]
                )
                # åªè¦ raw ä¸ä¸ºç©ºå°±ç®—æœ‰å“åº”
                if raw:
                    success = True
                    break
                time.sleep(2)
            except:
                time.sleep(2)
        
        # 3. è®¡ç®—ç»“æœ
        if success and scores:
            mem = calculate_membership(scores)
            v = mem.get("åŠ¨è¯", 0.0)
            n = mem.get("åè¯", 0.0)
            nv = mem.get("ååŠ¨è¯", 0.0)
        else:
            v, n, nv = 0.0, 0.0, 0.0
            
        # 4. æ„é€ æ–°è¡Œ
        new_row = {
            "è¯è¯­": word,
            "åŠ¨è¯": v, "åè¯": n, "ååŠ¨è¯": nv,
            "å·®å€¼/è·ç¦»": round(abs(v - n), 4),
            "åŸå§‹å“åº”": expl if len(expl) > 5 else raw, # ç¡®ä¿æ¨ç†ä¸ä¸¢å¤±
            "_predicted_pos": pred
        }
        final_rows.append(new_row)
        
        # 5. ã€æ ¸å¿ƒã€‘å®æ—¶è¿½åŠ å†™å…¥ CSV
        try:
            temp_df = pd.DataFrame([new_row])
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™å†™è¡¨å¤´ï¼Œå­˜åœ¨åˆ™ä¸å†™è¡¨å¤´ç›´æ¥è¿½åŠ 
            write_header = not os.path.exists(db_file)
            temp_df.to_csv(db_file, mode='a', header=write_header, index=False, encoding='utf-8-sig')
        except Exception as e:
            print(f"å†™å…¥å¤±è´¥: {e}")
            
        # 6. é˜²å°å·å»¶æ—¶
        time.sleep(1)
        bar.progress((i + 1) / total)

    # D. å¾ªç¯ç»“æŸï¼Œç”Ÿæˆæ¼‚äº®çš„ Excel
    status.success("âœ… å…¨éƒ¨å®Œæˆï¼")
    
    if not final_rows: return None
    
    res_df = pd.DataFrame(final_rows)
    # å¯¼å‡º
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        cols = ["è¯è¯­", "åŠ¨è¯", "åè¯", "ååŠ¨è¯", "å·®å€¼/è·ç¦»", "åŸå§‹å“åº”"]
        # ç¡®ä¿åˆ—å­˜åœ¨
        valid_cols = [c for c in cols if c in res_df.columns]
        res_df[valid_cols].to_excel(writer, index=False, sheet_name='ç»“æœ')
        # æ ‡é»„
        try:
            ws = writer.sheets['ç»“æœ']
            fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
            for idx, r in enumerate(final_rows):
                # å…¼å®¹ä»CSVè¯»å–çš„æ•°æ®ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼‰å’Œåˆšç”Ÿæˆçš„æ•°æ®
                p = str(r.get("_predicted_pos", ""))
                target = None
                if "åŠ¨è¯" in p: target = 2
                elif "åè¯" in p: target = 3
                elif "ååŠ¨è¯" in p: target = 4
                if target: ws.cell(row=idx+2, column=target).fill = fill
        except: pass
        
    return output.getvalue()

# ===============================
# 8. ä¸»ç¨‹åº UI
# ===============================
def main():
    st.title("ğŸ“° æ±‰è¯­è¯ç±»éš¶å±åº¦æ£€æµ‹ (æ‰¹é‡æ——èˆ°ç‰ˆ)")
    
    # é…ç½®åŒº
    with st.container():
        c1, c2 = st.columns([3, 1])
        with c1:
            if not AVAILABLE_MODEL_OPTIONS:
                st.error("âŒ æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·é…ç½®ç¯å¢ƒå˜é‡ã€‚")
                info = {"api_key": None}
            else:
                name = st.selectbox("é€‰æ‹©æ¨¡å‹", list(AVAILABLE_MODEL_OPTIONS.keys()))
                info = AVAILABLE_MODEL_OPTIONS[name]
        with c2:
            st.write("")
            if st.button("è¿æ¥æµ‹è¯•"):
                ok, _, msg = call_llm_api_cached(info["provider"], info["model"], info["api_key"], [{"role":"user","content":"hi"}], 5)
                if ok: st.success("é€šç•…")
                else: st.error(msg)

    st.markdown("---")
    
    t1, t2 = st.tabs(["ğŸ” å•ä¸ªè¯åˆ†æ", "ğŸ“‚ æ‰¹é‡å…¨è‡ªåŠ¨å¤„ç†"])
    
    # --- Tab 1: å•ä¸ª ---
    with t1:
        w = st.text_input("è¾“å…¥è¯è¯­", key="single_w")
        if st.button("åˆ†æ", disabled=not (w and info["api_key"])):
            with st.spinner("æ€è€ƒä¸­..."):
                scores, raw, pred, expl = ask_model_for_pos_and_scores(w, info["provider"], info["model"], info["api_key"])
                if scores:
                    mem = calculate_membership(scores)
                    st.success(f"ç»“æœ: {pred}")
                    c_a, c_b = st.columns(2)
                    with c_a:
                        st.table(pd.DataFrame(get_top_10_positions(mem), columns=["è¯ç±»","éš¶å±åº¦"]))
                        fig = go.Figure(go.Scatterpolar(r=list(mem.values())+[list(mem.values())[0]], theta=list(mem.keys())+[list(mem.keys())[0]], fill='toself'))
                        st.plotly_chart(fig, use_container_width=True)
                    with c_b:
                        st.info(expl)
                        with st.expander("åŸå§‹ JSON"): st.code(raw)

    # --- Tab 2: æ‰¹é‡ ---
    with t2:
        st.info("ä¸Šä¼  Excel (éœ€å«'è¯è¯­'åˆ—)ã€‚ç¨‹åºä¼šè‡ªåŠ¨ä¿å­˜è¿›åº¦åˆ° `history_database.csv`ï¼Œä¸­æ–­åé‡è·‘å³å¯è‡ªåŠ¨ç»­ä¼ ã€‚")
        
        up = st.file_uploader("ä¸Šä¼  Excel", type=["xlsx"])
        
        if up and info["api_key"]:
            try:
                df = pd.read_excel(up)
                target = next((c for c in df.columns if "è¯" in str(c) or "word" in str(c).lower()), None)
                
                if target:
                    if st.button("ğŸš€ å¼€å§‹æ‰¹é‡ (æ”¯æŒæ–­ç‚¹ç»­ä¼ )"):
                        res = process_batch(df, info, target)
                        if res:
                            st.download_button("ğŸ“¥ ä¸‹è½½æœ€ç»ˆç»“æœ (Excel)", res, "final_result.xlsx")
                else:
                    st.error("æœªæ‰¾åˆ°åŒ…å«'è¯'çš„åˆ—")
            except Exception as e:
                st.error(f"æ–‡ä»¶é”™è¯¯: {e}")

        # --- å†å²æ•°æ®ç®¡ç†åŒº ---
        st.markdown("---")
        st.subheader("ğŸ’¾ æ•°æ®ä¿é™©ç®±")
        db = "history_database.csv"
        if os.path.exists(db):
            try:
                hist = pd.read_csv(db)
                st.write(f"å½“å‰å·²å®‰å…¨ä¿å­˜ **{len(hist)}** æ¡æ•°æ®ã€‚")
                c_d1, c_d2 = st.columns([1, 4])
                with c_d1:
                    st.download_button("ğŸ“¥ ä¸‹è½½å†å²è®°å½• (CSV)", hist.to_csv(index=False).encode('utf-8-sig'), "history_database.csv", "text/csv")
                with c_d2:
                    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå†å² (é‡æ–°å¼€å§‹)"):
                        os.remove(db)
                        st.rerun()
                with st.expander("é¢„è§ˆæ•°æ®"):
                    st.dataframe(hist)
            except:
                st.error("å†å²æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œå¯èƒ½æ­£åœ¨å†™å…¥ä¸­ï¼Œè¯·ç¨ååˆ·æ–°ã€‚")

if __name__ == "__main__":
    main()
