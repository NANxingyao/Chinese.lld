#11.16æµ‹è¯•ç”¨
import streamlit as st
import requests
import json
import re
import os
import pandas as pd
import plotly.graph_objects as go
from typing import Tuple, Dict, Any

# ===============================
# é¡µé¢é…ç½®
# ===============================
st.set_page_config(
    page_title="æ±‰è¯­è¯ç±»éš¶å±åº¦æ£€æµ‹",  # é¡µé¢æ ‡é¢˜
    page_icon="ğŸ“°",                  # é¡µé¢å›¾æ ‡
    layout="centered",               # å¸ƒå±€å±…ä¸­
    initial_sidebar_state="expanded",  # ä¿®æ”¹ä¸ºé»˜è®¤å±•å¼€ä¾§è¾¹æ 
    menu_items=None                  # éšè—é»˜è®¤èœå•
)

# è‡ªå®šä¹‰CSSæ ·å¼ï¼Œéšè—Streamlité»˜è®¤çš„é¡¶éƒ¨å’Œåº•éƒ¨å…ƒç´ 
hide_streamlit_style = """
<style>
/* éšè—é¡¶éƒ¨èœå•æ ï¼ˆShare / GitHub ç­‰ï¼‰ */
header {visibility: hidden;}
/* éšè—å³ä¸‹è§’â€œManage appâ€ */
footer {visibility: hidden;}
/* è°ƒæ•´ä¾§è¾¹æ å®½åº¦ */
/*
[data-testid="stSidebar"][aria-expanded="true"]{
    min-width: 300px;
    max-width: 400px;
}
*/
</style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# ç”¨äºå…¼å®¹ call_llm_api æ—§å‡½æ•°
MODEL_CONFIGS = {
    "deepseek": {
        "base_url": "https://api.deepseek.com/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model,
            "messages": messages,
            "max_tokens": kw.get("max_tokens", 1024),
            "temperature": kw.get("temperature", 0.0),
            "stream": False,
        },
        "response_handler": lambda resp: resp.get("choices", [{}])[0].get("message", {}).get("content", "")
    },

    "openai": {
        "base_url": "https://api.openai.com/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model,
            "messages": messages,
            "max_tokens": kw.get("max_tokens", 1024),
            "temperature": kw.get("temperature", 0.0),
            "stream": False,
        },
        "response_handler": lambda resp: resp.get("choices", [{}])[0].get("message", {}).get("content", "")
    },

    "moonshot": {
        "base_url": "https://api.moonshot.cn/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model,
            "messages": messages,
            "max_tokens": kw.get("max_tokens", 1024),
            "temperature": kw.get("temperature", 0.0),
            "stream": False,
        },
        "response_handler": lambda resp: resp.get("choices", [{}])[0].get("message", {}).get("content", "")
    },

   "doubao": {
    "base_url": "https://ark.cn-beijing.volces.com/api/v3",
    "endpoint": "/chat/completions",
    "headers": lambda key: {
        "Authorization": f"Bearer {key}",
        "Content-Type": "application/json",
    },
    "payload": lambda model, messages, **kw: {
        "model": model,
        "messages": messages,
        "max_tokens": kw.get("max_tokens", 1024),
        "temperature": kw.get("temperature", 0.0),
        "stream": False,
    },
    "response_handler": lambda resp: resp.get("choices", [{}])[0].get("message", {}).get("content", "")
},

    "qwen": {
        "base_url": "https://dashscope.aliyuncs.com/api/v1",
        "endpoint": "/services/aigc/text-generation/generation",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model,
            "input": {"messages": messages},
            "parameters": {
                "max_tokens": kw.get("max_tokens", 1024),
                "temperature": kw.get("temperature", 0.0),
            },
        },
        "response_handler": lambda resp: resp.get("output", {}).get("text", "")
    },
}

# ===============================
# æ¨¡å‹é…ç½®ä¸ API Keyï¼ˆä»ç¯å¢ƒå˜é‡è·å–ï¼‰
# ===============================
MODEL_OPTIONS = {
    "DeepSeek Chat": {
        "provider": "deepseek",
        "model": "deepseek-chat",
        "api_url": "https://api.deepseek.com/v1/chat/completions",
        "api_key": os.getenv("DEEPSEEK_API_KEY", "sk-1f346646d29947d0a5e29dbaa37476b8"),
    },

    "OpenAI GPT-4o": {
        "provider": "openai",
        "model": "gpt-4o-mini",
        "api_url": "https://api.openai.com/v1/chat/completions",
        "api_key": os.getenv("OPENAI_API_KEY", "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"),
    },

    "Moonshotï¼ˆKimiï¼‰": {
        "provider": "moonshot",
        "model": "moonshot-v1-32k",
        "api_url": "https://api.moonshot.cn/v1/chat/completions",
        "api_key": os.getenv("MOONSHOT_API_KEY", "sk-l5FvRWegjM5DEk4AU71YPQ1QgvFPTHZIJOmq6qdssPY4sNtE"),
    },

    "Doubaoï¼ˆè±†åŒ…ï¼‰": {
        "provider": "doubao",
        "model": "doubao-pro-32k",
        "api_url": "https://ark.cn-beijing.volces.com/api/v3/chat/completions",
        "api_key": os.getenv("DOUBAO_API_KEY", "sk-222afa3f-5f27-403e-bf46-ced2a356ceee"),
    },

    "Qwenï¼ˆé€šä¹‰åƒé—®ï¼‰": {
        "provider": "qwen",
        "model": "qwen-max",
        "api_url": "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
        "api_key": os.getenv("QWEN_API_KEY", "sk-b3f7a1153e6f4a44804a296038aa86c5"),
    },
}

# ===============================
# è¯ç±»è§„åˆ™ç¤ºä¾‹ï¼ˆä¿æŒä¸å˜ï¼‰
# ===============================
RULE_SETS = {
    # 1.1 åè¯
    "åè¯": [
        {"name": "N1_å¯å—æ•°é‡è¯ä¿®é¥°", "desc": "å¯ä»¥å—æ•°é‡è¯ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "N2_ä¸èƒ½å—å‰¯è¯ä¿®é¥°", "desc": "ä¸èƒ½å—å‰¯è¯ä¿®é¥°", "match_score": 20, "mismatch_score": -20},
        {"name": "N3_å¯ä½œä¸»å®¾è¯­", "desc": "å¯ä»¥åšå…¸å‹çš„ä¸»è¯­æˆ–å®¾è¯­", "match_score": 20, "mismatch_score": 0},
        {"name": "N4_å¯ä½œä¸­å¿ƒè¯­æˆ–ä½œå®šè¯­", "desc": "å¯ä»¥åšä¸­å¿ƒè¯­å—å…¶ä»–åè¯ä¿®é¥°ï¼Œæˆ–è€…ä½œå®šè¯­ç›´æ¥ä¿®é¥°å…¶ä»–åè¯", "match_score": 10, "mismatch_score": 0},
        {"name": "N5_å¯åé™„çš„å­—ç»“æ„", "desc": "å¯ä»¥åé™„åŠ©è¯â€œçš„â€æ„æˆâ€œçš„â€å­—ç»“æ„", "match_score": 10, "mismatch_score": 0},
        {"name": "N6_å¯åé™„æ–¹ä½è¯æ„å¤„æ‰€", "desc": "å¯ä»¥åé™„æ–¹ä½è¯æ„æˆå¤„æ‰€ç»“æ„", "match_score": 10, "mismatch_score": 0},
        {"name": "N7_ä¸èƒ½ä½œè°“è¯­æ ¸å¿ƒ", "desc": "ä¸èƒ½åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒï¼ˆä¸èƒ½å¸¦å®¾è¯­ï¼Œä¸èƒ½å—çŠ¶è¯­å’Œè¡¥è¯­ï¼Œä¸èƒ½åé™„æ—¶ä½“åŠ©è¯ï¼‰", "match_score": 10, "mismatch_score": -10},
        {"name": "N8_ä¸èƒ½ä½œè¡¥è¯­/ä¸€èˆ¬ä¸ä½œçŠ¶è¯­", "desc": "ä¸èƒ½ä½œè¡¥è¯­ï¼Œå¹¶ä¸”ä¸€èˆ¬ä¸èƒ½åšçŠ¶è¯­ç›´æ¥ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "match_score": 10, "mismatch_score": 0},
    ],
    # çœç•¥å…¶ä»–è¯ç±»è§„åˆ™ï¼Œä¿æŒåŸæ ·
    # 1.2 æ—¶é—´è¯
    "æ—¶é—´è¯": [
        {"name": "T1_å¯ä½œä»‹å®¾æˆ–â€œçš„æ—¶å€™/ä»¥æ¥â€å‰", "desc": "å¯ä»¥ä½œä»‹è¯'åœ¨/åˆ°/ä»'å’ŒåŠ¨è¯æ€§ç»“æ„'ç­‰åˆ°'çš„å®¾è¯­ï¼Œæˆ–åœ¨'çš„æ—¶å€™/ä»¥æ¥'å‰", "match_score": 20, "mismatch_score": -20},
        {"name": "T2_ä¸èƒ½å—ç¨‹åº¦å‰¯è¯", "desc": "ä¸èƒ½å—å‰¯è¯'å¾ˆ'/'ä¸'ä¿®é¥°", "match_score": 10, "mismatch_score": -10},
        {"name": "T3_å¯ä½œä¸å…¸å‹ä¸»è¯­", "desc": "å¯ä»¥åšä¸å…¸å‹çš„ä¸»è¯­ï¼ˆæœ‰äººç§°ä¹‹ä¸ºçŠ¶è¯­ï¼Œæ­¤æ—¶ä¸€èˆ¬å¯åœ¨å‰é¢åŠ 'åœ¨'ï¼‰", "match_score": 10, "mismatch_score": -10},
        {"name": "T4_å¯åšä¸å…¸å‹è°“è¯­", "desc": "å¯ä»¥åšä¸å…¸å‹çš„è°“è¯­ï¼ˆåé™„'äº†'æˆ–å—æ—¶é—´å‰¯è¯ä¿®é¥°æ—¶ï¼Œä¸»è°“ä¹‹é—´ä¸€èˆ¬ä¸èƒ½æ’å…¥'æ˜¯'ï¼‰", "match_score": 10, "mismatch_score": 0},
        {"name": "T5_ä¸èƒ½å¸¦å®¾è¯­å’Œè¡¥è¯­", "desc": "ä¸èƒ½å¸¦å®¾è¯­å’Œè¡¥è¯­ï¼ˆä¸èƒ½ä½œè¿°è¯­ï¼‰", "match_score": 10, "mismatch_score": -10},
        {"name": "T6_å¯ä½œæ—¶é—´ä¸­å¿ƒè¯­/ä½œå®šè¯­", "desc": "ä¸€èˆ¬å¯ä»¥åšä¸­å¿ƒè¯­å—å…¶ä»–æ—¶é—´è¯ä¿®é¥°ï¼Œæˆ–ä½œå®šè¯­ä¿®é¥°æ—¶é—´è¯", "match_score": 10, "mismatch_score": 0},
        {"name": "T7_ä¸€èˆ¬ä¸èƒ½å—åè¯ä¿®é¥°", "desc": "ä¸€èˆ¬ä¸èƒ½ä½œä¸­å¿ƒè¯­å—åè¯ç›´æ¥ä¿®é¥°ï¼Œä¹Ÿä¸èƒ½ä½œå®šè¯­ç›´æ¥ä¿®é¥°åè¯", "match_score": 10, "mismatch_score": 0},
        {"name": "T8_å¯åé™„'çš„'ä½œå®šè¯­ä½†é€šå¸¸ä¸ä½œä¸»å®¾", "desc": "å¯ä»¥åé™„åŠ©è¯'çš„'ä½œå®šè¯­ï¼Œä½†ä¸€èˆ¬ä¸èƒ½ä½œä¸»è¯­å’Œå®¾è¯­", "match_score": 10, "mismatch_score": -10},
        {"name": "T9_å¯ç”¨'ä»€ä¹ˆæ—¶å€™'æé—®/å¯ç”¨'è¿™ä¸ªæ—¶å€™'æŒ‡ä»£", "desc": "å¯ä»¥ç”¨'ä»€ä¹ˆæ—¶å€™'æé—®æˆ–'è¿™ä¸ªæ—¶å€™/é‚£ä¸ªæ—¶å€™'æŒ‡ä»£", "match_score": 10, "mismatch_score": 0},
    ],
    # å…¶ä»–è¯ç±»è§„åˆ™ä¿æŒä¸å˜...
}

MAX_SCORES = {pos: sum(abs(r["match_score"]) for r in rules) for pos, rules in RULE_SETS.items()}

# ===============================
# å·¥å…·å‡½æ•°
# ===============================
def extract_text_from_response(resp_json: Dict[str, Any], provider: str) -> str:
    """æ ¹æ®ä¸åŒæä¾›å•†æå–å“åº”æ–‡æœ¬"""
    if not isinstance(resp_json, dict):
        return ""
    
    try:
        # ä½¿ç”¨æ¯ä¸ªæ¨¡å‹é…ç½®ä¸­å®šä¹‰çš„å“åº”å¤„ç†å™¨
        if provider in MODEL_CONFIGS:
            return MODEL_CONFIGS[provider]["response_handler"](resp_json)
        
        # é€šç”¨æå–æ–¹æ³•
        if "choices" in resp_json:
            choices = resp_json["choices"]
            if isinstance(choices, list) and len(choices) > 0:
                first = choices[0]
                if "message" in first and "content" in first["message"]:
                    return first["message"]["content"]
                if "content" in first:
                    return first["content"]
        
        # é€šä¹‰åƒé—®ç­‰ç‰¹æ®Šæ ¼å¼
        if "output" in resp_json and "text" in resp_json["output"]:
            return resp_json["output"]["text"]
            
    except Exception as e:
        st.warning(f"æå–å“åº”æ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
    
    return json.dumps(resp_json, ensure_ascii=False)

def extract_json_from_text(text: str) -> Tuple[dict, str]:
    if not text:
        return None, ""
    s = text.strip()
    try:
        return json.loads(s), s
    except:
        m = re.search(r"(\{[\s\S]*\})", s)
        if not m:
            return None, s
        cand = m.group(1)
        c = cand.replace("ï¼š", ":").replace("ï¼Œ", ",").replace("â€œ", '"').replace("â€", '"')
        c = re.sub(r"'(\s*[^']+?\s*)'\s*:", r'"\1":', c)
        c = re.sub(r":\s*'([^']*?)'", r': "\1"', c)
        c = re.sub(r",\s*([}\]])", r"\1", c)
        c = re.sub(r"\bTrue\b", "true", c)
        c = re.sub(r"\bFalse\b", "false", c)
        c = re.sub(r"\bNone\b", "null", c)
        try:
            return json.loads(c), c
        except:
            return None, s

def normalize_key(k: str, pos_rules: list) -> str:
    if not isinstance(k, str):
        return None
    kk = re.sub(r'\s+', '', k).upper()
    for r in pos_rules:
        if r["name"].upper() == kk or re.sub(r'\s+', '', r["name"]).upper() == kk:
            return r["name"]
    return None

def map_to_allowed_score(rule: dict, raw_val) -> int:
    match = rule["match_score"]
    mismatch = rule["mismatch_score"]
    if isinstance(raw_val, (int, float)):
        cand = [match, mismatch]
        return min(cand, key=lambda x: abs(x - float(raw_val)))
    if isinstance(raw_val, bool):
        return match if raw_val else mismatch
    if isinstance(raw_val, str):
        s = raw_val.strip().lower()
        if s in ("yes", "y", "true", "æ˜¯", "âˆš", "ç¬¦åˆ"):
            return match
        if s in ("no", "n", "false", "å¦", "Ã—", "ä¸ç¬¦åˆ"):
            return mismatch
    return mismatch

# ===============================
# å®‰å…¨çš„ LLM è°ƒç”¨å‡½æ•°
# ===============================
def call_llm_api(messages: list, provider: str, model: str, api_key: str,
                 max_tokens: int = 1024, temperature: float = 0.0, timeout: int = 30) -> Tuple[bool, dict, str]:
    """
    è°ƒç”¨æŒ‡å®š LLM API è·å–å“åº”ã€‚
    è¿”å›: (æˆåŠŸæ ‡å¿—, å“åº” dict, é”™è¯¯ä¿¡æ¯)
    """
    if not api_key:
        return False, {"error": "API Key ä¸ºç©º"}, "API Key æœªæä¾›"

    if provider not in MODEL_CONFIGS:
        return False, {"error": f"æœªçŸ¥æä¾›å•† {provider}"}, f"æœªçŸ¥æä¾›å•† {provider}"

    cfg = MODEL_CONFIGS[provider]
    url = cfg["base_url"].rstrip("/") + cfg.get("endpoint", "/chat/completions")
    headers = cfg["headers"](api_key)
    payload = cfg["payload"](model, messages, max_tokens=max_tokens, temperature=temperature)

    try:
        # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        with st.expander("æŸ¥çœ‹APIè¯·æ±‚è¯¦æƒ…", expanded=False):
            st.write(f"URL: {url}")
            st.write("Headers:", headers)
            st.write("Payload:", payload)

        r = requests.post(url, headers=headers, json=payload, timeout=timeout)
        
        # æ˜¾ç¤ºå“åº”çŠ¶æ€
        with st.expander("æŸ¥çœ‹APIå“åº”çŠ¶æ€", expanded=False):
            st.write(f"çŠ¶æ€ç : {r.status_code}")
            st.write("å“åº”å†…å®¹:", r.text[:1000])  # åªæ˜¾ç¤ºå‰1000å­—ç¬¦
            
        if r.status_code != 200:
            # å¢å¼ºé”™è¯¯ä¿¡æ¯
            error_detail = f"HTTPé”™è¯¯ {r.status_code}: {r.text[:500]}"
            return False, {"error": error_detail, "content": r.text}, error_detail
            
        r.raise_for_status()
        resp_json = r.json()
        return True, resp_json, ""
    except Exception as e:
        error_msg = str(e)
        st.error(f"APIè°ƒç”¨é”™è¯¯: {error_msg}")
        return False, {"error": error_msg}, error_msg

# ===============================
# å®‰å…¨çš„è¯ç±»åˆ¤å®šå‡½æ•°
# ===============================
def ask_model_for_pos_and_scores(word: str, provider: str, model: str, api_key: str, max_tokens: int, temperature: float) -> Tuple[Dict[str, Dict[str, int]], str, str]:
    """
    æ ¹æ®è¾“å…¥è¯è°ƒç”¨ LLM è·å–è¯ç±»éš¶å±åº¦è¯„åˆ†ï¼Œè¿”å›:
        - scores_all: æ¯ä¸ªè¯ç±»çš„è§„åˆ™å¾—åˆ†å­—å…¸
        - raw_text: æ¨¡å‹åŸå§‹è¾“å‡º
        - predicted_pos: æ¨¡å‹é¢„æµ‹çš„æœ€å¯èƒ½è¯ç±»
    """
    if not word:
        return {}, "", "æœªçŸ¥"

    rules_summary_lines = []
    for pos, rules in RULE_SETS.items():
        rules_summary_lines.append(f"{pos}:")
        for r in rules:
            rules_summary_lines.append(f"  - {r['name']}: {r['desc']} (match={r['match_score']}, mismatch={r['mismatch_score']})")
    rules_text = "\n".join(rules_summary_lines)

    system_msg = (
        "ä½ æ˜¯è¯­è¨€å­¦ç ”ç©¶ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹è§„åˆ™ï¼Œåˆ¤æ–­è¾“å…¥ä¸­æ–‡è¯è¯­çš„è¯ç±»éš¶å±åº¦ã€‚"
        "ä½ çš„ä»»åŠ¡æ˜¯ï¼š1. é¢„æµ‹æœ€å¯èƒ½çš„è¯ç±»ã€‚2. å¯¹æ¯ä¸ªè¯ç±»ï¼Œæ ¹æ®å…¶ä¸‹çš„è§„åˆ™è¿›è¡Œæ‰“åˆ†ï¼ˆæ˜¯/ç¬¦åˆä¸ºmatch_scoreï¼Œå¦/ä¸ç¬¦åˆä¸ºmismatch_scoreï¼‰ã€‚"
        "è¯·ä¸¥æ ¼è¿”å›ä»¥ä¸‹JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–è¯´æ˜æ–‡å­—ï¼š"
        '{"predicted_pos":"<è¯ç±»å>", "scores": {"<è¯ç±»å>": {"<è§„åˆ™å>": <å¾—åˆ†>, ...}, ...}, "explanation":"ç®€è¦è¯´æ˜"}'
    )
    user_prompt = f"è¯è¯­ï¼šã€{word}ã€\nè¯·åŸºäºä¸‹åˆ—è§„åˆ™åˆ¤å®šå¹¶è¯„åˆ†ï¼š\n\n{rules_text}\n\nä»…è¿”å›ä¸¥æ ¼ JSONã€‚"

    ok, resp_json, err_msg = call_llm_api(
        messages=[{"role": "system", "content": system_msg},
                  {"role": "user", "content": user_prompt}],
        provider=provider,
        model=model,
        api_key=api_key,
        max_tokens=max_tokens,
        temperature=temperature
    )

    if not ok or not resp_json:
        # è°ƒç”¨å¤±è´¥æˆ–è¿”å›ä¸ºç©º
        return {}, f"è°ƒç”¨å¤±è´¥æˆ–è¿”å›å¼‚å¸¸: {err_msg}", "æœªçŸ¥"

    # å°è¯•è§£æåŸå§‹æ–‡æœ¬ï¼Œä¼ å…¥providerå‚æ•°ä»¥ä¾¿æ­£ç¡®æå–
    raw_text = extract_text_from_response(resp_json, provider)
    parsed_json, _ = extract_json_from_text(raw_text)
    if not parsed_json:
        return {}, raw_text, "æœªçŸ¥"

    # è§£æå¾—åˆ†
    scores_out = {}
    predicted_pos = parsed_json.get("predicted_pos", "æœªçŸ¥")
    raw_scores = parsed_json.get("scores", {})

    for pos, rules in RULE_SETS.items():
        scores_out[pos] = {r["name"]: 0 for r in rules}
        raw_for_pos = raw_scores.get(pos, {})
        if isinstance(raw_for_pos, dict):
            for k, v in raw_for_pos.items():
                nk = normalize_key(k, rules)
                if nk:
                    rule_def = next(r for r in rules if r["name"] == nk)
                    scores_out[pos][nk] = map_to_allowed_score(rule_def, v)

    return scores_out, raw_text, predicted_pos

# ===============================
# é›·è¾¾å›¾
# ===============================
def plot_radar_chart_streamlit(scores_norm: Dict[str, float], title: str):
    if not scores_norm:
        st.warning("æ— æ³•ç»˜åˆ¶é›·è¾¾å›¾ï¼šæ²¡æœ‰æœ‰æ•ˆæ•°æ®ã€‚")
        return
    categories = list(scores_norm.keys())
    if not categories:
        st.warning("æ— æ³•ç»˜åˆ¶é›·è¾¾å›¾ï¼šæ²¡æœ‰æœ‰æ•ˆè¯ç±»ã€‚")
        return
    values = [float(scores_norm[c]) for c in categories]
    categories += [categories[0]]
    values += [values[0]]

    fig = go.Figure(
        data=[go.Scatterpolar(r=values, theta=categories, fill="toself", name="éš¶å±åº¦")]
    )
    fig.update_layout(
        polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
        showlegend=False, title=dict(text=title, x=0.5)
    )
    st.plotly_chart(fig)

# ===============================
# ä¸»é¡µé¢é€»è¾‘
# ===============================
def main():
    # ä¾§è¾¹æ æ¨¡å‹é€‰æ‹©
    with st.sidebar:
        st.title("æ¨¡å‹è®¾ç½®")
        selected_model = st.selectbox(
            "é€‰æ‹©æ¨¡å‹",
            list(MODEL_OPTIONS.keys())
        )
        
        # æ˜¾ç¤ºé€‰ä¸­æ¨¡å‹çš„ä¿¡æ¯å¹¶å…è®¸ä¿®æ”¹API Key
        model_info = MODEL_OPTIONS[selected_model]
        st.text(f"æä¾›å•†: {model_info['provider']}")
        st.text(f"æ¨¡å‹åç§°: {model_info['model']}")
        st.text(f"APIåœ°å€: {model_info['api_url'][:50]}...")
        
        # å…è®¸ç”¨æˆ·è¾“å…¥API Key
        api_key = st.text_input(
            "API Key",
            value=model_info["api_key"],
            type="password"
        )
        
        # å…¶ä»–å‚æ•°è®¾ç½®
        max_tokens = st.slider("æœ€å¤§ tokens", 512, 4096, 2048)
        temperature = st.slider("æ¸©åº¦å‚æ•°", 0.0, 1.0, 0.0, 0.1)

    # ä¸»é¡µé¢å†…å®¹
    st.title("æ±‰è¯­è¯ç±»éš¶å±åº¦æ£€æµ‹")
    
    # è¾“å…¥è¯è¯­
    word = st.text_input("è¯·è¾“å…¥è¦æ£€æµ‹çš„æ±‰è¯­è¯è¯­", "")
    
    if st.button("å¼€å§‹æ£€æµ‹"):
        if not word:
            st.warning("è¯·è¾“å…¥è¯è¯­åå†æ£€æµ‹")
            return
            
        with st.spinner(f"æ­£åœ¨ä½¿ç”¨ {selected_model} æ£€æµ‹è¯è¯­ã€{word}ã€çš„è¯ç±»éš¶å±åº¦..."):
            # è°ƒç”¨æ¨¡å‹è¿›è¡Œæ£€æµ‹
            scores, raw_text, predicted_pos = ask_model_for_pos_and_scores(
                word=word,
                provider=model_info["provider"],
                model=model_info["model"],
                api_key=api_key,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            # æ˜¾ç¤ºç»“æœ
            st.success(f"æ£€æµ‹å®Œæˆï¼æœ€å¯èƒ½çš„è¯ç±»: {predicted_pos}")
            
            # æ˜¾ç¤ºåŸå§‹å“åº”
            with st.expander("æŸ¥çœ‹æ¨¡å‹åŸå§‹å“åº”", expanded=False):
                st.text(raw_text)
                
            # è®¡ç®—å¹¶æ˜¾ç¤ºå½’ä¸€åŒ–åˆ†æ•°
            if scores:
                st.subheader("è¯ç±»éš¶å±åº¦åˆ†æ•°ï¼ˆå½’ä¸€åŒ–ï¼‰")
                scores_norm = {}
                for pos, pos_scores in scores.items():
                    total = sum(pos_scores.values())
                    max_total = MAX_SCORES.get(pos, 1)  # é¿å…é™¤ä»¥é›¶
                    if max_total == 0:
                        norm_score = 0.0
                    else:
                        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
                        norm_score = (total + max_total) / (2 * max_total)
                        norm_score = max(0.0, min(1.0, norm_score))  # ç¡®ä¿åœ¨0-1ä¹‹é—´
                    scores_norm[pos] = norm_score
                
                # æ˜¾ç¤ºåˆ†æ•°è¡¨æ ¼
                scores_df = pd.DataFrame(list(scores_norm.items()), columns=["è¯ç±»", "éš¶å±åº¦"])
                scores_df = scores_df.sort_values(by="éš¶å±åº¦", ascending=False)
                st.dataframe(scores_df.style.format({"éš¶å±åº¦": "{:.2%}"}))
                
                # ç»˜åˆ¶é›·è¾¾å›¾
                st.subheader("è¯ç±»éš¶å±åº¦é›·è¾¾å›¾")
                plot_radar_chart_streamlit(scores_norm, f"è¯è¯­ã€{word}ã€çš„è¯ç±»éš¶å±åº¦åˆ†å¸ƒ")

if __name__ == "__main__":
    main()



