import streamlit as st
import requests
import json
import re
import os
import pandas as pd
import plotly.graph_objects as go
import io
import time  # <--- å¿…é¡»æ·»åŠ è¿™è¡Œï¼Œç”¨äºé™é€Ÿå’Œé‡è¯•
from typing import Tuple, Dict, Any, List
from openpyxl import load_workbook
from openpyxl.styles import PatternFill

# ===============================
# é¡µé¢é…ç½®
# ===============================
st.set_page_config(
    page_title="æ±‰è¯­è¯ç±»éš¶å±åº¦æ£€æµ‹åˆ’ç±»",
    page_icon="ğŸ“°",
    layout="wide",  # ä½¿ç”¨å®½å¸ƒå±€
    initial_sidebar_state="collapsed",  # é»˜è®¤æŠ˜å ä¾§è¾¹æ 
    menu_items=None
)

# è‡ªå®šä¹‰CSSæ ·å¼
hide_streamlit_style = """
<style>
/* éšè—é¡¶éƒ¨èœå•æ å’Œé¡µè„š */
header {visibility: hidden;}
footer {visibility: hidden;}

/* è°ƒæ•´è¡¨æ ¼æ ·å¼ */
.dataframe {font-size: 12px;}

/* éšè—é»˜è®¤çš„ä¾§è¾¹æ  */
[data-testid="stSidebar"] {
    display: none !important;
}

/* ä¸ºé¡¶éƒ¨æ§åˆ¶åŒºæ·»åŠ è¾¹æ¡†å’ŒèƒŒæ™¯è‰²ï¼Œä½¿å…¶çœ‹èµ·æ¥åƒä¸€ä¸ªå›ºå®šçš„é¢æ¿ */
.stApp > div:first-child {
    padding-top: 2rem;
}
</style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# ===============================
# æ¨¡å‹é…ç½® (ä¿®æ”¹ç‰ˆï¼šå¯ç”¨æµå¼ Stream)
# ===============================
MODEL_CONFIGS = {
    "deepseek": {
        "base_url": "https://api.deepseek.com/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": kw.get("max_tokens", 4096), 
            "temperature": kw.get("temperature", 0.0), 
            "stream": True, 
        },
    },
    "openai": {
        "base_url": "https://api.openai.com/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": kw.get("max_tokens", 4096), 
            "temperature": kw.get("temperature", 0.0), 
            "stream": True,
        },
    },
    "moonshot": {
        "base_url": "https://api.moonshot.cn/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": kw.get("max_tokens", 4096), 
            "temperature": kw.get("temperature", 0.0), 
            "stream": True,
        },
    },
    "qwen": {
        "base_url": "https://dashscope.aliyuncs.com/api/v1",
        "endpoint": "/services/aigc/text-generation/generation",
        "headers": lambda key: {
            "Authorization": f"Bearer {key}", 
            "Content-Type": "application/json",
            "X-DashScope-SSE": "enable",  # æ˜¾å¼å¼€å¯ SSE
            "Accept": "text/event-stream" # å…³é”®ï¼šå‘Šè¯‰æœåŠ¡å™¨æˆ‘ä»¬è¦æµå¼
        },
        "payload": lambda model, messages, **kw: {
            "model": model, 
            "input": {"messages": messages}, 
            "parameters": {
                "max_tokens": kw.get("max_tokens", 4096), 
                "temperature": kw.get("temperature", 0.0),
                "result_format": "message",
                "incremental_output": True 
            },
        },
    },
}

# æ¨¡å‹é€‰é¡¹ï¼ˆä»…ä»ç¯å¢ƒå˜é‡è·å–API Keyï¼Œå·²ç§»é™¤é»˜è®¤å€¼ï¼‰
MODEL_OPTIONS = {
    "DeepSeek Chat": {
        "provider": "deepseek", 
        "model": "deepseek-chat", 
        "api_key": os.getenv("DEEPSEEK_API_KEY"),
        "env_var": "DEEPSEEK_API_KEY"
    },
    "OpenAI GPT-4oï¼ˆæ¨èï¼‰": {
        "provider": "openai", 
        "model": "gpt-4o-mini", 
        "api_key": os.getenv("OPENAI_API_KEY"),
        "env_var": "OPENAI_API_KEY"
    },
    "Moonshotï¼ˆKimiï¼‰": {
        "provider": "moonshot", 
        "model": "moonshot-v1-32k", 
        "api_key": os.getenv("MOONSHOT_API_KEY"),
        "env_var": "MOONSHOT_API_KEY"
    },
    "Qwenï¼ˆé€šä¹‰åƒé—®ï¼‰": {
        "provider": "qwen", 
        "model": "qwen-max", 
        "api_key": os.getenv("QWEN_API_KEY"),
        "env_var": "QWEN_API_KEY"
    },
}

# è¿‡æ»¤æ‰æ²¡æœ‰é…ç½® API Key çš„æ¨¡å‹ï¼Œåªä¿ç•™å¯ç”¨çš„
AVAILABLE_MODEL_OPTIONS = {
    name: info for name, info in MODEL_OPTIONS.items() if info["api_key"]
}

# å¦‚æœæ²¡æœ‰å¯ç”¨æ¨¡å‹ï¼Œåˆ™æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹ä½†ç»™å‡ºè­¦å‘Š
if not AVAILABLE_MODEL_OPTIONS:
    AVAILABLE_MODEL_OPTIONS = MODEL_OPTIONS

# ===============================
# è¯ç±»è§„åˆ™ä¸æœ€å¤§å¾—åˆ†
# ===============================
RULE_SETS = {
    # 1.1 åè¯
    "åè¯": [
        {"name": "N1_å¯å—æ•°é‡è¯ä¿®é¥°", "desc": "å¯ä»¥å—æ•°é‡è¯ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "N2_ä¸èƒ½å—å‰¯è¯ä¿®é¥°", "desc": "ä¸èƒ½å—å‰¯è¯ä¿®é¥°", "match_score": 20, "mismatch_score": -20},
        {"name": "N3_å¯ä½œä¸»å®¾è¯­", "desc": "å¯ä»¥åšå…¸å‹çš„ä¸»è¯­æˆ–å®¾è¯­", "match_score": 20, "mismatch_score": 0},
        {"name": "N4_å¯ä½œä¸­å¿ƒè¯­æˆ–ä½œå®šè¯­", "desc": "å¯ä»¥åšä¸­å¿ƒè¯­å—å…¶ä»–åè¯ä¿®é¥°ï¼Œæˆ–è€…ä½œå®šè¯­ç›´æ¥ä¿®é¥°å…¶ä»–åè¯", "match_score": 10, "mismatch_score": 0},
        {"name": "N5_å¯åé™„çš„å­—ç»“æ„", "desc": "å¯ä»¥åé™„åŠ©è¯â€œçš„â€æ„æˆâ€œçš„â€å­—ç»“æ„", "match_score": 10, "mismatch_score": 0},
        {"name": "N6_å¯åé™„æ–¹ä½è¯æ„å¤„æ‰€", "desc": "å¯ä»¥åé™„æ–¹ä½è¯æ„æˆå¤„æ‰€ç»“æ„", "match_score": 10, "mismatch_score": 0},
        {"name": "N7_ä¸èƒ½ä½œè°“è¯­æ ¸å¿ƒ", "desc": "ä¸èƒ½åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒï¼ˆä¸èƒ½å¸¦å®¾è¯­ï¼Œä¸èƒ½å—çŠ¶è¯­å’Œè¡¥è¯­ï¼Œä¸èƒ½åé™„æ—¶ä½“åŠ©è¯ï¼‰", "match_score": 10, "mismatch_score": -10},
        {"name": "N8_ä¸èƒ½ä½œè¡¥è¯­/ä¸€èˆ¬ä¸ä½œçŠ¶è¯­", "desc": "ä¸èƒ½ä½œè¡¥è¯­ï¼Œå¹¶ä¸”ä¸€èˆ¬ä¸èƒ½åšçŠ¶è¯­ç›´æ¥ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "match_score": 10, "mismatch_score": 0},
    ],
    # 1.5 åŠ¨è¯
    "åŠ¨è¯": [
        {"name": "V1_å¯å—å¦å®š'ä¸/æ²¡æœ‰'ä¿®é¥°", "desc": "å¯ä»¥å—å¦å®šå‰¯è¯'ä¸'æˆ–'æ²¡æœ‰'ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "V2_å¯åé™„/æ’å…¥æ—¶ä½“åŠ©è¯'ç€/äº†/è¿‡'", "desc": "å¯ä»¥åé™„æˆ–ä¸­é—´æ’å…¥æ—¶ä½“åŠ©è¯'ç€/äº†/è¿‡'ï¼Œæˆ–è¿›å…¥'...äº†æ²¡æœ‰'æ ¼å¼", "match_score": 10, "mismatch_score": 0},
        {"name": "V3_å¯å¸¦çœŸå®¾è¯­æˆ–é€šè¿‡ä»‹è¯å¼•å¯¼è®ºå…ƒ", "desc": "å¯ä»¥å¸¦çœŸå®¾è¯­ï¼Œæˆ–é€šè¿‡'å’Œ/ä¸º/å¯¹/å‘/æ‹¿/äº'ç­‰ä»‹è¯å¼•å¯¼è®ºå…ƒ", "match_score": 20, "mismatch_score": 0},
        {"name": "V4_ç¨‹åº¦å‰¯è¯ä¸å¸¦å®¾è¯­çš„å…³ç³»", "desc": "ä¸èƒ½å—ç¨‹åº¦å‰¯è¯'å¾ˆ'ä¿®é¥°ï¼Œæˆ–èƒ½åŒæ—¶å—'å¾ˆ'ä¿®é¥°å¹¶å¸¦å®¾è¯­ï¼ˆæŒ‰æ¡ç›®ç»™äºˆå¾—åˆ†ï¼‰", "match_score": 10, "mismatch_score": -10},
        {"name": "V5_å¯æœ‰é‡å /æ­£åé‡å å½¢å¼", "desc": "å¯ä»¥æœ‰'VV, Vä¸€V, Väº†V, Vä¸V, Väº†æ²¡æœ‰'ç­‰å½¢å¼", "match_score": 10, "mismatch_score": 0},
        {"name": "V6_å¯åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒ", "desc": "å¯ä»¥åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒï¼ˆä¸€èˆ¬å¯å—çŠ¶è¯­æˆ–è¡¥è¯­ä¿®é¥°ï¼‰", "match_score": 10, "mismatch_score": -10},
        {"name": "V7_ä¸èƒ½ä½œçŠ¶è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "desc": "ä¸èƒ½ä½œçŠ¶è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "match_score": 10, "mismatch_score": 0},
        {"name": "V8_å¯ä½œ'æ€ä¹ˆ/æ€æ ·'æé—®æˆ–'è¿™ä¹ˆ/è¿™æ ·/é‚£ä¹ˆ'å›ç­”", "desc": "å¯ä»¥è·Ÿåœ¨'æ€ä¹ˆ/æ€æ ·'ä¹‹åæé—®æˆ–è·Ÿåœ¨'è¿™ä¹ˆ/è¿™æ ·/é‚£ä¹ˆ'ä¹‹åå›ç­”", "match_score": 10, "mismatch_score": 0},
        {"name": "V9_ä¸èƒ½è·Ÿåœ¨'å¤š/å¤šä¹ˆ'ä¹‹åæé—®æˆ–è¡¨ç¤ºæ„Ÿå¹", "desc": "ä¸èƒ½è·Ÿåœ¨'å¤š'ä¹‹åå¯¹æ€§è´¨æé—®ï¼Œä¸èƒ½è·Ÿåœ¨'å¤šä¹ˆ'ä¹‹åè¡¨ç¤ºæ„Ÿå¹", "match_score": 10, "mismatch_score": -10},
    ],
    # 4.6 ååŠ¨è¯
    "ååŠ¨è¯": [
        {"name": "NV1_å¯è¢«\"ä¸/æ²¡æœ‰\"å¦å®šä¸”è‚¯å®šå½¢å¼-1", "desc": "å¯ä»¥ç”¨\"ä¸\"å’Œ\"æ²¡æœ‰\"æ¥å¦å®šï¼Œå¹¶ä¸”\"æ²¡æœ‰â€¦â€¦\"çš„è‚¯å®šå½¢å¼å¯ä»¥æ˜¯\"â€¦â€¦äº†\"å’Œ\"æœ‰â€¦â€¦\"(å‰ä¸€ç§æƒ…å†µä¸­çš„\"æ²¡æœ‰\"æ˜¯å‰¯è¯ï¼Œåä¸€ç§æƒ…å†µä¸­çš„\"æ²¡æœ‰\"æ˜¯åŠ¨è¯)", "match_score": 10, "mismatch_score": -10},
        {"name": "NV2_å¯é™„æ—¶ä½“åŠ©è¯æˆ–è¿›å…¥\"â€¦â€¦äº†æ²¡æœ‰\"æ ¼å¼", "desc": "å¯ä»¥åé™„æ—¶ä½“åŠ©è¯\"ç€ã€äº†ã€è¿‡\"ï¼Œæˆ–è€…å¯ä»¥è¿›å…¥\"â€¦â€¦â€¦äº†æ²¡æœ‰\"æ ¼å¼", "match_score": 10, "mismatch_score": -10},
        {"name": "NV3_å¯å¸¦çœŸå®¾è¯­ä¸”ä¸å—\"å¾ˆ\"ä¿®é¥°", "desc": "å¯ä»¥å¸¦çœŸå®¾è¯­ï¼Œå¹¶ä¸”ä¸èƒ½å—ç¨‹åº¦å‰¯è¯\"å¾ˆ\"ç­‰ä¿®é¥°", "match_score": 10, "mismatch_score": -10},
        {"name": "NV4_æœ‰é‡å å’Œæ­£åé‡å å½¢å¼", "desc": "å¯ä»¥æœ‰\"VVã€Vä¸€Vã€Väº†Vã€Vä¸V\"ç­‰é‡å å’Œæ­£åé‡å å½¢å¼", "match_score": 10, "mismatch_score": 0},
        {"name": "NV5_å¯ä½œå¤šç§å¥æ³•æˆåˆ†ä¸”å¯ä½œå½¢å¼åŠ¨è¯å®¾è¯­", "desc": "æ—¢å¯ä»¥ä½œè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒï¼Œåˆå¯ä»¥ä½œä¸»è¯­æˆ–å®¾è¯­ï¼Œå¹¶ä¸”ï¼Œå¯ä»¥ä½œå½¢å¼åŠ¨è¯\"ä½œã€è¿›è¡Œã€åŠ ä»¥ã€ç»™äºˆã€å—åˆ°\"ç­‰çš„å®¾è¯­", "match_score": 10, "mismatch_score": -10},
        {"name": "NV6_ä¸èƒ½ç›´æ¥ä½œçŠ¶è¯­", "desc": "ä¸èƒ½ç›´æ¥ä½œçŠ¶è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "match_score": 10, "mismatch_score": -10},
        {"name": "NV7_å¯ä¿®é¥°åè¯æˆ–å—åè¯/æ•°é‡è¯ä¿®é¥°", "desc": "å¯ä»¥ä¿®é¥°åè¯æˆ–è€…å—åè¯ä¿®é¥°ï¼Œæˆ–è€…å¯ä»¥å—æ•°é‡è¯ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "NV8_å¯è·Ÿåœ¨\"æ€ä¹ˆ/æ€æ ·/è¿™ä¹ˆ/è¿™æ ·/é‚£ä¹ˆ/é‚£æ ·\"ä¹‹å", "desc": "å¯ä»¥è·Ÿåœ¨\"æ€ä¹ˆã€æ€æ ·\"ä¹‹åï¼Œå¯¹åŠ¨ä½œçš„æ–¹å¼è¿›è¡Œæé—®ï¼Œå¹¶ä¸”å¯ä»¥è·Ÿåœ¨\"è¿™ä¹ˆã€è¿™æ ·ã€é‚£ä¹ˆã€é‚£æ ·\"ä¹‹åï¼Œç”¨ä»¥ä½œå‡ºç›¸åº”çš„å›ç­”", "match_score": 10, "mismatch_score": 0},
        {"name": "NV9_ä¸èƒ½è·Ÿåœ¨\"å¤š/å¤šä¹ˆ\"ä¹‹å", "desc": "ä¸èƒ½è·Ÿåœ¨\"å¤š\"ä¹‹åï¼Œå¯¹æ€§è´¨çš„ç¨‹åº¦è¿›è¡Œæé—®ï¼Œä¹Ÿä¸èƒ½è·Ÿåœ¨\"å¤šä¹ˆ\"ä¹‹åï¼Œè¡¨ç¤ºæ„Ÿå¹", "match_score": 10, "mismatch_score": -10},
        {"name": "NV10_å¯åé™„æ–¹ä½è¯æ„æˆå¤„æ‰€ç»“æ„", "desc": "å¯ä»¥åé™„æ–¹ä½è¯æ„æˆå¤„æ‰€ç»“æ„(ç„¶åä½œâ€œåœ¨ã€åˆ°ã€ä»â€ç­‰ä»‹è¯çš„å®¾è¯­ï¼Œè¿™ç§ä»‹è¯ç»“æ„åˆå¯ä»¥ä½œçŠ¶è¯­æˆ–è¡¥è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†)", "match_score": 10, "mismatch_score": 0},
    ]
}

# ===============================
# å·¥å…·å‡½æ•°
# ===============================
def extract_text_from_response(resp_json: Dict[str, Any]) -> str:
    """ä»ä¸åŒæ ¼å¼çš„LLMå“åº”ä¸­å®‰å…¨æå–æ–‡æœ¬å†…å®¹ã€‚"""
    if not isinstance(resp_json, dict):
        return ""
    try:
        # Qwen æ ¼å¼
        if "output" in resp_json and "text" in resp_json["output"]:
            return resp_json["output"]["text"]
        
        # OpenAI/DeepSeek/Moonshot æ ¼å¼
        if "choices" in resp_json and len(resp_json["choices"]) > 0:
            choice = resp_json["choices"][0]
            if "message" in choice and "content" in choice["message"]:
                return choice["message"]["content"]
            
        # å…œåº•
        return json.dumps(resp_json, ensure_ascii=False)
    except Exception as e:
        # st.error(f"æå–æ–‡æœ¬å¤±è´¥: {e}")
        return json.dumps(resp_json, ensure_ascii=False)

def extract_json_from_text(text: str) -> Tuple[Dict[str, Any], str]:
    """
    ã€æ–°å¢ã€‘ä»åŒ…å«æ¨ç†è¿‡ç¨‹å’ŒJSONçš„æ··åˆæ–‡æœ¬ä¸­æå–å¹¶è§£æJSONå¯¹è±¡ã€‚
    å¯»æ‰¾æœ€å¤–å±‚å¤§æ‹¬å· {} åŒ…è£¹çš„JSONç»“æ„ã€‚
    """
    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä»¥ '{' å¼€å§‹ï¼Œä»¥ '}' ç»“æŸçš„æœ€å¤–å±‚ç»“æ„
    match = re.search(r"(\{.*\})", text.strip(), re.DOTALL)
    
    if not match:
        return None, text

    json_text = match.group(1).strip()
    
    # å°è¯•è§£æ
    try:
        parsed_json = json.loads(json_text)
        return parsed_json, json_text
    except json.JSONDecodeError as e:
        # st.error(f"JSONè§£æå¤±è´¥: {e}. å°è¯•è§£æçš„æ–‡æœ¬ç‰‡æ®µ:\n{json_text}")
        return None, json_text

def normalize_key(k: str, pos_rules: list) -> str:
    """æ ‡å‡†åŒ–æ¨¡å‹è¿”å›çš„è§„åˆ™åç§°ï¼Œç¡®ä¿åŒ¹é…åˆ° RULE_SETS ä¸­çš„é”®ã€‚"""
    if not isinstance(k, str): return None
    # ç§»é™¤ç©ºæ ¼å’Œä¸‹åˆ’çº¿ï¼Œè½¬ä¸ºå¤§å†™è¿›è¡ŒåŒ¹é…
    k_norm = re.sub(r'[\s_]+', '', k).upper()
    for r in pos_rules:
        r_norm = re.sub(r'[\s_]+', '', r["name"]).upper()
        if r_norm == k_norm:
            return r["name"]
    return None

def map_to_allowed_score(rule: dict, raw_val) -> int:
    """å°†æ¨¡å‹è¿”å›çš„å¸ƒå°”å€¼/å­—ç¬¦ä¸²æ˜ å°„ä¸ºè§„åˆ™å®šä¹‰çš„ match_score æˆ– mismatch_scoreã€‚"""
    match_score, mismatch_score = rule["match_score"], rule["mismatch_score"]
    
    if isinstance(raw_val, bool):
        return match_score if raw_val is True else mismatch_score
    
    if isinstance(raw_val, str):
        s = raw_val.strip().lower()
        if s in ("yes", "y", "true", "æ˜¯", "âˆš", "ç¬¦åˆ"):
            return match_score
        if s in ("no", "n", "false", "å¦", "Ã—", "ä¸ç¬¦åˆ"):
            return mismatch_score
            
    # å³ä½¿æ¨¡å‹é”™è¯¯åœ°è¿”å›äº†æ•°å€¼ï¼Œä¹Ÿå°è¯•åŒ¹é…è§„åˆ™åˆ†ï¼Œå¦åˆ™é»˜è®¤ä¸åŒ¹é…
    if isinstance(raw_val, (int, float)):
        raw_val_int = int(raw_val)
        if raw_val_int == match_score: return match_score
        if raw_val_int == mismatch_score: return mismatch_score
    
    # é»˜è®¤è¿”å›ä¸åŒ¹é…å¾—åˆ†
    return mismatch_score

def calculate_membership(scores_all: Dict[str, Dict[str, int]]) -> Dict[str, float]:
    """è®¡ç®—éš¶å±åº¦ï¼šæ€»åˆ†é™¤ä»¥ 100ï¼Œå¹¶é™åˆ¶åœ¨ [-1, 1] åŒºé—´ã€‚"""
    membership = {}
    for pos, scores in scores_all.items():
        total_score = sum(scores.values())
        # æ€»å¾—åˆ†é™¤ä»¥100å¾—åˆ°éš¶å±åº¦ï¼ˆå‡ ååˆ†å¯¹åº”é›¶ç‚¹å‡ ï¼‰
        normalized = total_score / 100
        # é™åˆ¶åœ¨ [-1.0, 1.0] åŒºé—´
        membership[pos] = max(-1.0, min(1.0, normalized))
    return membership

def get_top_10_positions(membership: Dict[str, float]) -> List[Tuple[str, float]]:
    """è·å–éš¶å±åº¦æœ€é«˜çš„å‰ 10 ä¸ªè¯ç±»ã€‚"""
    return sorted(membership.items(), key=lambda x: x[1], reverse=True)[:10]

# ===============================
# å®‰å…¨çš„ LLM è°ƒç”¨å‡½æ•° (æµå¼ç‰ˆ)
# ===============================
def call_llm_api_cached(_provider, _model, _api_key, messages, max_tokens=4096, temperature=0.0):
    """
    å°è£…è¯·æ±‚é€»è¾‘ï¼Œä½¿ç”¨æµå¼ä¼ è¾“ (Streaming) è§£å†³è¶…æ—¶é—®é¢˜ã€‚
    é€æ­¥æ¥æ”¶æ•°æ®å¹¶æ‹¼æ¥ï¼Œæœ€åè¿”å›å®Œæ•´çš„å“åº”ç»“æ„ã€‚
    """
    if not _api_key: return False, {"error": "API Key ä¸ºç©º"}, "API Key æœªæä¾›"
    if _provider not in MODEL_CONFIGS: return False, {"error": f"æœªçŸ¥æä¾›å•† {_provider}"}, f"æœªçŸ¥æä¾›å•† {_provider}"

    cfg = MODEL_CONFIGS[_provider]
    url = f"{cfg['base_url'].rstrip('/')}{cfg['endpoint']}"
    headers = cfg["headers"](_api_key)
    payload = cfg["payload"](_model, messages, max_tokens=max_tokens, temperature=temperature)

    # ç”¨äºåœ¨ç•Œé¢ä¸Šå®æ—¶æ˜¾ç¤ºè¿›åº¦çš„å ä½ç¬¦ï¼ˆå¯é€‰ï¼Œæå‡ä½“éªŒï¼‰
    streaming_placeholder = st.empty()
    full_content = ""

    try:
        # 1. å¼€å¯ stream=True
        with requests.post(url, headers=headers, json=payload, stream=True, timeout=60) as response:
            response.raise_for_status()
            
            # 2. é€è¡Œè¯»å–æµå¼å“åº”
            for line in response.iter_lines():
                if not line: continue
                
                # è§£ç å¹¶å»é™¤å‰ç¼€
                line_text = line.decode('utf-8').strip()
                
                # å¤„ç† SSE æ ¼å¼ (é€šå¸¸ä»¥ "data: " å¼€å¤´)
                if line_text.startswith("data:"):
                    json_str = line_text[5:].strip() # å»æ‰ "data:"
                else:
                    # éƒ¨åˆ†éæ ‡å‡†æµå¯èƒ½ä¸å¸¦ data: å‰ç¼€ï¼Œç›´æ¥å°è¯•è§£æ
                    json_str = line_text

                # é‡åˆ°ç»“æŸæ ‡è®°åœæ­¢
                if json_str == "[DONE]":
                    break
                
                try:
                    chunk = json.loads(json_str)
                    
                    # --- æå–æ–‡æœ¬ç‰‡æ®µ (Delta) ---
                    delta_text = ""
                    
                    # æƒ…å†µ A: OpenAI / DeepSeek / Moonshot æ ¼å¼
                    if "choices" in chunk and len(chunk["choices"]) > 0:
                        delta = chunk["choices"][0].get("delta", {})
                        delta_text = delta.get("content", "")
                    
                    # æƒ…å†µ B: Qwen Native æ ¼å¼ (incremental_output=True)
                    elif "output" in chunk:
                        # Qwen Native åœ¨ incremental_output=True æ—¶ï¼Œoutput.text æ˜¯å¢é‡
                        output = chunk["output"]
                        if "choices" in output and len(output["choices"]) > 0:
                             # Qwen å…¼å®¹ message æ ¼å¼
                             msg = output["choices"][0].get("message", {})
                             delta_text = msg.get("content", "")
                        elif "text" in output:
                             # Qwen çº¯æ–‡æœ¬æ ¼å¼
                             delta_text = output["text"]

                    if delta_text:
                        full_content += delta_text
                        # (å¯é€‰) å®æ—¶åœ¨ç•Œé¢å±•ç¤ºéƒ¨åˆ†å†…å®¹ï¼Œè®©ç”¨æˆ·çŸ¥é“æ²¡æ­»æœº
                        # streaming_placeholder.markdown(full_content + "â–Œ")

                except json.JSONDecodeError:
                    continue
        
        # æ¸…é™¤æµå¼å ä½ç¬¦
        streaming_placeholder.empty()

        # 3. æ„é€ ä¸€ä¸ªæ¨¡æ‹Ÿçš„å®Œæ•´å“åº”ï¼Œä»¥ä¾¿å…¼å®¹åç»­çš„ extract_text_from_response å‡½æ•°
        # è¿™æ ·æ‚¨å°±ä¸éœ€è¦ä¿®æ”¹åé¢çš„ä»£ç äº†
        mock_response = {
            "choices": [{"message": {"content": full_content}}], # OpenAI é£æ ¼
            "output": {"text": full_content} # Qwen é£æ ¼å…¼å®¹
        }
        
        if not full_content:
             return False, {"error": "æœªæ¥æ”¶åˆ°æœ‰æ•ˆå†…å®¹"}, "æ¨¡å‹æœªè¿”å›å†…å®¹"

        return True, mock_response, ""

    except requests.exceptions.RequestException as e:
        error_msg = f"ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {str(e)}"
        return False, {"error": error_msg}, error_msg
    except Exception as e:
        error_msg = f"æµå¼å¤„ç†æœªçŸ¥é”™è¯¯: {str(e)}\nå·²æ¥æ”¶å†…å®¹: {full_content[:100]}..."
        return False, {"error": error_msg}, error_msg

# ===============================
# è¯ç±»åˆ¤å®šä¸»å‡½æ•°
# ===============================
def ask_model_for_pos_and_scores(word: str, provider: str, model: str, api_key: str) -> Tuple[Dict[str, Dict[str, int]], str, str, str]:
    if not word:
        return {}, "", "æœªçŸ¥", ""

    # è§„åˆ™æ–‡å­—è¯´æ˜ï¼ˆç»™æ¨¡å‹çœ‹ï¼Œè®©å®ƒè€è€å®å®æŒ‰è§„åˆ™æ¥åˆ¤æ–­ï¼‰
    full_rules_by_pos = {
        pos: "\n".join([
            f"- {r['name']}: {r['desc']}ï¼ˆç¬¦åˆ: {r['match_score']} åˆ†ï¼Œä¸ç¬¦åˆ: {r['mismatch_score']} åˆ†ï¼‰"
            for r in rules
        ])
        for pos, rules in RULE_SETS.items()
    }

    # ===== ç³»ç»Ÿæç¤ºï¼šåªå…è®¸è¾“å‡ºâ€œç¬¦åˆ/ä¸ç¬¦åˆâ€ï¼Œç¦æ­¢è‡ªå·±æ‰“æ•°å­—åˆ† =====
    system_msg = f"""ä½ æ˜¯ä¸€åä¸­æ–‡è¯æ³•ä¸è¯­æ³•æ–¹é¢çš„ä¸“å®¶ã€‚ç°åœ¨è¦åˆ†æè¯è¯­ã€Œ{word}ã€åœ¨ä¸‹åˆ—è¯ç±»ä¸­çš„è¡¨ç°ï¼š

- éœ€è¦åˆ¤æ–­çš„è¯ç±»ï¼šåè¯ã€åŠ¨è¯ã€ååŠ¨è¯
- è¯„åˆ†è§„åˆ™å·²ç»ç”±ç³»ç»Ÿå®šä¹‰ï¼Œä½ **ä¸è¦**è‡ªå·±è®¾è®¡åˆ†å€¼ï¼Œä¹Ÿ**ä¸è¦**åœ¨ JSON ä¸­ç»™å‡ºå…·ä½“æ•°å­—åˆ†æ•°ã€‚ç¨‹åºå°†æ ¹æ®ä½ çš„åˆ¤æ–­ï¼ˆtrue/falseï¼‰è‡ªåŠ¨èµ‹å€¼ã€‚
- ä½ åªéœ€è¦åˆ¤æ–­æ¯ä¸€æ¡è§„åˆ™æ˜¯â€œç¬¦åˆâ€è¿˜æ˜¯â€œä¸ç¬¦åˆâ€ã€‚

ã€å„è¯ç±»çš„è§„åˆ™è¯´æ˜ï¼ˆä»…ä¾›ä½ åˆ¤æ–­ä½¿ç”¨ï¼‰ã€‘

ã€åè¯ã€‘
{full_rules_by_pos["åè¯"]}

ã€åŠ¨è¯ã€‘
{full_rules_by_pos["åŠ¨è¯"]}

ã€ååŠ¨è¯ã€‘
{full_rules_by_pos["ååŠ¨è¯"]}

ã€è¾“å‡ºè¦æ±‚ã€‘

1. åœ¨ explanation å­—æ®µä¸­ï¼Œå¿…é¡»**é€æ¡è§„åˆ™**è¯´æ˜åˆ¤æ–­ä¾æ®ï¼Œå¹¶ä¸¾ä¾‹ï¼ˆå¯ä»¥è‡ªå·±é€ å¥ï¼‰ï¼š
   - æ ¼å¼ç¤ºä¾‹ï¼š
     - ã€Œåè¯-N1_å¯å—æ•°é‡è¯ä¿®é¥°ï¼šç¬¦åˆã€‚ç†ç”±ï¼šâ€¦â€¦ã€‚ä¾‹å¥ï¼šâ€¦â€¦ã€‚ã€
     - ã€ŒåŠ¨è¯-V2_å¯åé™„/æ’å…¥æ—¶ä½“åŠ©è¯'ç€/äº†/è¿‡'ï¼šä¸ç¬¦åˆã€‚ç†ç”±ï¼šâ€¦â€¦ã€‚ä¾‹å¥ï¼šâ€¦â€¦ã€‚ã€
   - explanation é‡Œè¦è¦†ç›– **ä¸‰ä¸ªè¯ç±»çš„æ‰€æœ‰è§„åˆ™**ï¼Œä¸èƒ½åªå†™å‡ æ¡ã€‚

2. åœ¨ JSON ä¸­çš„ scores å­—æ®µé‡Œï¼š
   - æ¯ä¸€ç±»ä¸‹çš„æ¯ä¸€æ¡è§„åˆ™ï¼Œåªèƒ½ç»™å‡º **å¸ƒå°”å€¼ true / false**ï¼Œè¡¨ç¤ºæ˜¯å¦ç¬¦åˆè¯¥è§„åˆ™
   - ä¸¥ç¦åœ¨ scores é‡Œä½¿ç”¨æ•°å€¼åˆ†æ•°ï¼ˆä¾‹å¦‚ 0, 5, 10 ç­‰ï¼‰
   - å¦‚æœä½ ä¸ç¡®å®šï¼Œä¹Ÿå¿…é¡»åšå‡ºåˆ¤æ–­ï¼ˆtrue æˆ– falseï¼‰ï¼Œä¸è¦ç”¨ nullã€0 æˆ–å…¶å®ƒå€¼
   - JSON ç»“æ„å¿…é¡»æ˜¯ï¼š{{"explanation": "...", "predicted_pos": "...", "scores": {{"åè¯": {{...}}, "åŠ¨è¯": {{...}}, "ååŠ¨è¯": {{...}}}}}}

3. predicted_posï¼š
   - è¯·é€‰æ‹©ã€Œåè¯ã€ã€ŒåŠ¨è¯ã€ã€ŒååŠ¨è¯ã€ä¹‹ä¸€ï¼Œä½œä¸ºè¯¥è¯è¯­æœ€å…¸å‹çš„è¯ç±»ã€‚

4. **æœ€åè¾“å‡ºæ—¶ï¼Œå…ˆå†™è¯¦ç»†çš„æ–‡å­—æ¨ç†ï¼Œæœ€åå•ç‹¬ä¸”å®Œæ•´åœ°ç»™å‡ºä¸€æ®µåˆæ³•çš„ JSONï¼ˆä¸è¦å†åŠ æ³¨é‡Šï¼‰ã€‚**
"""

    # ç”¨æˆ·æç¤ºï¼šå†å¼ºè°ƒä¸€æ¬¡
    user_prompt = f"""
è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è¦æ±‚åˆ†æè¯è¯­ã€Œ{word}ã€ã€‚

ç‰¹åˆ«æ³¨æ„ï¼š
- åœ¨ JSON çš„ scores éƒ¨åˆ†ï¼Œåªèƒ½ç”¨ true/false è¡¨ç¤ºâ€œæ˜¯å¦ç¬¦åˆè§„åˆ™â€ï¼Œä¸èƒ½ä½¿ç”¨ä»»ä½•æ•°å­—ã€‚
- explanation ä¸­å¿…é¡»å¯¹æ¯ä¸€æ¡è§„åˆ™å†™æ˜â€œç¬¦åˆ/ä¸ç¬¦åˆ + ç†ç”± + ä¾‹å¥â€ã€‚

è¯·å…ˆç»™å‡ºè¯¦ç»†æ¨ç†è¿‡ç¨‹ï¼Œç„¶ååœ¨æœ€åå•ç‹¬è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ã€‚
"""

    with st.spinner(f"æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ ({model}) è¿›è¡Œåˆ†æï¼Œè¯·ç¨å€™..."):
        ok, resp_json, err_msg = call_llm_api_cached(
            _provider=provider,
            _model=model,
            _api_key=api_key,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_prompt}
            ]
        )

    if not ok:
        st.error(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {err_msg}")
        # è¿”å›ä¸€ä¸ªç©ºç»“æœï¼Œä½†ä¿ç•™é”™è¯¯ä¿¡æ¯
        return {}, f"è°ƒç”¨å¤±è´¥: {err_msg}", "æœªçŸ¥", f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {err_msg}"

    raw_text = extract_text_from_response(resp_json)
    
    # ã€ä¿®å¤æ ¸å¿ƒé—®é¢˜ã€‘è°ƒç”¨æ–°å¢çš„ JSON æå–å‡½æ•°
    parsed_json, cleaned_json_text = extract_json_from_text(raw_text)

    # è§£æ JSON
    if parsed_json and isinstance(parsed_json, dict):
        explanation = parsed_json.get("explanation", "æ¨¡å‹æœªæä¾›è¯¦ç»†æ¨ç†è¿‡ç¨‹ã€‚")
        predicted_pos = parsed_json.get("predicted_pos", "æœªçŸ¥")
        raw_scores = parsed_json.get("scores", {})
        if predicted_pos not in RULE_SETS:
             st.warning(f"æ¨¡å‹é¢„æµ‹çš„è¯ç±» '{predicted_pos}' ä¸åœ¨åˆ†æèŒƒå›´å†… ('åè¯', 'åŠ¨è¯', 'ååŠ¨è¯')ã€‚")
    else:
        st.error("âŒ æœªèƒ½ä»æ¨¡å‹å“åº”ä¸­è§£æå‡ºæœ‰æ•ˆçš„JSONã€‚è¯·æ£€æŸ¥æ¨¡å‹è¾“å‡ºæ˜¯å¦ç¬¦åˆè¦æ±‚ã€‚")
        explanation = "æ— æ³•è§£ææ¨¡å‹è¾“å‡ºã€‚åŸå§‹å“åº”ï¼š\n" + raw_text
        predicted_pos = "æœªçŸ¥"
        raw_scores = {}
        cleaned_json_text = raw_text  # å±•ç¤ºåŸå§‹æ–‡æœ¬

    # --- å…³é”®ï¼šåˆå§‹åŒ–æ‰€æœ‰è¯ç±»çš„å¾—åˆ†å­—å…¸ï¼Œå¹¶è¿›è¡Œåˆ†æ•°è½¬æ¢ ---
    scores_out = {pos: {} for pos in RULE_SETS.keys()}

    # åªæŠŠâ€œç¬¦åˆ/ä¸ç¬¦åˆâ€è½¬æˆå…·ä½“åˆ†å€¼ï¼ˆæ­£åˆ† / è´Ÿåˆ†ï¼‰
    for pos, rules in RULE_SETS.items():
        raw_pos_scores = raw_scores.get(pos, {})
        if isinstance(raw_pos_scores, dict):
            for k, v in raw_pos_scores.items():
                normalized_key = normalize_key(k, rules)
                if normalized_key:
                    # æ‰¾åˆ°å½“å‰è§„åˆ™çš„å®šä¹‰
                    rule_def = next(r for r in rules if r["name"] == normalized_key)
                    # ä½¿ç”¨ map_to_allowed_scoreï¼štrue/false/â€œæ˜¯/å¦â€ç­‰ â†’ match_score / mismatch_score
                    scores_out[pos][normalized_key] = map_to_allowed_score(rule_def, v)

    # ä¿è¯æ¯æ¡è§„åˆ™éƒ½æœ‰å¾—åˆ†ï¼Œæ²¡æœ‰å°±é»˜è®¤ Mismatch Scoreï¼ˆæ›´ä¸¥æ ¼ï¼‰æˆ– 0 åˆ†ï¼ˆæ›´ä¿å®ˆï¼‰
    # é‡‡ç”¨æ›´ä¿å®ˆçš„ 0 åˆ†ï¼Œå› ä¸ºæ¨¡å‹æ²¡æåŠï¼Œå¯èƒ½æ˜¯ä¸é€‚ç”¨ï¼Œè€Œä¸æ˜¯æ˜ç¡®ä¸ç¬¦åˆ
    for pos, rules in RULE_SETS.items():
        for rule in rules:
            rule_name = rule["name"]
            if rule_name not in scores_out[pos]:
                scores_out[pos][rule_name] = 0

    return scores_out, raw_text, predicted_pos, explanation

# ===============================
# é›·è¾¾å›¾
# ===============================
def plot_radar_chart_streamlit(scores_norm: Dict[str, float], title: str):
    if not scores_norm:
        st.warning("æ— æ³•ç»˜åˆ¶é›·è¾¾å›¾ï¼šæ²¡æœ‰æœ‰æ•ˆæ•°æ®ã€‚")
        return
    
    # è¿‡æ»¤æ‰éš¶å±åº¦å°äºç­‰äº 0 çš„è¯ç±»ï¼Œä»¥ç¾åŒ–é›·è¾¾å›¾ï¼ˆå¯é€‰ï¼Œä½†é€šå¸¸é›·è¾¾å›¾åªæ˜¾ç¤ºæ­£å‘ç»“æœï¼‰
    # è¿™é‡Œæˆ‘ä»¬ä¿ç•™æ‰€æœ‰æ•°æ®ï¼Œå› ä¸ºéš¶å±åº¦å¯èƒ½ä¸ºè´Ÿã€‚ä½†åªæ˜¾ç¤ºåˆ†æçš„è¯ç±»ã€‚
    
    categories = list(scores_norm.keys())
    if not categories:
        st.warning("æ— æ³•ç»˜åˆ¶é›·è¾¾å›¾ï¼šæ²¡æœ‰æœ‰æ•ˆè¯ç±»ã€‚")
        return
        
    values = list(scores_norm.values())
    
    # é—­åˆé›·è¾¾å›¾
    categories += [categories[0]]
    values += [values[0]]
    
    # ç¡®ä¿ radialaxis range åŒ…å«è´Ÿå€¼ï¼Œä»¥æ­£ç¡®æ˜¾ç¤ºè´Ÿéš¶å±åº¦
    min_val = min(values)
    max_val = max(values)
    
    # ç¡®ä¿èŒƒå›´è‡³å°‘ä» 0 å¼€å§‹æˆ–åŒ…å« -1 åˆ° 1
    axis_min = min(min_val, -0.1) 
    axis_max = max(max_val, 1.0)
    
    # è°ƒæ•´é›·è¾¾å›¾çš„é…ç½®ï¼Œä½¿å…¶æ›´é€‚ç”¨äºè´Ÿå€¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
    fig = go.Figure(data=[
        go.Scatterpolar(
            r=values, 
            theta=categories, 
            fill="toself", 
            name="éš¶å±åº¦",
            hovertemplate = '<b>%{theta}</b><br>éš¶å±åº¦: %{r:.4f}<extra></extra>' # ä¼˜åŒ–æ‚¬åœæ˜¾ç¤º
        )
    ])
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True, 
                range=[axis_min, axis_max], # è°ƒæ•´èŒƒå›´ä»¥åŒ…å«è´Ÿåˆ†
                tickvals=[0, 0.25, 0.5, 0.75, 1.0] if axis_min >= 0 else [-1.0, -0.5, 0, 0.5, 1.0] # è°ƒæ•´åˆ»åº¦
            )
        ),
        showlegend=False,
        title=dict(text=title, x=0.5, font=dict(size=16))
    )
    st.plotly_chart(fig, use_container_width=True)

# ===============================
# ã€æœ€ç»ˆå¢å¼ºç‰ˆã€‘æ‰¹é‡å¤„ç†ï¼ˆå®æ—¶å­˜æ¡£ + æ–­ç‚¹ç”Ÿæˆï¼‰
# ===============================
def process_and_style_excel(df, selected_model_info, target_col_name):
    output = io.BytesIO()
    
    # --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ session_state å­˜å‚¨å†å²ï¼Œé˜²æ­¢ç•Œé¢åˆ·æ–°ä¸¢å¤± ---
    if 'processed_history' not in st.session_state:
        st.session_state.processed_history = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    backup_info_placeholder = st.container() # ç”¨äºæ˜¾ç¤ºå®æ—¶è¿›åº¦å’Œå¤‡ä»½æç¤º
    
    total = len(df)
    backup_file = "batch_process_history.csv" # å†å²è®°å½•å­˜ç›˜æ–‡ä»¶

    try:
        for index, row in df.iterrows():
            word = str(row[target_col_name]).strip()
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡ï¼ˆå¯é€‰ï¼šé¿å…é‡å¤æ¶ˆè€—Tokenï¼‰
            # if any(h['è¯è¯­'] == word for h in st.session_state.processed_history): continue

            # é‡è¯•æœºåˆ¶
            max_retries = 3
            success = False
            scores_all, raw_text, predicted_pos, explanation = {}, "", "è¯·æ±‚å¤±è´¥", ""
            
            for attempt in range(max_retries):
                try:
                    status_text.text(f"æ­£åœ¨å¤„ç† ({index + 1}/{total}): {word} ... (å°è¯• {attempt + 1})")
                    scores_all, raw_text, predicted_pos, explanation = ask_model_for_pos_and_scores(
                        word=word,
                        provider=selected_model_info["provider"],
                        model=selected_model_info["model"],
                        api_key=selected_model_info["api_key"]
                    )
                    if scores_all:
                        success = True
                        break
                    time.sleep(2)
                except Exception:
                    time.sleep(2)
            
            # è®¡ç®—æ•°æ®
            membership = calculate_membership(scores_all) if success else {}
            new_row = {
                "åºæ•°": index + 1,
                "è¯è¯­": word,
                "åŠ¨è¯": membership.get("åŠ¨è¯", 0.0),
                "åè¯": membership.get("åè¯", 0.0),
                "ååŠ¨è¯": membership.get("ååŠ¨è¯", 0.0),
                "å·®å€¼/è·ç¦»": round(abs(membership.get("åŠ¨è¯", 0.0) - membership.get("åè¯", 0.0)), 4),
                "é¢„æµ‹è¯ç±»": predicted_pos,
                "åŸå§‹å“åº”": raw_text if success else f"é”™è¯¯: {explanation}",
                "æ—¶é—´æˆ³": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            # --- å®æ—¶å­˜ç•™è®°å½• ---
            # 1. å­˜å…¥ SessionState (å†…å­˜)
            st.session_state.processed_history.append(new_row)
            
            # 2. å†™å…¥æœ¬åœ°ç£ç›˜ CSV (æŒä¹…åŒ–)
            try:
                temp_df = pd.DataFrame([new_row])
                # header åªåœ¨æ–‡ä»¶ä¸å­˜åœ¨æ—¶å†™å…¥
                header_needed = not os.path.exists(backup_file)
                temp_df.to_csv(backup_file, mode='a', header=header_needed, index=False, encoding='utf-8-sig')
            except:
                pass

            # 3. å®æ—¶åœ¨ç•Œé¢å±•ç¤ºå·²å®Œæˆçš„è®°å½•ï¼ˆè®©ç”¨æˆ·æ”¾å¿ƒï¼‰
            with backup_info_placeholder:
                st.info(f"ğŸ’¾ å·²è‡ªåŠ¨ä¿å­˜ç¬¬ {index+1} æ¡è®°å½•ã€‚å¦‚é‡ä¸­æ–­ï¼Œè¯·æ£€æŸ¥ç›®å½•ä¸‹ `{backup_file}`")

            progress_bar.progress((index + 1) / total)
            time.sleep(0.5)

    except Exception as e:
        st.error(f"âš ï¸ æ‰¹é‡å¤„ç†æ„å¤–ä¸­æ–­: {e}")
    
    # ===============================
    # å¯¼å‡ºé€»è¾‘ï¼ˆæ”¯æŒä¸­æ–­åå¯¼å‡ºå·²å®Œæˆéƒ¨åˆ†ï¼‰
    # ===============================
    final_data = st.session_state.processed_history
    if not final_data:
        return None

    result_df = pd.DataFrame(final_data)
    
    try:
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            cols = ["è¯è¯­", "åŠ¨è¯", "åè¯", "ååŠ¨è¯", "å·®å€¼/è·ç¦»", "é¢„æµ‹è¯ç±»", "åŸå§‹å“åº”"]
            result_df[cols].to_excel(writer, index=False, sheet_name='åˆ†æç»“æœ')
            
            workbook = writer.book
            worksheet = writer.sheets['åˆ†æç»“æœ']
            yellow_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
            
            for i, data_row in enumerate(final_data):
                row_num = i + 2
                pred = data_row["é¢„æµ‹è¯ç±»"]
                target_idx = {"åŠ¨è¯": 2, "åè¯": 3, "ååŠ¨è¯": 4}.get(pred)
                if target_idx:
                    worksheet.cell(row=row_num, column=target_idx).fill = yellow_fill
                    
        return output.getvalue()
    except Exception as e:
        st.error(f"Excel ç”Ÿæˆå¤±è´¥: {e}")
        return None
    
# ===============================
# ä¸»é¡µé¢é€»è¾‘
# ===============================
def main():
    st.title("ğŸ“° æ±‰è¯­è¯ç±»éš¶å±åº¦æ£€æµ‹åˆ’ç±»")
    
    # --- é¡¶éƒ¨å›ºå®šæ§åˆ¶åŒº ---
    control_container = st.container()
    with control_container:
        col1, col2 = st.columns([3, 1])
        
        with col1:
            st.subheader("âš™ï¸ æ¨¡å‹è®¾ç½®")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨æ¨¡å‹
            if not AVAILABLE_MODEL_OPTIONS:
                st.error("âŒ æ‰¾ä¸åˆ°å¯ç”¨çš„ API Keyï¼è¯·è®¾ç½®ä»¥ä¸‹ä»»æ„ä¸€ä¸ªç¯å¢ƒå˜é‡æ¥å¯ç”¨æ¨¡å‹:")
                for name, info in MODEL_OPTIONS.items():
                      st.code(f"export {info['env_var']}='ä½ çš„API Key'", language="bash")
                
                # ç¦ç”¨æ‰€æœ‰åŠŸèƒ½
                selected_model_display_name = list(MODEL_OPTIONS.keys())[0] # å ä½ç¬¦
                selected_model_info = MODEL_OPTIONS[selected_model_display_name]
                st.selectbox("é€‰æ‹©å¤§æ¨¡å‹ (ä¸å¯ç”¨)", list(MODEL_OPTIONS.keys()), disabled=True)
            else:
                selected_model_display_name = st.selectbox(
                    "é€‰æ‹©å¤§æ¨¡å‹", 
                    list(AVAILABLE_MODEL_OPTIONS.keys()), 
                    key="model_select"
                )
                selected_model_info = AVAILABLE_MODEL_OPTIONS[selected_model_display_name]
                
        
        with col2:
            st.subheader("ğŸ”— è¿æ¥æµ‹è¯•")
            st.write("") # Spacer
            if not selected_model_info["api_key"]:
                st.button("æµ‹è¯•æ¨¡å‹é“¾æ¥ (ä¸å¯ç”¨)", type="secondary", disabled=True)
            else:
                if st.button("æµ‹è¯•æ¨¡å‹é“¾æ¥", type="secondary"):
                    with st.spinner("æ­£åœ¨æµ‹è¯•è¿æ¥..."):
                        # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„pingè¯·æ±‚æ¥æµ‹è¯•è¿æ¥
                        ok, _, err_msg = call_llm_api_cached(
                            _provider=selected_model_info["provider"],
                            _model=selected_model_info["model"],
                            _api_key=selected_model_info["api_key"],
                            messages=[{"role": "user", "content": "è¯·å›å¤'pong'"}],
                            max_tokens=10
                        )
                    if ok:
                        st.success("âœ… æˆåŠŸï¼")
                    else:
                        st.error(f"âŒ å¤±è´¥: {err_msg}")

    st.markdown("---")

    # ===============================
    # åˆ†é¡µåŠŸèƒ½ï¼šå•ä¸ªåˆ†æ / æ‰¹é‡å¤„ç†
    # ===============================
    tab1, tab2 = st.tabs(["ğŸ” å•ä¸ªè¯è¯­è¯¦ç»†åˆ†æ", "ğŸ“‚ Excel æ‰¹é‡å¤„ç†"])

    # --- Tab 1: åŸæœ‰çš„å•ä¸ªè¯è¯­åˆ†æé€»è¾‘ ---
    with tab1:
        st.subheader("ğŸ”¤ è¯è¯­è¾“å…¥")
        word = st.text_input("è¯·è¾“å…¥è¦åˆ†æçš„æ±‰è¯­è¯è¯­", placeholder="ä¾‹å¦‚ï¼šè‹¹æœã€è·‘ã€ç¾ä¸½...", key="word_input")
        
        # å¼€å§‹åˆ†ææŒ‰é’®ï¼ˆAPI Keyä¸ºç©ºæ—¶ç¦ç”¨ï¼‰
        analyze_button = st.button(
            "ğŸš€ å¼€å§‹åˆ†æ", 
            type="primary",
            disabled=not (selected_model_info["api_key"] and word)
        )

        # --- ä½¿ç”¨è¯´æ˜ ---
        with st.expander("â„¹ï¸ ä½¿ç”¨è¯´æ˜", expanded=False):
            st.info("""
            1. **é…ç½® API Key**: è¯·åœ¨è¿è¡Œç¨‹åºå‰è®¾ç½®å¿…è¦çš„ç¯å¢ƒå˜é‡ã€‚
            2. **è¯è¯­è¾“å…¥**ï¼šåœ¨ä¸Šæ–¹çš„â€œè¯è¯­è¾“å…¥â€æ¡†ä¸­è¾“å…¥ä¸€ä¸ªæ±‰è¯­è¯ã€‚
            3. **å¼€å§‹åˆ†æ**ï¼šç‚¹å‡»â€œå¼€å§‹åˆ†æâ€æŒ‰é’®ã€‚
            4. **ç»“æœè§£æ**ï¼šç³»ç»Ÿå°†æ˜¾ç¤ºéš¶å±åº¦ã€é›·è¾¾å›¾å’Œè¯¦ç»†è§„åˆ™å¾—åˆ†ã€‚
            """)

        # --- ç»“æœæ˜¾ç¤ºåŒº ---
        if analyze_button and word and selected_model_info["api_key"]:
            status_placeholder = st.empty()
            status_placeholder.info(f"æ­£åœ¨ä¸ºè¯è¯­ã€Œ{word}ã€å¯åŠ¨åˆ†æï¼Œä½¿ç”¨æ¨¡å‹ï¼š{selected_model_display_name}...")

            scores_all, raw_text, predicted_pos, explanation = ask_model_for_pos_and_scores(
                word=word,
                provider=selected_model_info["provider"],
                model=selected_model_info["model"],
                api_key=selected_model_info["api_key"]
            )
            
            status_placeholder.empty()
            
            # åªæœ‰åœ¨æˆåŠŸè§£æå‡ºåˆ†æ•°æ—¶æ‰è¿›è¡Œåç»­æ˜¾ç¤º
            if scores_all:
                membership = calculate_membership(scores_all)
                final_membership = membership.get(predicted_pos, 0)
                
                st.success(f'**åˆ†æå®Œæˆ**ï¼šè¯è¯­ã€Œ{word}ã€æœ€å¯èƒ½çš„è¯ç±»æ˜¯ **ã€{predicted_pos}ã€‘**ï¼Œéš¶å±åº¦ä¸º **{final_membership:.4f}**')
                
                col_results_1, col_results_2 = st.columns(2)
                
                with col_results_1:
                    st.subheader("ğŸ† è¯ç±»éš¶å±åº¦æ’å")
                    top10 = get_top_10_positions(membership)
                    top10_df = pd.DataFrame(top10, columns=["è¯ç±»", "éš¶å±åº¦"])
                    top10_df["éš¶å±åº¦"] = top10_df["éš¶å±åº¦"].apply(lambda x: f"{x:.4f}")
                    st.table(top10_df)
                    
                    st.subheader("ğŸ“Š è¯ç±»éš¶å±åº¦é›·è¾¾å›¾")
                    plot_radar_chart_streamlit(dict(top10), f"ã€Œ{word}ã€çš„è¯ç±»éš¶å±åº¦åˆ†å¸ƒ")

                with col_results_2:
                    st.subheader("ğŸ“‹ å„è¯ç±»è¯¦ç»†å¾—åˆ†")
                    
                    # 1. è®¡ç®—æ‰€æœ‰è¯ç±»çš„æ€»åˆ†
                    pos_total_scores = {pos: sum(scores_all[pos].values()) for pos in RULE_SETS.keys()}
                    
                    # æŒ‰æ€»åˆ†é™åºæ’åº
                    sorted_pos_names = sorted(pos_total_scores.keys(), key=lambda pos: pos_total_scores[pos], reverse=True)
                    
                    # 2. ä¾æ¬¡æ˜¾ç¤ºæ‰€æœ‰è¯ç±»ï¼ˆè€Œä¸æ˜¯åªæ˜¾ç¤ºå‰10ï¼Œè®©ç”¨æˆ·å¯ä»¥çœ‹å…¨éƒ¨ï¼‰
                    for pos in sorted_pos_names:
                        total_score = pos_total_scores[pos]
                        
                        # æ‰¾åˆ°è¯¥è¯ç±»ä¸‹å¾—åˆ†æœ€é«˜çš„è§„åˆ™
                        max_rule = max(scores_all[pos].items(), key=lambda x: x[1], default=("æ— ", 0))
                        
                        # åˆ›å»ºexpanderï¼Œæ˜¾ç¤ºè¯ç±»åç§°ã€æ€»åˆ†å’Œæœ€é«˜åˆ†è§„åˆ™
                        with st.expander(f"**{pos}** (æ€»åˆ†: {total_score}, æœ€é«˜åˆ†è§„åˆ™: {max_rule[0]} - {max_rule[1]}åˆ†)"):
                            # æ˜¾ç¤ºè¯¥è¯ç±»ä¸‹çš„æ‰€æœ‰è§„åˆ™å¾—åˆ†ï¼ˆæŒ‰è§„åˆ™å¾—åˆ†é™åºæ’åˆ—ï¼‰
                            rule_data = []
                            for rule in RULE_SETS[pos]:
                                rule_score = scores_all[pos][rule["name"]]
                                rule_data.append({
                                    "è§„åˆ™ä»£ç ": rule["name"],
                                    "è§„åˆ™æè¿°": rule["desc"],
                                    "å¾—åˆ†": rule_score
                                })
                            
                            # æŒ‰å¾—åˆ†é™åºæ’åºè§„åˆ™ï¼Œè®©é«˜åˆ†è§„åˆ™æ’åœ¨å‰é¢
                            rule_data_sorted = sorted(rule_data, key=lambda x: x["å¾—åˆ†"], reverse=True)
                            rule_df = pd.DataFrame(rule_data_sorted)
                            
                            # è´Ÿåˆ†æ ‡çº¢
                            styled_df = rule_df.style.applymap(
                                lambda x: "color: #ff4b4b; font-weight: bold" if isinstance(x, int) and x < 0 else "",
                                subset=["å¾—åˆ†"]
                            )
                            
                            st.dataframe(
                                styled_df,
                                use_container_width=True,
                                # åŠ¨æ€è°ƒæ•´é«˜åº¦ï¼Œé¿å…è¿‡é«˜
                                height=min(len(rule_df) * 30 + 50, 400) 
                            )
                    
                    st.subheader("ğŸ“¥ æ¨¡å‹åŸå§‹å“åº”")
                    with st.expander("ç‚¹å‡»å±•å¼€æŸ¥çœ‹åŸå§‹å“åº”", expanded=False):
                        st.code(raw_text, language="text") # æ”¹ä¸º text ä»¥æ›´å¥½åœ°å±•ç¤ºæ··åˆæ–‡æœ¬
# --- Tab 2: æ‰¹é‡å¤„ç†é€»è¾‘ (å†å²è®°å½•å¸¸é©»æ˜¾ç¤ºç‰ˆ) ---
    with tab2:
        st.header("ğŸ“‚ æ‰¹é‡ Excel å¤„ç†")
        
        # å®šä¹‰æœ¬åœ°æŒä¹…åŒ–æ–‡ä»¶å
        BACKUP_FILE = "batch_history_log.csv"

        # ==========================================
        # 1. å®æ—¶å†å²è®°å½•çœ‹æ¿ (å¸¸é©»æ˜¾ç¤º)
        # ==========================================
        st.subheader("ğŸ“Š å†å²è®°å½•ä¸æ¢å¤é¢æ¿")
        
        if os.path.exists(BACKUP_FILE):
            try:
                # è¯»å–æœ¬åœ°å·²ä¿å­˜çš„æ‰€æœ‰æ•°æ®
                history_df = pd.read_csv(BACKUP_FILE)
                total_saved = len(history_df)
                
                # ä½¿ç”¨ä¸¤æ å±•ç¤ºçŠ¶æ€å’Œä¸‹è½½
                stat_col, action_col = st.columns([2, 1])
                with stat_col:
                    st.info(f"âœ… **æ£€æµ‹åˆ°æœ¬åœ°å­˜ç•™æ•°æ®**ï¼šå…±è®¡ **{total_saved}** æ¡è®°å½•ã€‚")
                    # å±•ç¤ºæœ€å3æ¡å®æ—¶ç»“æœ
                    st.dataframe(history_df.tail(3), use_container_width=True, height=150)
                
                with action_col:
                    st.write("æ“ä½œå·²å­˜æ•°æ®ï¼š")
                    # å³ä½¿ç¨‹åºä¸­æ–­ï¼Œè¿™ä¸ªæŒ‰é’®ä¹Ÿä¸€ç›´å­˜åœ¨ï¼Œå¯ä»¥ç›´æ¥ä¸‹è½½å·²å®Œæˆçš„éƒ¨åˆ†
                    st.download_button(
                        "ğŸ“¥ å¯¼å‡ºå·²å®Œæˆè®°å½• (CSV)",
                        data=history_df.to_csv(index=False, encoding='utf-8-sig'),
                        file_name=f"åˆ†æè®°å½•å¤‡ä»½_{time.strftime('%m%d_%H%M')}.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
                    if st.button("ğŸ—‘ï¸ æ¸…ç©ºæœ¬åœ°ç¼“å­˜", use_container_width=True, help="å¤„ç†æ–°æ–‡ä»¶å‰è¯·å…ˆæ¸…ç©ºæ—§æ•°æ®"):
                        os.remove(BACKUP_FILE)
                        if 'processed_history' in st.session_state:
                            st.session_state.processed_history = []
                        st.rerun()
            except Exception as e:
                st.warning(f"è¯»å–å¤‡ä»½æ—¶é‡åˆ°ç‚¹å°é—®é¢˜: {e}")
        else:
            st.write("ğŸ’¡ å½“å‰æš‚æ— å†å²å­˜ç•™è®°å½•ã€‚å¼€å§‹å¤„ç†åï¼Œæ•°æ®å°†å®æ—¶æ˜¾ç¤ºåœ¨è¿™é‡Œã€‚")

        st.divider()

        # ==========================================
        # 2. ä¸Šä¼ ä¸å¤„ç†åŒº
        # ==========================================
        st.subheader("ğŸš€ æ–°ä»»åŠ¡ä¸Šä¼ ")
        uploaded_file = st.file_uploader("ä¸Šä¼ å¾…åˆ†æçš„ Excel æ–‡ä»¶", type=["xlsx", "xls"])
        
        if uploaded_file:
            try:
                df = pd.read_excel(uploaded_file)
                # è‡ªåŠ¨è¯†åˆ«åˆ—å
                target_col = next((col for col in df.columns if "è¯" in str(col) or "word" in str(col).lower()), None)
                
                if not target_col:
                    st.error("âŒ æ— æ³•è¯†åˆ«ç›®æ ‡åˆ—ã€‚è¯·ç¡®ä¿ Excel ä¸­æœ‰ä¸€åˆ—æ ‡é¢˜åŒ…å« 'è¯' æˆ– 'Word'ã€‚")
                else:
                    st.success(f"å‡†å¤‡å°±ç»ªï¼ç›®æ ‡åˆ—ï¼š`{target_col}` | æ€»æ•°ï¼š{len(df)}")
                    
                    if st.button("å¼€å§‹æ‰¹é‡æ‰§è¡Œ (è‡ªåŠ¨å­˜ç›˜)", type="primary"):
                        if not selected_model_info["api_key"]:
                            st.error("API Key æœªé…ç½®ï¼Œè¯·åœ¨ä¸Šæ–¹è®¾ç½®ã€‚")
                        else:
                            # è°ƒç”¨å¤„ç†å‡½æ•°
                            excel_data = process_and_style_excel(df, selected_model_info, target_col)
                            
                            if excel_data:
                                st.balloons()
                                st.success("ğŸ‰ å…¨éƒ¨ä»»åŠ¡å¤„ç†å®Œæˆï¼")
                                st.download_button(
                                    label="ğŸ’¾ ä¸‹è½½æœ€ç»ˆæ ‡é»„ç‰ˆ Excel",
                                    data=excel_data,
                                    file_name="è¯ç±»åˆ†æç»“æœ_æœ€ç»ˆç‰ˆ.xlsx",
                                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                    use_container_width=True
                                )
            
            except Exception as e:
                st.error(f"æ–‡ä»¶å¤„ç†å‡ºé”™: {e}")


# ===============================
# è¿è¡Œä¸»å‡½æ•°
# ===============================
if __name__ == "__main__":
    main()
# ===============================
# é¡µé¢åº•éƒ¨è¯´æ˜
# ===============================
st.markdown("---")
st.markdown(
    "<div style='text-align:center; color:#666;'>"
    "Â© 2025 æ±‰è¯­è¯ç±»éš¶å±åº¦æ£€æµ‹åˆ’ç±»  "
    "</div>",
    unsafe_allow_html=True
)
