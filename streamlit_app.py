import streamlit as st
import requests
import json
import re
import os
import pandas as pd
import plotly.graph_objects as go
import io
import time
import traceback
from typing import Tuple, Dict, Any, List
from openpyxl import load_workbook
from openpyxl.styles import PatternFill

# ==========================================
# 1. é¡µé¢é…ç½®ä¸æ ·å¼
# ==========================================
st.set_page_config(
    page_title="æ±‰è¯­è¯ç±»éš¶å±åº¦æ£€æµ‹ ",
    page_icon="ğŸ›¡ï¸",
    layout="wide",
    initial_sidebar_state="collapsed"
)

st.markdown("""
<style>
header {visibility: hidden;}
footer {visibility: hidden;}
.dataframe {font-size: 12px;}
[data-testid="stSidebar"] { display: none !important; }
.stApp > div:first-child { padding-top: 2rem; }
.stCode { max-height: 400px; overflow-y: auto; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. æ¨¡å‹é…ç½® (OpenAI å…¼å®¹åè®®)
# ==========================================
MODEL_CONFIGS = {
    "deepseek": {
        "base_url": "https://api.deepseek.com/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": 4096, "temperature": 0.0, "stream": True
        },
    },
    "openai": {
        "base_url": "https://api.openai.com/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": 4096, "temperature": 0.0, "stream": True
        },
    },
    "moonshot": {
        "base_url": "https://api.moonshot.cn/v1",
        "endpoint": "/chat/completions",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        "payload": lambda model, messages, **kw: {
            "model": model, "messages": messages, "max_tokens": 4096, "temperature": 0.0, "stream": True
        },
    },
    "qwen": {
        "base_url": "https://dashscope.aliyuncs.com/api/v1",
        "endpoint": "/services/aigc/text-generation/generation",
        "headers": lambda key: {
            "Authorization": f"Bearer {key}", "Content-Type": "application/json",
            "X-DashScope-SSE": "enable", "Accept": "text/event-stream"
        },
        "payload": lambda model, messages, **kw: {
            "model": model, "input": {"messages": messages}, 
            "parameters": {"max_tokens": 4096, "temperature": 0.0, "result_format": "message", "incremental_output": True},
        },
    },
}

MODEL_OPTIONS = {
    "DeepSeek Chat": {"provider": "deepseek", "model": "deepseek-chat", "api_key": os.getenv("DEEPSEEK_API_KEY"), "env_var": "DEEPSEEK_API_KEY"},
    "OpenAI GPT-4o-mini": {"provider": "openai", "model": "gpt-4o-mini", "api_key": os.getenv("OPENAI_API_KEY"), "env_var": "OPENAI_API_KEY"},
    "Moonshot (Kimi)": {"provider": "moonshot", "model": "moonshot-v1-32k", "api_key": os.getenv("MOONSHOT_API_KEY"), "env_var": "MOONSHOT_API_KEY"},
    "Qwen (é€šä¹‰åƒé—®)": {"provider": "qwen", "model": "qwen-max", "api_key": os.getenv("QWEN_API_KEY"), "env_var": "QWEN_API_KEY"},
}

AVAILABLE_MODEL_OPTIONS = {name: info for name, info in MODEL_OPTIONS.items() if info["api_key"]}
if not AVAILABLE_MODEL_OPTIONS: AVAILABLE_MODEL_OPTIONS = MODEL_OPTIONS

# ==========================================
# 3. è¯­è¨€å­¦è§„åˆ™å®šä¹‰
# ==========================================
RULE_SETS = {
    "åè¯": [
        {"name": "N1_å¯å—æ•°é‡è¯ä¿®é¥°", "desc": "å¯ä»¥å—æ•°é‡è¯ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "N2_ä¸èƒ½å—å‰¯è¯ä¿®é¥°", "desc": "ä¸èƒ½å—å‰¯è¯ä¿®é¥°", "match_score": 20, "mismatch_score": -20},
        {"name": "N3_å¯ä½œä¸»å®¾è¯­", "desc": "å¯ä»¥åšå…¸å‹çš„ä¸»è¯­æˆ–å®¾è¯­", "match_score": 20, "mismatch_score": 0},
        {"name": "N4_å¯ä½œä¸­å¿ƒè¯­æˆ–ä½œå®šè¯­", "desc": "å¯ä»¥åšä¸­å¿ƒè¯­å—å…¶ä»–åè¯ä¿®é¥°ï¼Œæˆ–è€…ä½œå®šè¯­ç›´æ¥ä¿®é¥°å…¶ä»–åè¯", "match_score": 10, "mismatch_score": 0},
        {"name": "N5_å¯åé™„çš„å­—ç»“æ„", "desc": "å¯ä»¥åé™„åŠ©è¯â€œçš„â€æ„æˆâ€œçš„â€å­—ç»“æ„", "match_score": 10, "mismatch_score": 0},
        {"name": "N6_å¯åé™„æ–¹ä½è¯æ„å¤„æ‰€", "desc": "å¯ä»¥åé™„æ–¹ä½è¯æ„æˆå¤„æ‰€ç»“æ„", "match_score": 10, "mismatch_score": 0},
        {"name": "N7_ä¸èƒ½ä½œè°“è¯­æ ¸å¿ƒ", "desc": "ä¸èƒ½åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒ", "match_score": 10, "mismatch_score": -10},
        {"name": "N8_ä¸èƒ½ä½œè¡¥è¯­/ä¸€èˆ¬ä¸ä½œçŠ¶è¯­", "desc": "ä¸èƒ½ä½œè¡¥è¯­ï¼Œå¹¶ä¸”ä¸€èˆ¬ä¸èƒ½åšçŠ¶è¯­", "match_score": 10, "mismatch_score": 0},
    ],
    "åŠ¨è¯": [
        {"name": "V1_å¯å—å¦å®š'ä¸/æ²¡æœ‰'ä¿®é¥°", "desc": "å¯ä»¥å—å¦å®šå‰¯è¯'ä¸'æˆ–'æ²¡æœ‰'ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "V2_å¯åé™„/æ’å…¥æ—¶ä½“åŠ©è¯", "desc": "å¯ä»¥åé™„æˆ–ä¸­é—´æ’å…¥æ—¶ä½“åŠ©è¯'ç€/äº†/è¿‡'", "match_score": 10, "mismatch_score": 0},
        {"name": "V3_å¯å¸¦çœŸå®¾è¯­", "desc": "å¯ä»¥å¸¦çœŸå®¾è¯­ï¼Œæˆ–é€šè¿‡ä»‹è¯å¼•å¯¼è®ºå…ƒ", "match_score": 20, "mismatch_score": 0},
        {"name": "V4_ç¨‹åº¦å‰¯è¯ä¸å¸¦å®¾è¯­çš„å…³ç³»", "desc": "ä¸èƒ½å—'å¾ˆ'ä¿®é¥°ï¼Œæˆ–èƒ½åŒæ—¶å—'å¾ˆ'ä¿®é¥°å¹¶å¸¦å®¾è¯­", "match_score": 10, "mismatch_score": -10},
        {"name": "V5_å¯æœ‰é‡å /æ­£åé‡å å½¢å¼", "desc": "å¯ä»¥æœ‰'VV, Vä¸€V'ç­‰å½¢å¼", "match_score": 10, "mismatch_score": 0},
        {"name": "V6_å¯åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒ", "desc": "å¯ä»¥åšè°“è¯­æˆ–è°“è¯­æ ¸å¿ƒ", "match_score": 10, "mismatch_score": -10},
        {"name": "V7_ä¸èƒ½ä½œçŠ¶è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "desc": "ä¸èƒ½ä½œçŠ¶è¯­ä¿®é¥°åŠ¨è¯æ€§æˆåˆ†", "match_score": 10, "mismatch_score": 0},
        {"name": "V8_å¯ä½œ'æ€ä¹ˆ/æ€æ ·'æé—®", "desc": "å¯ä»¥è·Ÿåœ¨'æ€ä¹ˆ/æ€æ ·'ä¹‹åæé—®", "match_score": 10, "mismatch_score": 0},
        {"name": "V9_ä¸èƒ½è·Ÿåœ¨'å¤š/å¤šä¹ˆ'ä¹‹å", "desc": "ä¸èƒ½è·Ÿåœ¨'å¤š'ä¹‹åå¯¹æ€§è´¨æé—®", "match_score": 10, "mismatch_score": -10},
    ],
    "ååŠ¨è¯": [
        {"name": "NV1_å¯è¢«å¦å®šä¸”è‚¯å®šå½¢å¼-1", "desc": "å¯ä»¥ç”¨'ä¸/æ²¡æœ‰'å¦å®š", "match_score": 10, "mismatch_score": -10},
        {"name": "NV2_å¯é™„æ—¶ä½“åŠ©è¯", "desc": "å¯ä»¥åé™„æ—¶ä½“åŠ©è¯'ç€/äº†/è¿‡'", "match_score": 10, "mismatch_score": -10},
        {"name": "NV3_å¯å¸¦çœŸå®¾è¯­ä¸”ä¸å—å¾ˆä¿®é¥°", "desc": "å¯ä»¥å¸¦çœŸå®¾è¯­ï¼Œå¹¶ä¸”ä¸èƒ½å—'å¾ˆ'ä¿®é¥°", "match_score": 10, "mismatch_score": -10},
        {"name": "NV4_æœ‰é‡å å’Œæ­£åé‡å å½¢å¼", "desc": "æœ‰é‡å å½¢å¼", "match_score": 10, "mismatch_score": 0},
        {"name": "NV5_å¯ä½œå¤šç§å¥æ³•æˆåˆ†", "desc": "æ—¢å¯ä»¥ä½œè°“è¯­ï¼Œåˆå¯ä»¥ä½œä¸»è¯­æˆ–å®¾è¯­", "match_score": 10, "mismatch_score": -10},
        {"name": "NV6_ä¸èƒ½ç›´æ¥ä½œçŠ¶è¯­", "desc": "ä¸èƒ½ç›´æ¥ä½œçŠ¶è¯­", "match_score": 10, "mismatch_score": -10},
        {"name": "NV7_å¯ä¿®é¥°åè¯æˆ–å—åè¯ä¿®é¥°", "desc": "å¯ä»¥ä¿®é¥°åè¯æˆ–è€…å—åè¯ä¿®é¥°", "match_score": 10, "mismatch_score": 0},
        {"name": "NV8_å¯è·Ÿåœ¨æ€ä¹ˆ/æ€æ ·ä¹‹å", "desc": "å¯ä»¥è·Ÿåœ¨'æ€ä¹ˆ/æ€æ ·'ä¹‹åæé—®", "match_score": 10, "mismatch_score": 0},
        {"name": "NV9_ä¸èƒ½è·Ÿåœ¨å¤š/å¤šä¹ˆä¹‹å", "desc": "ä¸èƒ½è·Ÿåœ¨'å¤š/å¤šä¹ˆ'ä¹‹å", "match_score": 10, "mismatch_score": -10},
        {"name": "NV10_å¯åé™„æ–¹ä½è¯", "desc": "å¯ä»¥åé™„æ–¹ä½è¯æ„æˆå¤„æ‰€ç»“æ„", "match_score": 10, "mismatch_score": 0},
    ]
}

# ==========================================
# 4. å…³é”®å·¥å…·å‡½æ•° (å¢å¼ºæå–ä¸è®¡ç®—)
# ==========================================
def extract_text_from_response(resp_json: Dict[str, Any]) -> str:
    """æå–APIå“åº”ä¸­çš„æ–‡æœ¬å†…å®¹ï¼Œå…¼å®¹å¤šç§APIæ ¼å¼"""
    if not isinstance(resp_json, dict): return ""
    try:
        # Qwen Native
        if "output" in resp_json and "text" in resp_json["output"]: return resp_json["output"]["text"]
        # OpenAI Compatible
        if "choices" in resp_json and len(resp_json["choices"]) > 0:
            choice = resp_json["choices"][0]
            if "message" in choice and "content" in choice["message"]: return choice["message"]["content"]
        return json.dumps(resp_json, ensure_ascii=False)
    except Exception:
        return json.dumps(resp_json, ensure_ascii=False)

def extract_json_from_text(text: str) -> Tuple[Dict[str, Any], str]:
    """
    å¼ºåŠ›æå–å™¨ï¼š
    1. ä¼˜å…ˆæå– Markdown ```json ä»£ç å—
    2. å…¶æ¬¡æå–æœ€å¤–å±‚ {}
    3. å¤±è´¥åˆ™è¿”å› None
    """
    if not text: return None, ""
    
    json_str = ""
    # ç­–ç•¥ 1: æ‰¾ä»£ç å—
    code_match = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
    if code_match:
        json_str = code_match.group(1).strip()
    
    # ç­–ç•¥ 2: æ‰¾å¤§æ‹¬å·
    if not json_str:
        match = re.search(r"(\{.*\})", text.strip(), re.DOTALL)
        if match: json_str = match.group(1).strip()

    if not json_str: return None, text

    try:
        return json.loads(json_str), json_str
    except json.JSONDecodeError:
        return None, text

def normalize_key(k: str, pos_rules: list) -> str:
    if not isinstance(k, str): return None
    k_norm = re.sub(r'[\s_]+', '', k).upper()
    for r in pos_rules:
        r_norm = re.sub(r'[\s_]+', '', r["name"]).upper()
        # æ¨¡ç³ŠåŒ¹é…ï¼šåªè¦åŒ…å«è§„åˆ™ä»£ç ï¼ˆå¦‚N1ï¼‰å³å¯
        if r_norm in k_norm or k_norm in r_norm: return r["name"]
    return None

def map_to_allowed_score(rule: dict, raw_val) -> int:
    match, mismatch = rule["match_score"], rule["mismatch_score"]
    if isinstance(raw_val, bool): return match if raw_val else mismatch
    if isinstance(raw_val, str):
        s = raw_val.strip().lower()
        if s in ("yes", "y", "true", "æ˜¯", "âˆš", "ç¬¦åˆ"): return match
        return mismatch
    return mismatch

def calculate_membership(scores_all: Dict[str, Dict[str, int]]) -> Dict[str, float]:
    membership = {}
    for pos, scores in scores_all.items():
        total = sum(scores.values())
        membership[pos] = max(-1.0, min(1.0, total / 100))
    return membership

def get_top_10_positions(membership: Dict[str, float]) -> List[Tuple[str, float]]:
    return sorted(membership.items(), key=lambda x: x[1], reverse=True)[:10]

# ==========================================
# 5. API è°ƒç”¨ (æµå¼+é‡è¯•æœºåˆ¶)
# ==========================================
def call_llm_api_cached(_provider, _model, _api_key, messages, max_tokens=4096, temperature=0.0):
    if not _api_key: return False, {"error": "API Keyç¼ºå¤±"}, "è¯·å…ˆé…ç½® Key"
    cfg = MODEL_CONFIGS[_provider]
    url = f"{cfg['base_url'].rstrip('/')}{cfg['endpoint']}"
    headers = cfg["headers"](_api_key)
    payload = cfg["payload"](_model, messages, max_tokens=max_tokens, temperature=temperature)
    
    full_content = ""
    try:
        # è®¾ç½® stream=True å’Œ 60ç§’è¿æ¥è¶…æ—¶ï¼Œé˜²æ­¢200æ¡æ—¶ç½‘ç»œæ³¢åŠ¨
        with requests.post(url, headers=headers, json=payload, stream=True, timeout=60) as response:
            if response.status_code != 200:
                return False, {"error": f"API Error {response.status_code}"}, response.text
            
            for line in response.iter_lines():
                if not line: continue
                line_text = line.decode('utf-8').strip()
                if line_text.startswith("data:"): json_str = line_text[5:].strip()
                else: json_str = line_text
                
                if json_str == "[DONE]": break
                try:
                    chunk = json.loads(json_str)
                    delta_text = ""
                    # å…¼å®¹ä¸åŒå‚å•†çš„æµå¼æ ¼å¼
                    if "choices" in chunk and len(chunk["choices"]) > 0:
                        delta_text = chunk["choices"][0].get("delta", {}).get("content", "")
                    elif "output" in chunk: # Qwen Native
                        if "choices" in chunk["output"]:
                            delta_text = chunk["output"]["choices"][0].get("message", {}).get("content", "")
                        elif "text" in chunk["output"]:
                            delta_text = chunk["output"]["text"]
                    if delta_text: full_content += delta_text
                except: continue
        
        if not full_content: return False, {"error": "ç©ºå“åº”"}, "æ¨¡å‹æœªè¿”å›å†…å®¹"
        # æ„é€ ä¼ªå“åº”ä»¥ä¾¿é€šç”¨å¤„ç†
        return True, {"choices": [{"message": {"content": full_content}}], "output": {"text": full_content}}, ""
    except Exception as e:
        return False, {"error": str(e)}, str(e)

# ==========================================
# 6. å•ä¸ªè¯åˆ†æé€»è¾‘ (å¼ºPromptçº¦æŸ)
# ==========================================
def ask_model_for_pos_and_scores(word: str, provider: str, model: str, api_key: str):
    full_rules = {p: "\n".join([f"- {r['name']}: {r['desc']}" for r in rs]) for p, rs in RULE_SETS.items()}
    
    # å¼ºåˆ¶å…ˆè¾“å‡ºè¯¦ç»†æ–‡æœ¬ï¼Œå†è¾“å‡ºJSON
    system = f"""ä½ æ˜¯ä¸€åæ±‰è¯­è¯­è¨€å­¦ä¸“å®¶ã€‚è¯·å¯¹è¯è¯­ã€Œ{word}ã€è¿›è¡Œä¸¥æ ¼çš„å¥æ³•åˆ†æã€‚

ã€è¾“å‡ºè¦æ±‚ã€‘
1. ç¬¬ä¸€æ­¥ï¼šå¿…é¡»è¾“å‡º Markdown æ ¼å¼çš„è¯¦ç»†æ¨ç†è¿‡ç¨‹ã€‚
   æ ¼å¼ï¼š
   ### è¯¦ç»†æ¨ç†è¿‡ç¨‹
   #### åè¯
   - N1_...: ç¬¦åˆ/ä¸ç¬¦åˆã€‚ç†ç”±... ä¾‹å¥...
   (è¯·é€æ¡åˆ†ææ‰€æœ‰è§„åˆ™ï¼Œä¸èƒ½é—æ¼)
   ...ï¼ˆåŠ¨è¯ã€ååŠ¨è¯åŒç†ï¼‰

2. ç¬¬äºŒæ­¥ï¼šåˆ†æç»“æŸåï¼Œè¾“å‡ºä¸€ä¸ª JSON ä»£ç å—ï¼š
```json
{{
  "explanation": "è¿™é‡Œå¡«å…¥ä¸Šé¢çš„è¯¦ç»†æ¨ç†å…¨æ–‡ï¼Œä¿ç•™Markdownæ ¼å¼",
  "predicted_pos": "åè¯/åŠ¨è¯/ååŠ¨è¯",
  "scores": {{
    "åè¯": {{ "N1_...": true, ... }},
    "åŠ¨è¯": {{ "V1_...": false, ... }},
    "ååŠ¨è¯": {{ "NV1_...": true, ... }}
  }}
}}
""" ok, resp, err = call_llm_api_cached(provider, model, api_key, [ {"role": "system", "content": system}, {"role": "user", "content": f"åˆ†æã€Œ{word}ã€"} ])

if not ok: return {}, "", "å¤±è´¥", err

raw = extract_text_from_response(resp)
data, _ = extract_json_from_text(raw)

# å…œåº•ï¼šç¡®ä¿ explanation ä¸ä¸ºç©º
if data:
    json_expl = data.get("explanation", "")
    # å¦‚æœJSONé‡Œçš„è§£é‡Šå¤ªçŸ­ï¼Œè¯´æ˜æ¨¡å‹å¯èƒ½å·æ‡’äº†ï¼Œå¼ºåˆ¶ç”¨å…¨æ–‡ä½œä¸ºè§£é‡Š
    expl = json_expl if len(json_expl) > 50 else raw
    pred = data.get("predicted_pos", "æœªçŸ¥")
    raw_scores = data.get("scores", {})
else:
    # JSONè§£æå¤±è´¥ï¼Œä¿ç•™å…¨æ–‡ï¼Œä¸ç®—ä½œå®Œå…¨å¤±è´¥
    expl = raw 
    pred = "æœªçŸ¥"
    raw_scores = {}
    
# åˆ†æ•°è½¬æ¢
scores_out = {p: {} for p in RULE_SETS}
for pos, rules in RULE_SETS.items():
    s_map = raw_scores.get(pos, {})
    for r in rules:
        val = False
        for k, v in s_map.items():
            if normalize_key(k, [r]) == r["name"]:
                val = v
                break
        scores_out[pos][r["name"]] = map_to_allowed_score(r, val)
        
return scores_out, raw, pred, expl


def process_batch(df, model_info, col_name): """ æ ¸å¿ƒæœºåˆ¶ï¼š 1. å®æ—¶è¿½åŠ å†™å…¥ 'history_database.csv'ã€‚ 2. Try-Except åŒ…è£¹æ•´ä¸ªå•æ¬¡å¾ªç¯ï¼ŒæŠ¥é”™ä¹Ÿç»§ç»­ã€‚ 3. å¯åŠ¨æ—¶è¯»å– CSVï¼Œè·³è¿‡å·²å­˜åœ¨çš„è¯ã€‚ """ db_file = "history_database.csv" output = io.BytesIO()

# A. è¯»å–å†å²ï¼Œæ„å»ºè·³è¿‡åˆ—è¡¨
existing_data = {}
if os.path.exists(db_file):
    try:
        # è¯»æˆå­—ç¬¦ä¸²é˜²æ­¢ç±»å‹é”™è¯¯
        hist_df = pd.read_csv(db_file, dtype=str)
        for _, row in hist_df.iterrows():
            if "è¯è¯­" in row and pd.notna(row["è¯è¯­"]):
                existing_data[str(row["è¯è¯­"]).strip()] = row.to_dict()
        st.info(f"ğŸ“š å·²åŠ è½½å†å²åº“ï¼š{len(existing_data)} æ¡ã€‚å°†è‡ªåŠ¨è·³è¿‡è¿™äº›è¯ï¼Œç›´æ¥ä»æ–­ç‚¹å¤„ç»§ç»­ï¼")
    except:
        st.warning("å†å²åº“è¯»å–å¼‚å¸¸ï¼Œæœ¬æ¬¡å°†å…¨éƒ¨é‡è¯•ã€‚")

total = len(df)
bar = st.progress(0)
status = st.empty()

# å†…å­˜ä¸­çš„ç»“æœï¼Œç”¨äºæœ€åç”Ÿæˆ Excel
final_rows = [] 

# B. å¼€å§‹ä¸å¯é˜»æŒ¡çš„å¾ªç¯
for i, row_data in df.iterrows():
    try:
        word = str(row_data[col_name]).strip()
        
        # --- 1. æ£€æŸ¥ç¼“å­˜ (ç§’ä¼ ) ---
        if word in existing_data:
            status.text(f"â™»ï¸ [è·³è¿‡] {word} (å·²åœ¨åº“ä¸­)")
            
            # ä»å†å²æ¢å¤æ•°æ®ç»“æ„
            cached = existing_data[word]
            # ç®€å•ç±»å‹è½¬æ¢å› float/str
            try:
                v = float(cached.get("åŠ¨è¯", 0))
                n = float(cached.get("åè¯", 0))
                nv = float(cached.get("ååŠ¨è¯", 0))
                d = float(cached.get("å·®å€¼/è·ç¦»", 0))
            except:
                v, n, nv, d = 0,0,0,0
            
            new_row = {
                "è¯è¯­": word,
                "åŠ¨è¯": v, "åè¯": n, "ååŠ¨è¯": nv,
                "å·®å€¼/è·ç¦»": d,
                "åŸå§‹å“åº”": cached.get("åŸå§‹å“åº”", ""),
                "_predicted_pos": cached.get("_predicted_pos", "æœªçŸ¥")
            }
            final_rows.append(new_row)
            
            time.sleep(0.01)
            bar.progress((i + 1) / total)
            continue

        # --- 2. çœŸå®åˆ†æ (å¸¦é‡è¯•) ---
        status.text(f"ğŸš€ [æ­£åœ¨åˆ†æ] ({i+1}/{total}): {word}")
        
        # æ— è®ºå¦‚ä½•é‡è¯• 3 æ¬¡
        retries = 3
        success = False
        scores, raw, pred, expl = {}, "", "è¯·æ±‚å¤±è´¥", "å¤šæ¬¡é‡è¯•æ— æœ"
        
        for attempt in range(retries):
            try:
                scores, raw, pred, expl = ask_model_for_pos_and_scores(
                    word, model_info["provider"], model_info["model"], model_info["api_key"]
                )
                # åªè¦ raw ä¸ä¸ºç©ºï¼Œå°±ç®—æ‹¿åˆ°ä¸œè¥¿äº†
                if raw:
                    success = True
                    break
                time.sleep(2) # å¤±è´¥ä¼‘çœ 
            except Exception as e:
                print(f"Retry Error: {e}")
                time.sleep(2)
        
        # --- 3. ç»“æœè®¡ç®— ---
        if success and scores:
            mem = calculate_membership(scores)
            v = mem.get("åŠ¨è¯", 0.0)
            n = mem.get("åè¯", 0.0)
            nv = mem.get("ååŠ¨è¯", 0.0)
        else:
            v, n, nv = 0.0, 0.0, 0.0
        
        diff = round(abs(v - n), 4)
        
        # å…œåº•ï¼šå¦‚æœ explanation ä¾ç„¶ä¸ºç©ºï¼Œå¼ºè¡Œå¡«å…¥ raw
        final_expl = expl if (expl and len(expl) > 5) else raw
        if not final_expl: final_expl = "API Error: No Response"

        new_row = {
            "è¯è¯­": word,
            "åŠ¨è¯": v, "åè¯": n, "ååŠ¨è¯": nv,
            "å·®å€¼/è·ç¦»": diff,
            "åŸå§‹å“åº”": final_expl, # è¿™é‡Œç»å¯¹åŒ…å«äº†å®Œæ•´æ¨ç†
            "_predicted_pos": pred
        }
        final_rows.append(new_row)
        
        # --- 4. å®æ—¶è½ç›˜ (è¿½åŠ æ¨¡å¼) ---
        # è¿™ä¸€æ­¥ä¿è¯äº†å³ä½¿ä¸‹ä¸€ç§’æ–­ç”µï¼Œå½“å‰è¿™ä¸ªè¯ä¹Ÿä¿å­˜äº†
        try:
            temp_df = pd.DataFrame([new_row])
            write_hdr = not os.path.exists(db_file)
            temp_df.to_csv(db_file, mode='a', header=write_hdr, index=False, encoding='utf-8-sig')
        except Exception as e:
            print(f"Disk Write Error: {e}")

        # --- 5. å¼ºåˆ¶åœé¡¿ (é˜²å°) ---
        time.sleep(1) 
        bar.progress((i + 1) / total)

    except Exception as e:
        # æ•æ‰ä¸€åˆ‡æœªçŸ¥å¼‚å¸¸ï¼Œç¡®ä¿å¾ªç¯ç»§ç»­ï¼
        print(f"CRITICAL ERROR on {word}: {e}")
        # å³ä½¿æŠ¥é”™ï¼Œä¹Ÿå°è¯•å¾€åˆ—è¡¨é‡ŒåŠ ä¸ªç©ºè¡Œï¼Œä¿æŒç´¢å¼•å¯¹é½(å¯é€‰)
        time.sleep(1)
        continue

# C. å¯¼å‡ºæœ€ç»ˆ Excel
status.success("âœ… å…¨éƒ¨å®Œæˆï¼")

if not final_rows: return None

res_df = pd.DataFrame(final_rows)
try:
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        cols = ["è¯è¯­", "åŠ¨è¯", "åè¯", "ååŠ¨è¯", "å·®å€¼/è·ç¦»", "åŸå§‹å“åº”"]
        valid_cols = [c for c in cols if c in res_df.columns]
        res_df[valid_cols].to_excel(writer, index=False, sheet_name='ç»“æœ')
        
        # æ ‡é»„
        ws = writer.sheets['ç»“æœ']
        fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
        for idx, r in enumerate(final_rows):
            p = str(r.get("_predicted_pos", ""))
            t = None
            if "åŠ¨è¯" in p: t = 2
            elif "åè¯" in p: t = 3
            elif "ååŠ¨è¯" in p: t = 4
            if t: ws.cell(row=idx+2, column=t).fill = fill
except:
    pass
    
return output.getvalue()


def main(): st.title("ğŸ“° æ±‰è¯­è¯ç±»éš¶å±åº¦æ£€æµ‹ (æ‰¹é‡æ——èˆ°ç‰ˆ)")

with st.container():
    c1, c2 = st.columns([3, 1])
    with c1:
        if not AVAILABLE_MODEL_OPTIONS:
            st.error("âŒ æ—  API Key")
            info = {"api_key": None}
        else:
            name = st.selectbox("é€‰æ‹©æ¨¡å‹", list(AVAILABLE_MODEL_OPTIONS.keys()))
            info = AVAILABLE_MODEL_OPTIONS[name]
    with c2:
        st.write("")
        if st.button("æµ‹è¯•è¿æ¥"):
            ok, _, msg = call_llm_api_cached(info["provider"], info["model"], info["api_key"], [{"role":"user","content":"hi"}], 5)
            if ok: st.success("æˆåŠŸ")
            else: st.error(msg)

st.markdown("---")

t1, t2 = st.tabs(["ğŸ” å•ä¸ªåˆ†æ", "ğŸ“‚ æ‰¹é‡å…¨è‡ªåŠ¨å¤„ç†"])

# Tab 1: å•ä¸ª
with t1:
    w = st.text_input("è¾“å…¥è¯è¯­", key="s_input")
    if st.button("åˆ†æ", disabled=not (w and info["api_key"])):
        with st.spinner("åˆ†æä¸­..."):
            scores, raw, pred, expl = ask_model_for_pos_and_scores(w, info["provider"], info["model"], info["api_key"])
            if scores:
                mem = calculate_membership(scores)
                st.success(f"ç»“æœ: {pred}")
                c_a, c_b = st.columns(2)
                with c_a:
                    st.table(pd.DataFrame(get_top_10_positions(mem), columns=["è¯ç±»","éš¶å±åº¦"]))
                    fig = go.Figure(go.Scatterpolar(r=list(mem.values())+[list(mem.values())[0]], theta=list(mem.keys())+[list(mem.keys())[0]], fill='toself'))
                    st.plotly_chart(fig, use_container_width=True)
                with c_b:
                    st.info("æ¨ç†ç®€è¿°")
                    st.markdown(expl[:500]+"..." if len(expl)>500 else expl)
                    with st.expander("å®Œæ•´åŸå§‹å“åº”"): st.code(raw)

# Tab 2: æ‰¹é‡
with t2:
    st.info("ğŸ’¡ æ ¸å¿ƒç‰¹æ€§ï¼šè‡ªåŠ¨æ–­ç‚¹ç»­ä¼  + å®æ—¶å­˜ç›˜ã€‚æ¯è·‘ä¸€ä¸ªè¯éƒ½ä¼šå­˜å…¥å†å²åº“ï¼Œä¸­æ–­ååˆ·æ–°é‡è·‘å³å¯æ¥å…³ã€‚")
    
    up = st.file_uploader("ä¸Šä¼  Excel", type=["xlsx"])
    
    if up and info["api_key"]:
        try:
            df = pd.read_excel(up)
            target = next((c for c in df.columns if "è¯" in str(c) or "word" in str(c).lower()), None)
            
            if target:
                if st.button("ğŸš€ å¼€å§‹æ‰¹é‡ (200+æ¡ç¨³å®šæ¨¡å¼)"):
                    res = process_batch(df, info, target)
                    if res:
                        st.download_button("ğŸ“¥ ä¸‹è½½æœ¬æ¬¡ç»“æœ (Excel)", res, "final_result.xlsx")
            else:
                st.error("æœªæ‰¾åˆ°'è¯'åˆ—")
        except Exception as e:
            st.error(f"æ–‡ä»¶é”™è¯¯: {e}")

    st.markdown("---")
    st.subheader("ğŸ“š å†å²è®°å½•æ•°æ®åº“")
    st.caption("è¿™é‡Œæ˜¯æ€»è´¦æœ¬ã€‚å³ä½¿ç¨‹åºç°åœ¨å´©æºƒï¼Œæ•°æ®ä¹Ÿå…¨åœ¨è¿™é‡Œé¢ã€‚")
    
    db = "history_database.csv"
    if os.path.exists(db):
        try:
            hist = pd.read_csv(db)
            st.write(f"å½“å‰æ•°æ®åº“å·²å®‰å…¨ä¿å­˜ **{len(hist)}** æ¡è®°å½•ã€‚")
            c_d1, c_d2 = st.columns([1, 4])
            with c_d1:
                st.download_button("ğŸ“¥ ä¸‹è½½æ‰€æœ‰å†å² (CSV)", hist.to_csv(index=False).encode('utf-8-sig'), "history_database.csv", "text/csv")
            with c_d2:
                if st.button("ğŸ—‘ï¸ æ¸…ç©ºå†å² (æ…é‡)"):
                    os.remove(db)
                    st.rerun()
            with st.expander("é¢„è§ˆæ•°æ®"):
                st.dataframe(hist.tail(10))
        except:
            st.warning("æ­£åœ¨å†™å…¥ä¸­ï¼Œè¯·ç¨ååˆ·æ–°...")
    else:
        st.info("æš‚æ— æ•°æ®ã€‚")
if name == "main": main()

